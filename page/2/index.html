<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">

  <!-- PACE Progress Bar START -->
  
    <script src="https://raw.githubusercontent.com/HubSpot/pace/v1.0.2/pace.min.js"></script>
    <link rel="stylesheet" href="https://github.com/HubSpot/pace/raw/master/themes/orange/pace-theme-flash.css">
  
  

  <!-- PACE Progress Bar START -->

  
  <title>THE DATASCIENTIST</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  
  <meta property="og:type" content="website">
<meta property="og:title" content="THE DATASCIENTIST">
<meta property="og:url" content="http://foxsayy.github.io/page/2/index.html">
<meta property="og:site_name" content="THE DATASCIENTIST">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="THE DATASCIENTIST">
  
    <link rel="alternate" href="/atom.xml" title="THE DATASCIENTIST" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="https://cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/hiero.css">
  <link rel="stylesheet" href="/css/glyphs.css">
  

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/my.css">
  <!-- Google Adsense -->
  
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-0123456789ABCDEF",
          enable_page_level_ads: true
      });
  </script>
  
</head>
</html>
<script>
var themeMenus = {};

  themeMenus["/"] = "Home"; 

  themeMenus["/archives"] = "Archives"; 

  themeMenus["/categories/index.html"] = "Categories"; 

  themeMenus["/log/index.html"] = "log"; 

  themeMenus["/about/index.html"] = "About"; 

</script>


  <body>


  <header id="allheader" class="site-header" role="banner">
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="THE DATASCIENTIST" rel="home"> THE DATASCIENTIST </a>
            
          </h1>

          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>
            <div class="clearfix sf-menu">

              <ul id="main-nav" class="nmenu sf-js-enabled">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">Home</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">Archives</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories/index.html">Categories</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/log/index.html">log</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about/index.html">About</a> </li>
                    
              </ul>
            </div>
          </nav>


      </div>
  </div>
</header>


  <div id="originBgDiv" style="background: #fff; width: 100%;">

      <div style="max-height:600px; overflow: hidden;  display: flex; display: -webkit-flex; align-items: center;">
        <img id="originBg" width="100%" alt="" src="">
      </div>

  </div>

  <script>
  function setAboutIMG(){
      var imgUrls = "css/images/pose.jpg,https://source.unsplash.com/collection/954550/1920x1080".split(",");
      var random = Math.floor((Math.random() * imgUrls.length ));
      if (imgUrls[random].startsWith('http') || imgUrls[random].indexOf('://') >= 0) {
        document.getElementById("originBg").src=imgUrls[random];
      } else {
        document.getElementById("originBg").src='/' + imgUrls[random];
      }
  }
  bgDiv=document.getElementById("originBgDiv");
  if(location.pathname.match('about')){
    setAboutIMG();
    bgDiv.style.display='block';
  }else{
    bgDiv.style.display='none';
  }
  </script>



  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main">
  
    <article id="post-Deep-Learning-with-Python-Ch-02" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/01/28/Deep-Learning-with-Python-Ch-02/">Deep Learning with Python - Ch.02</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/01/28/Deep-Learning-with-Python-Ch-02/" class="article-date">
	  <time datetime="2019-01-28T07:04:24.000Z" itemprop="datePublished">January 28, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://ridibooks.com/v2/Detail?id=754024868&amp;_s=search&amp;_q=%EC%BC%80%EB%9D%BC%EC%8A%A4" target="_blank" rel="noopener">케라스 창시자에게 배우는 딥러닝</a>을 실습하면서 정리한 포스트입니다. 코드 예제와 코드 설명은 <a href="https://github.com/rickiepark/deep-learning-with-python-notebooks" target="_blank" rel="noopener">역자 깃허브</a>에서 받아볼 수 있습니다. 출판물이고 개인적으로만 참고하기 위한 요약노트이다 보니 설명이 불친절한 점은 양해 바랍니다. 보다 자세한 내용을 원하시는 분은 위 링크의 책을 참고하시기 바랍니다.</p>
<hr>
<h2 id="2장-시작-전에-신경망의-수학적-구성-요소"><a href="#2장-시작-전에-신경망의-수학적-구성-요소" class="headerlink" title="2장. 시작 전에: 신경망의 수학적 구성 요소"></a>2장. 시작 전에: 신경망의 수학적 구성 요소</h2><h3 id="신경망과의-첫-만남"><a href="#신경망과의-첫-만남" class="headerlink" title="신경망과의 첫 만남"></a>신경망과의 첫 만남</h3><ul>
<li>MNIST 예제</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 케라스에서 MNIST 데이터셋 불러오기</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 신경망 구조</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line">network = models.Sequential()</span><br><span class="line">network.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span> * <span class="number">28</span>,)))</span><br><span class="line">network.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>신경망의 핵심 구성 요소인 층(layer)은 일종의 데이터 처리 필터</li>
<li>어떤 데이터가 들어가면 더 유용한 형태로 출력됨(층은 주어진 문제에 더 의미 있는 표현을 입력된 데이터로부터 추출함)</li>
<li>딥러닝 모델은 데이터 정제 필터(층)가 연속되어 있는 데이터 프로세싱을 위한 여과기와 같음</li>
<li>위 예시는 조밀하게 연결된 신경망 층인 Dense 층 2개가 연속된 구조의 신경망 구조</li>
<li>두 번째 층은 10개의 확률 점수가 들어 있는 배열(모두 더하면 1)을 반환하는 소프트맥스 층</li>
<li>각 점수는 현재 숫자 이미지가 10개의 숫자 클래스 중 하나에 속할 확률임</li>
<li>신경망이 훈련 준비를 마치기 위해서 컴파일 단계에 포함될 세 가지가 더 필요<ul>
<li>손실 함수 : 훈련 데이터에서 신경망의 성능을 측정하는 방법. 네트워크가 옳은 방향으로 학습될 수 있도록 도움</li>
<li>옵티마이저: 입력된 데이터와 손실 함수를 기반으로 네트워크를 업데이트하는 메커니즘입니다.</li>
<li>훈련과 테스트 과정을 모니터링할 지표 : 여기에서는 정확도(정확히 분류된 이미지의 비율)만 고려하겠습니다.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 컴파일 단계</span></span><br><span class="line">network.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">               loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">               metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>트레이닝 시작 전에 데이터를 네트워크에 맞는 크기로 바꾸고 모든 값을 0과 1 사이로 스케일을 조정</li>
<li>MNIST 이미지는 [0, 255] 사이의 값인 unit8 타입의 (60000, 28, 28) 크기의 배열로 저장돼 있음. 이를 0과 1 사이 값을 가지는 float32 타입의 (60000, 28*28) 크기인 배열로 변경</li>
<li>categorical_crossentropy: 손실 함수. 가중치 텐서를 학습하기 위한 피드백 신호로 사용. 훈련하는 동안 최소화됨</li>
<li>rmsprop : 경사 하강법 적용 방식은 이 옵티마이저에 의해 결정</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 이미지 데이터 준비하기</span></span><br><span class="line">train_images = train_images.reshape((<span class="number">60000</span>, <span class="number">28</span> * <span class="number">28</span>))</span><br><span class="line">train_images = train_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">test_images = test_images.reshape((<span class="number">10000</span>, <span class="number">28</span> * <span class="number">28</span>))</span><br><span class="line">test_images = test_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 레이블 준비_범주형으로 인코딩</span></span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line">train_labels = to_categorical(train_labels)</span><br><span class="line">test_labels = to_categorical(test_labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fit 메서드로 트레이닝 데이터에 모델을 학습시킴</span></span><br><span class="line">network.fit(train_images, train_labels, epochs=<span class="number">5</span>, batch_size=<span class="number">128</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>네트워크가 128개 샘플씩 미니 배치로 훈련 데이터를 5번 반복</li>
<li>5번의 에포크 동안 네트워크는 2345번의 그래디언트 업데이트를 수행(에포크당 469번)</li>
<li>훈련 데이터에 대한 네트워크의 손실과 정확도 정보가 출력됨</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/5</span><br><span class="line">60000/60000 [==============================] - 3s 54us/step - loss: 0.2556 - acc: 0.9261</span><br><span class="line">Epoch 2/5</span><br><span class="line">60000/60000 [==============================] - 3s 51us/step - loss: 0.1042 - acc: 0.9688</span><br><span class="line">Epoch 3/5</span><br><span class="line">60000/60000 [==============================] - 3s 51us/step - loss: 0.0685 - acc: 0.9796</span><br><span class="line">Epoch 4/5</span><br><span class="line">60000/60000 [==============================] - 3s 52us/step - loss: 0.0499 - acc: 0.9845</span><br><span class="line">Epoch 5/5</span><br><span class="line">60000/60000 [==============================] - 3s 52us/step - loss: 0.0369 - acc: 0.9888</span><br><span class="line">&lt;keras.callbacks.History at 0x137a2e860&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 테스트셋에서 모델이 잘 작동하는지 확인하기</span></span><br><span class="line">test_loss, test_acc = network.evaluate(test_images, test_labels)</span><br><span class="line">print(<span class="string">'test_acc:'</span>, test_acc)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10000/10000 [==============================] - 0s 38us/step</span><br><span class="line">test_acc: 0.9798</span><br></pre></td></tr></table></figure>
<p>잘 작동한다!</p>
<ul>
<li>테스트셋의 정확도는 97.8%. 트레이닝셋 정확도와 차이가 나는 이유는 <strong>오버피팅</strong> 때문임</li>
</ul>
<h3 id="신경망을-위한-데이터-표현"><a href="#신경망을-위한-데이터-표현" class="headerlink" title="신경망을 위한 데이터 표현"></a>신경망을 위한 데이터 표현</h3><ul>
<li><strong>텐서</strong> : 데이터(숫자)를 위한 컨테이너</li>
<li>최근의 모든 머신러닝 시스템은 텐서를 기본 데이터 구조로 사용</li>
</ul>
<h4 id="스칼라-0D-텐서"><a href="#스칼라-0D-텐서" class="headerlink" title="스칼라(0D 텐서)"></a>스칼라(0D 텐서)</h4><ul>
<li>하나의 숫자만 담고 있는 텐서</li>
<li>넘파이에서는 float32나 float64 타입의 숫자가 스칼라 텐서</li>
<li>ndim 속성을 사용하면 넘파이 배열의 축 개수를 확인 가능</li>
<li>스칼라 텐서의 축 개수는 0(ndim == 0)</li>
<li>텐서의 축 개수를 랭크(rank)라고도 부름</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 스칼라 텐서(0D 텐서))</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.array(<span class="number">12</span>)</span><br><span class="line">x, x.ndim</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(array(12), 0)</span><br></pre></td></tr></table></figure>
<h4 id="벡터-1D-텐서"><a href="#벡터-1D-텐서" class="headerlink" title="벡터(1D 텐서)"></a>벡터(1D 텐서)</h4><ul>
<li>숫자의 배열</li>
<li>하나의 축을 가짐</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 벡터(1D 텐서)</span><br><span class="line">x = np.array([12, 3, 6, 14, 7])</span><br><span class="line">x, x.ndim</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(array([12,  3,  6, 14,  7]), 1)</span><br></pre></td></tr></table></figure>
<ul>
<li>위 예에서 x는 5개의 원소를 가지고 있으므로 5차원 벡터<ul>
<li>5D 벡터: 하나의 축을 따라 5개의 차원을 가진 것</li>
<li>5D 텐서: 5개의 축을 가진 것</li>
</ul>
</li>
<li>차원수(dimensionality)는 특정 축을 따라 놓인 원소의 개수(5D 벡터와 같은 경우)이거나 텐서의 축 개수(5D 텐서와 같은 경우)이거나 텐ㅅ의 축 개수(5D 텐서와 같은 경우)를 의미하므로 가끔 혼동하기 쉽다</li>
<li>후자의 경우 랭크 5인 텐서라고 말하는게 보다 정확(텐서의 랭크가 축의 개수)</li>
</ul>
<h4 id="행렬-2D-텐서"><a href="#행렬-2D-텐서" class="headerlink" title="행렬(2D 텐서)"></a>행렬(2D 텐서)</h4><ul>
<li>벡터의 배열이 행렬 또는 2D 텐서</li>
<li>행렬에는 행과 열 2개의 축이 있음</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">5</span>, <span class="number">78</span>, <span class="number">2</span>, <span class="number">34</span>, <span class="number">0</span>],</span><br><span class="line">              [<span class="number">6</span>, <span class="number">79</span>, <span class="number">3</span>, <span class="number">35</span>, <span class="number">1</span>],</span><br><span class="line">              [<span class="number">7</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="number">36</span>, <span class="number">2</span>]])</span><br><span class="line">x.ndim</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure>
<h4 id="3D-텐서와-고차원-텐서"><a href="#3D-텐서와-고차원-텐서" class="headerlink" title="3D 텐서와 고차원 텐서"></a>3D 텐서와 고차원 텐서</h4><ul>
<li>행렬들을 하나의 새로운 배열로 합치면 숫자로 채워진 직육면체 형태인 3D 텐서</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3D 텐서</span></span><br><span class="line">x = np.array([[[<span class="number">2</span>, <span class="number">45</span>, <span class="number">2</span>, <span class="number">34</span>, <span class="number">0</span>],</span><br><span class="line">               [<span class="number">5</span>, <span class="number">34</span>, <span class="number">5</span>, <span class="number">36</span>, <span class="number">1</span>],</span><br><span class="line">               [<span class="number">7</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="number">36</span>, <span class="number">2</span>]],</span><br><span class="line">              [[<span class="number">5</span>, <span class="number">78</span>, <span class="number">2</span>, <span class="number">34</span>, <span class="number">0</span>],</span><br><span class="line">               [<span class="number">6</span>, <span class="number">79</span>, <span class="number">3</span>, <span class="number">35</span>, <span class="number">1</span>],</span><br><span class="line">               [<span class="number">7</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="number">37</span>, <span class="number">2</span>]],</span><br><span class="line">              [[<span class="number">5</span>, <span class="number">89</span>, <span class="number">3</span>, <span class="number">35</span>, <span class="number">1</span>],</span><br><span class="line">               [<span class="number">7</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="number">36</span>, <span class="number">2</span>],</span><br><span class="line">               [<span class="number">8</span>, <span class="number">72</span>, <span class="number">2</span>, <span class="number">40</span>, <span class="number">5</span>]]])</span><br><span class="line">x.ndim</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></table></figure>
<ul>
<li>3D 텐서들을 하나의 배열로 합치면 4D 텐서</li>
<li>딥러닝에선 보통 0D에서 4D까지의 텐서를 다룬다</li>
<li>동영상 데이터를 다룰 때는 5D 텐서까지 가기도</li>
</ul>
<h4 id="핵심-속성"><a href="#핵심-속성" class="headerlink" title="핵심 속성"></a>핵심 속성</h4><ul>
<li>텐서의 핵심 속성 3가지<ul>
<li>축의 개수(rank) : 3D 텐서에는 3개의 축, 행렬에는 2개의 축</li>
<li>크기(shape) : 텐서의 각 축을 따라 얼마나 많은 차원이 있는지 나타낸 파이썬의 튜플.</li>
<li>데이터 타입 : float32, float64. uint8 등</li>
</ul>
</li>
</ul>
<h4 id="배치-데이터"><a href="#배치-데이터" class="headerlink" title="배치 데이터"></a>배치 데이터</h4><ul>
<li>일반적으로 데이터 텐서의 첫 번째 축(0번 축)은 샘플 축(MNIST 예제에서는 숫자 이미지가 샘플)</li>
<li>딥러닝 모델은 데이터를 작은 batch로 나눠서 처리</li>
<li>이런 batch 데이터를 다룰 땐 0번 축을 배치 축 또는 배치 차원이라 부른다</li>
</ul>
<h4 id="텐서의-실제-사례"><a href="#텐서의-실제-사례" class="headerlink" title="텐서의 실제 사례"></a>텐서의 실제 사례</h4><ul>
<li>벡터 데이터<ul>
<li>(샘플들, 피처들) 크기의 2D 텐서</li>
<li>ex) 사람의 나이, 우편번호, 소득으로 구성된 인구 통계 데이터. 각 사람은 3개의 값을 가진 벡터로 구성. 10만명이 포함된 전체 데이터셋은 (100000, 3) 크기의 텐서에 저장  </li>
</ul>
</li>
<li>시계열 데이터<ul>
<li>(샘플들, timsteps, 피처들) 크기의 3D 텐서</li>
<li>시간이 중요할 때는 시간 축을 포함해 3D 텐서로 저장</li>
<li>관례적으로 시간 축은 항상 1번(두 번째) 축</li>
<li>ex) 주식 가격 데이터셋. 1분마다 현재 주식 가격, 지난 1분 동안 최고가와 최소가를 저장. 1분마다 데이터는 3D 벡터로 인코딩. 하루(390분) 거래는 (390, 3) 크기의 2D 텐서로 인코딩. 250일치 데이터는 (250, 390, 3) 크기의 3D 텐서로 저장될 수 있음.     </li>
</ul>
</li>
<li>이미지<ul>
<li>(샘플들, 높이, 너비, 컬러 채널) 크기의 4D 텐서</li>
<li>ex) 256 x 256 크기의 흑백 이미지에 대한 128개의 배치는 (128, 256, 256, 1) 크기의 텐서에 저장 가능. 컬러라면 (128, 256, 256, 3)</li>
</ul>
</li>
<li>동영상<ul>
<li>(samples, frames, height, width, channels) 크기의 5D 텐서</li>
<li>ex) 60초짜리 144 x 256 유튜브 영상을 초당 4프레임으로 샘플링하면 240프레임. 이 영상을 4개 가진 배치는 (4, 240, 144, 256, 3) 크기의 텐서에 저장.</li>
</ul>
</li>
</ul>
<h3 id="신경망의-톱니바퀴-텐서-연산"><a href="#신경망의-톱니바퀴-텐서-연산" class="headerlink" title="신경망의 톱니바퀴 : 텐서 연산"></a>신경망의 톱니바퀴 : 텐서 연산</h3><ul>
<li>케라스 층 생성하기<ul>
<li><code>keras.layers.Dense(512, activation=&#39;relu&#39;)</code></li>
<li>2D 텐서를 입력받고 입력 텐서의 새로운 표현인 또 다른 2D 텐서를 반환하는 함수로 볼 수 있음</li>
<li><code>output = relu(dot(W, input) + b)</code> 함수와 같음</li>
<li>이 함수에는 3개의 텐서 연산이 있음. 입력 텐서와 W의 dot, dot의 결과인 2D 텐서와 벡터 b 사이의 덧셈, relu 연산</li>
</ul>
</li>
</ul>
<h4 id="원소별-연산"><a href="#원소별-연산" class="headerlink" title="원소별 연산"></a>원소별 연산</h4><ul>
<li>relu 함수와 덧셈은 원소별 연산.</li>
<li>각 원소에 독립적으로 적용됨</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파이썬으로 구현한 원소별 연산_relu</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len((x.shape) == <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    x = x.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">            x[i, j] = max(x[i, j], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 파이썬으로 구현한 원소별 연산_덧셈</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_add</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">2</span></span><br><span class="line">    <span class="keyword">assert</span> x.shape == y.shape</span><br><span class="line"></span><br><span class="line">    x = x.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">            x[i, j] += y[i, j]</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="브로드캐스팅"><a href="#브로드캐스팅" class="headerlink" title="브로드캐스팅"></a>브로드캐스팅</h4><ul>
<li>크기가 다른 두 텐서가 더해진다면? 실행 가능하다면 작은 텐서가 큰 텐서 크기에 맞춰 브로드캐스팅됨<ol>
<li>큰 텐서의 ndim에 맞게 작은 텐서에 브로드캐스팅 축이 추가됨</li>
<li>작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복됨</li>
</ol>
</li>
<li>ex) X.shape = (32, 10), y.shape = (10,)<ol>
<li>y에 비어 있는 축을 추가해 (1, 10)으로</li>
<li>y를 이 축에 32번 반복하면 텐서 Y.shape는 (32, 10)</li>
<li>크기가 같아져서 더할 수 있음</li>
</ol>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 단순 구현</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_add_matrix_and_vector</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">2</span></span><br><span class="line">    <span class="keyword">assert</span> len(y.shape) == <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> x.shape[<span class="number">1</span>] == y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    x = x.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">            x[i, j] += y[j]</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 크기가 다른 두 텐서에 브로드캐스팅으로 원소별 maximum 연산 적용</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.random.random((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">10</span>))</span><br><span class="line">y = np.random.random((<span class="number">32</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">z = np.maximum(x, y)</span><br></pre></td></tr></table></figure>
<h4 id="tensor-product"><a href="#tensor-product" class="headerlink" title="tensor product"></a>tensor product</h4><ul>
<li>원소별 연산과 반대로 입력 텐서의 원소들을 결합시킴</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 점곱 연산</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_vector_dot</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> len(y.shape) == <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> x.shape[<span class="number">0</span>] == y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    z = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        z += x[i] * y[i]</span><br><span class="line">    <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 행렬 x와 벡터 y의 점곱</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_matrix_vector_dot</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">2</span></span><br><span class="line">    <span class="keyword">assert</span> len(y.shape) == <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> x.shape[<span class="number">1</span>] == y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    z = np.zeros(x.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">            z[i] += x[i, j] * y[j]</span><br><span class="line">    <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure>
<h4 id="텐서-크기-변환-tensor-reshaping"><a href="#텐서-크기-변환-tensor-reshaping" class="headerlink" title="텐서 크기 변환(tensor reshaping)"></a>텐서 크기 변환(tensor reshaping)</h4><ul>
<li>특정 크기에 맞게 열과 행을 재배열</li>
<li>행과 열을 바꾸는 전치도 자주 사용</li>
</ul>
<h4 id="텐서-연산의-기하학적-해석"><a href="#텐서-연산의-기하학적-해석" class="headerlink" title="텐서 연산의 기하학적 해석"></a>텐서 연산의 기하학적 해석</h4><ul>
<li>아핀 변환, 회전, 스케일링 등 기본적인 기하학적 연산은 텐선으로 표현 가능</li>
</ul>
<h4 id="딥러닝의-기하학적-해석"><a href="#딥러닝의-기하학적-해석" class="headerlink" title="딥러닝의 기하학적 해석"></a>딥러닝의 기하학적 해석</h4><ul>
<li>빨간색 파란색 2장의 색종이를 겹친 다음 뭉쳐서 작은 공을 만든다고 가정</li>
<li>종이공: 입력 데이터, 색종이: 분류 문제의 데이터 클래스</li>
<li>신경망의 역할: 종이 공을 펼쳐서 두 클래스가 분리되는 변환을 찾는 것</li>
</ul>
<h3 id="신경망의-엔진-그래디언트-기반-최적화"><a href="#신경망의-엔진-그래디언트-기반-최적화" class="headerlink" title="신경망의 엔진: 그래디언트 기반 최적화"></a>신경망의 엔진: 그래디언트 기반 최적화</h3><p><code>output = relu(dot(W, input) + b)</code></p>
<ul>
<li>텐서 W와 b는 층의 속성. 가중치 또는 훈련되는 파라미터.</li>
<li>이런 가중치에는 훈련 데이터를 신경망에 노출시켜 학습된 정보가 담겨 있음</li>
<li>초기에는 가중치 행렬이 작은 난수로 채워짐(무작위 초기화 단계)</li>
<li>피드백 신호에 따라 가중치가 점진적으로 조정됨(훈련 단계)</li>
<li>훈련 반복 루프<ol>
<li>트레이닝 샘플 x와 타깃 y의 배치를 추출</li>
<li>x를 사용해 네트워크를 실행(forward pass 단계)하고 y_pred 구하기</li>
<li>y와 y_pred 차이를 측정해 이 배치에 대한 네트워크 손실 계산</li>
<li>배치에 대한 손실이 감소되도록 네트워크 가중치 업데이트</li>
</ol>
</li>
<li>이 접근법은 모든 가중치 행렬의 원소바다 두 번의 forward pass를 계산해야 하므로 비효율적</li>
<li>신경망에 사용된 연산이 미분 가능하다는 점을 이용해 네트워크 가중치에 대한 손실의 그래디언트를 계산하는 게 더 좋은 방법</li>
</ul>
<h4 id="변화율"><a href="#변화율" class="headerlink" title="변화율"></a>변화율</h4><ul>
<li>derivative!</li>
</ul>
<h4 id="텐서-연산의-변화율-그래디언트"><a href="#텐서-연산의-변화율-그래디언트" class="headerlink" title="텐서 연산의 변화율: 그래디언트"></a>텐서 연산의 변화율: 그래디언트</h4><ul>
<li>다차원 입력, 즉 텐서를 입력으로 받는 함수에 변화율 개념을 확장시킨 것</li>
<li>y_pred = dot(W, x)</li>
<li>loss_value = loss(y_pred, y)</li>
<li>입력 데이터 x와 y가 고정돼 있다면 이 함수는 W를 손실 값에 매핑하는 함수로 볼 수 있음</li>
<li>loss_value = f(W)</li>
<li>W0(W의 현재값)에서 f의 변화율: W와 같은 크기의 텐서인 gradient(f)(W0)</li>
<li>이 텐서의 각 원소 gradient(f)(W0)[i, j]: W0[i, j]를 변경했을 때 loss_value가 바뀌는 방향과 크기를 나타냄</li>
<li>즉 gradient(f)(W0)가 W0에서 함수 f(W) = loss_value의 그래디언트</li>
<li>gradient(f)(W0)는 W0에서 f(W)의 기울기를 나타내는 텐서로 해석 가능</li>
<li>f(W)에서 그래디언트의 반대 방향으로 W를 움직이면 f(W) 값을 줄일 수 있음</li>
</ul>
<h4 id="확률적-경사-하강법"><a href="#확률적-경사-하강법" class="headerlink" title="확률적 경사 하강법"></a>확률적 경사 하강법</h4><ul>
<li>절충안: 미니 배치 확률적 경사 하강법(미니 배치 SGD)<ol>
<li>훈련샘플 배치 x와 타깃 y를 추출</li>
<li>x로 네트워크를 실행하고 y_pred 구하기</li>
<li>y와 y_pred 사이의 오차를 측정해 네트워크 손실 계산</li>
<li>네트워크의 파라미터에 대한 손실 함수의 그래디언트 계산(backward pass)</li>
<li>그래디언트 반대 방향으로 파라미터 이동</li>
</ol>
</li>
<li>트루 SGD<ul>
<li>반복마다 하나의 샘플과 하나의 타깃을 뽑음</li>
</ul>
</li>
<li>배치 SGD<ul>
<li>가용한 모든 데이터로 반복 실행</li>
<li>업데이트가 정확하지만 많은 비용</li>
</ul>
</li>
<li>SGD 변종들<ul>
<li>업데이트할 다음 가중치를 계산할 때 현재 그래디언트 값만 보지 않고 이전에 업데이트된 가중치를 여러 가지 다른 방식으로 고려</li>
<li>ex) 모멘텀을 사용한 SGD, Adagrad, RMSProp 등</li>
<li>이런 변종들을 최적화 방법 or 옵티마이저라 부름</li>
<li>모멘텀은 SGD에 있는 2개의 문제점인 수렴 속도와 지역 최솟값을 해결</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모멘텀 단순 구현</span></span><br><span class="line">past_velocity = <span class="number">0.</span></span><br><span class="line">momentum = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">while</span> loss &gt; <span class="number">0.1</span>:</span><br><span class="line">    w, loss, gradient = get_current_parameters()</span><br><span class="line">    velocity = momentum * past_velocity - learning_rate * gradient</span><br><span class="line">    w = w + momentum * velocity - learning_rate * gradient</span><br><span class="line">    past_velocity = velocity</span><br><span class="line">    update_parameter(w)</span><br></pre></td></tr></table></figure>
<h4 id="변화율-연결-역전파-알고리즘"><a href="#변화율-연결-역전파-알고리즘" class="headerlink" title="변화율 연결: 역전파 알고리즘"></a>변화율 연결: 역전파 알고리즘</h4><ul>
<li>3개의 텐서 연산 a, b, c와 가중치 행렬 W1, W2, W3로 구성된 네트워크 f의 예</li>
<li>f(W1, W2, W3) = a(W1, b(W2, c(W3)))</li>
<li>연쇄법칙을 신경망 그래디언트 계산에 적용한 역전파 알고리즘</li>
</ul>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/DSBooks/">DSBooks</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Keras-Deep-Learning-with-Python/">Keras, Deep Learning with Python</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Deep-Learning-with-Python-Ch-01" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/01/28/Deep-Learning-with-Python-Ch-01/">Deep Learning with Python - Ch.01</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/01/28/Deep-Learning-with-Python-Ch-01/" class="article-date">
	  <time datetime="2019-01-28T06:36:09.000Z" itemprop="datePublished">January 28, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://ridibooks.com/v2/Detail?id=754024868&amp;_s=search&amp;_q=%EC%BC%80%EB%9D%BC%EC%8A%A4" target="_blank" rel="noopener">케라스 창시자에게 배우는 딥러닝</a>을 실습하면서 정리한 포스트입니다. 코드 예제와 코드 설명은 <a href="https://github.com/rickiepark/deep-learning-with-python-notebooks" target="_blank" rel="noopener">역자 깃허브</a>에서 받아볼 수 있습니다. 출판물이고 개인적으로만 참고하기 위한 요약노트이다 보니 설명이 불친절한 점은 양해 바랍니다. 보다 자세한 내용을 원하시는 분은 위 링크의 책을 참고하시기 바랍니다.</p>
<hr>
<h2 id="1장-딥러닝이란-무엇인가"><a href="#1장-딥러닝이란-무엇인가" class="headerlink" title="1장 딥러닝이란 무엇인가?"></a>1장 딥러닝이란 무엇인가?</h2><h3 id="인공지능과-머신-러닝-딥러닝"><a href="#인공지능과-머신-러닝-딥러닝" class="headerlink" title="인공지능과 머신 러닝, 딥러닝"></a>인공지능과 머신 러닝, 딥러닝</h3><h4 id="인공지능"><a href="#인공지능" class="headerlink" title="인공지능"></a>인공지능</h4><ul>
<li>보통의 사람이 수행하는 지능적인 작업을 자동화하기 위한 연구 활동</li>
<li>인공지능 ⊃ 머신러닝 ⊃ 딥러닝</li>
<li>심볼릭 AI : “명시적인 규칙을 충분히 많이 만들어 지식을 다루면 인간 수준의 인공지능을 만들 수 있다” ☞ 하지만 명확한 규칙을 찾는게 쉽지 않음</li>
</ul>
<h4 id="머신러닝"><a href="#머신러닝" class="headerlink" title="머신러닝"></a>머신러닝</h4><ul>
<li>전통적 프로그래밍 : 규칙, 데이터를 넣어서 해답을 찾는다</li>
<li>머신러닝 : 데이터, 해답을 넣어서 규칙을 찾는다</li>
</ul>
<h4 id="데이터에서-표현을-학습하기"><a href="#데이터에서-표현을-학습하기" class="headerlink" title="데이터에서 표현을 학습하기"></a>데이터에서 표현을 학습하기</h4><ul>
<li>머신 러닝과 딥러닝의 핵심 문제는 의미 있는 데이터로의 변환</li>
<li>입력 데이터를 기반으로 기대 출력에 가깝게 만드는 유용한 표현을 학습하는 것</li>
</ul>
<h4 id="딥러닝에서-‘딥’이란"><a href="#딥러닝에서-‘딥’이란" class="headerlink" title="딥러닝에서 ‘딥’이란?"></a>딥러닝에서 ‘딥’이란?</h4><ul>
<li>딥러닝은 머신러닝의 특정한 한 분야로 연속된 층에서 점진적으로 의미 있는 표현을 배우는 데 강점이 있음</li>
<li>‘딥’이란 연속된 층으로 표현을 학습한다는 개념을 나타냄</li>
<li>딥러닝에선 기본 층을 겹겹이 쌓아 올려 구성한 신경망이라는 모델을 사용</li>
</ul>
<h4 id="그림-3개로-딥러닝의-작동-원리-이해하기"><a href="#그림-3개로-딥러닝의-작동-원리-이해하기" class="headerlink" title="그림 3개로 딥러닝의 작동 원리 이해하기"></a>그림 3개로 딥러닝의 작동 원리 이해하기</h4><ul>
<li>신경망은 가중치를 파라미터로 가진다</li>
<li>손실 함수가 신경망의 출력 품질을 측정한다</li>
<li>손실 점수를 피드백 신호로 사용하여 가중치를 조정한다</li>
</ul>
<h4 id="지금까지-딥러닝-성과"><a href="#지금까지-딥러닝-성과" class="headerlink" title="지금까지 딥러닝 성과"></a>지금까지 딥러닝 성과</h4><ul>
<li>이미지 분류, 음성, 필기 인식 등</li>
<li>여전히 할 수 있는 일을 알아가는 중. 형식 추론과 같은 다양한 문제에 적용하기 시작</li>
</ul>
<h4 id="단기간의-과대-선전을-믿지-말자"><a href="#단기간의-과대-선전을-믿지-말자" class="headerlink" title="단기간의 과대 선전을 믿지 말자"></a>단기간의 과대 선전을 믿지 말자</h4><ul>
<li>딥러닝이 할 수 있는 것과 할 수 없는 것에 대해 명확히 이해하자</li>
</ul>
<h4 id="AI에-대한-전망"><a href="#AI에-대한-전망" class="headerlink" title="AI에 대한 전망"></a>AI에 대한 전망</h4><ul>
<li>단기 기대는 비현실적일수도 있지만 장기 전망은 매우 밝다</li>
</ul>
<h3 id="딥러닝-이전-머신-러닝의-간략한-역사"><a href="#딥러닝-이전-머신-러닝의-간략한-역사" class="headerlink" title="딥러닝 이전: 머신 러닝의 간략한 역사"></a>딥러닝 이전: 머신 러닝의 간략한 역사</h3><h4 id="확률적-모델링"><a href="#확률적-모델링" class="headerlink" title="확률적 모델링"></a>확률적 모델링</h4><ul>
<li>통계학 이론을 데이터 분석에 응용한 것</li>
<li>나이브 베이즈 알고리즘 : 입력 데이터의 특성이 모두 독립적이라 가정하고 베이즈 정리를 적용하는 머신 러닝 분류 알고리즘</li>
<li>로지스틱 회귀 : 현대 머신 러닝의 “hello world”</li>
</ul>
<h4 id="초창기-신경망"><a href="#초창기-신경망" class="headerlink" title="초창기 신경망"></a>초창기 신경망</h4><ul>
<li>경사 하강법 최적화를 사용해 연쇄적으로 변수가 연결된 연산을 훈련하는 방법</li>
<li>최초의 성공적인 신경망 애플리케이션은 합성곱 신경망과 역전파를 연결해 손글씨 숫자 이미지를 분류하는 문제에 적용됨</li>
</ul>
<h4 id="커널-방법"><a href="#커널-방법" class="headerlink" title="커널 방법"></a>커널 방법</h4><ul>
<li>분류 알고리즘의 한 종류로 SVM(서포트 벡터 머신)이 가장 유명</li>
<li>SVM은 2개의 다른 범주에 속한 데이터 그룹 사이에 결정 경계(decision boundary)를 찾는다</li>
<li>decision boundary를 찾는 두 단계 과정<ul>
<li>decision boundary가 하나의 초평면(hyperplane)으로 표현될 수 있는 새로운 고차원 표현으로 데이터를 매핑</li>
<li>초평면과 각 클래스의 가장 가까운 데이터 포인트 사이의 거리가 최대가 되는 최선의 결정 경계(하나의 분할 초평면)를 찾는다 == 마진 최대화 단계</li>
</ul>
</li>
</ul>
<h4 id="결정트리-랜덤-포레스트-그래디언트-부스팅"><a href="#결정트리-랜덤-포레스트-그래디언트-부스팅" class="headerlink" title="결정트리, 랜덤 포레스트, 그래디언트 부스팅"></a>결정트리, 랜덤 포레스트, 그래디언트 부스팅</h4><ul>
<li>결정트리 : 플로차트 같은 구조를 가지며 입력 데이터 포인트를 분류하거나 주어진 입력에 대해 출력 값을 예측</li>
<li>랜덤 포레스트 : 서로 다른 결정 트리를 많이 만들고 그 출력을 앙상블하는 방법을 사용</li>
<li>그래디언트 부스팅 : 이전 모델에서 놓친 데이터 포인트를 보완하는 새로운 모델을 반복적으로 훈련해 모델을 향상시킴</li>
</ul>
<h4 id="다시-신경망으로"><a href="#다시-신경망으로" class="headerlink" title="다시 신경망으로"></a>다시 신경망으로</h4><ul>
<li>2011년 IDSIA의 댄 크리슨이 심층 신경망으로 이미지 분류 대회에서 우승했는데 이것이 현대적 딥러닝의 첫 번째 성공</li>
<li>2012년부터 심층 합성곱이 모든 컴퓨터 비전 작업의 주력 알고리즘이 됨</li>
</ul>
<h4 id="딥러닝의-특징"><a href="#딥러닝의-특징" class="headerlink" title="딥러닝의 특징"></a>딥러닝의 특징</h4><ul>
<li>딥러닝의 변환 능력은 모델이 모든 표현 층을 순차적이 아니라 동시에 공동으로 학습하게 만든다</li>
<li>딥러닝이 데이터로부터 학습하는 방법의 특징<ul>
<li>층을 거치면서 점진적으로 더 복잡한 표현이 만들어진다</li>
<li>이런 점진적인 중간 표현이 공동으로 학습된다</li>
</ul>
</li>
</ul>
<h4 id="머신러닝의-최근-동향"><a href="#머신러닝의-최근-동향" class="headerlink" title="머신러닝의 최근 동향"></a>머신러닝의 최근 동향</h4><ul>
<li>캐글의 머신 러닝 경연을 살펴보면 동향을 알 수 있다</li>
<li>2016 2017년 캐글 주류는 그래디언트 부스팅 머신(구조적인 데이터의 경우)과 딥러닝(이미지 분류 등 지각에 관한 문제)</li>
</ul>
<h3 id="왜-딥러닝일까-왜-지금"><a href="#왜-딥러닝일까-왜-지금" class="headerlink" title="왜 딥러닝일까? 왜 지금?"></a>왜 딥러닝일까? 왜 지금?</h3><ul>
<li>컴퓨터 비전 딥러닝 핵심 아이디어인 합성곱 신경망과 역전파는 1989년에, 시계열을 위한 딥러닝 기본인 LSTM(Long Short-Term Memory) 알고리즘은 1997년에 개발됐는데 2012년 이후에 딥러닝이 부상한 배경에는 다음 세 가지가 있다<ul>
<li>하드웨어</li>
<li>데이터셋과 벤치마크</li>
<li>알고리즘 향상</li>
</ul>
</li>
</ul>
<h4 id="새로운-투자의-바람"><a href="#새로운-투자의-바람" class="headerlink" title="새로운 투자의 바람"></a>새로운 투자의 바람</h4><ul>
<li>딥러닝은 테크 공룡들의 핵심 상품 전략</li>
</ul>
<h4 id="딥러닝의-대중화"><a href="#딥러닝의-대중화" class="headerlink" title="딥러닝의 대중화"></a>딥러닝의 대중화</h4><ul>
<li>초창기에는 C++과 CUDA 전문가가 되어야 했지만 요즘에는 기본 파이썬 스크립트 기술만 있으면 딥러닝 연구에 충분(씨아노와 텐서플로 덕분)</li>
</ul>
<h4 id="지속될까"><a href="#지속될까" class="headerlink" title="지속될까?"></a>지속될까?</h4><ul>
<li>20년 뒤에는 신경망을 쓰지 않을지도 모르지만 딥러닝과 그 핵심개념에서 파생된 무엇인가를 사용할 것</li>
<li>단순함, 확장성, 다용도와 재사용성</li>
</ul>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/DSBooks/">DSBooks</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Keras-Deep-Learning-with-Python/">Keras, Deep Learning with Python</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-World-This-Week-Jan-26th-2019" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/01/28/World-This-Week-Jan-26th-2019/">[World This Week] Jan 26th 2019</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/01/28/World-This-Week-Jan-26th-2019/" class="article-date">
	  <time datetime="2019-01-28T05:42:35.000Z" itemprop="datePublished">January 28, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<h4 id="All-the-world-is-a-laboratory-to-the-inquiring-mind"><a href="#All-the-world-is-a-laboratory-to-the-inquiring-mind" class="headerlink" title="All the world is a laboratory to the inquiring mind."></a><strong>All the world is a laboratory to the inquiring mind.</strong></h4><p> <em>- Martin H. Fischer</em></p>
</blockquote>
<p><br></p>
<p><a href="https://www.economist.com/printedition/2019-01-26" target="_blank" rel="noopener"><img src="https://www.economist.com/sites/default/files/imagecache/400-width/print-covers/20190126_cuk400hires_0.jpg" alt="2019-01-26"></a></p>
<h4 id="The-world-this-week-Business-this-Week"><a href="#The-world-this-week-Business-this-Week" class="headerlink" title="| The world this week | Business this Week"></a>| The world this week | Business this Week</h4><ul>
<li></li>
</ul>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/The-Economist/">The Economist</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/The-economist/">The economist</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-FC-Tensorflow-패키지-소개-2" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/01/23/FC-Tensorflow-패키지-소개-2/">[FC] Tensorflow 패키지 소개(2)</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/01/23/FC-Tensorflow-패키지-소개-2/" class="article-date">
	  <time datetime="2019-01-23T01:47:33.000Z" itemprop="datePublished">January 23, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>패스트캠퍼스 데이터사이언스스쿨의 김도형 박사님 수업을 듣고 강의자료를 요약한 글입니다. 개인적으로 참고하기 위한 요약노트이니 보다 자세한 내용을 원하시는 분은 <a href="https://datascienceschool.net" target="_blank" rel="noopener">https://datascienceschool.net</a> 에 올라와 있는 강의자료를 참고하시기 바랍니다.</p>
<hr>
<h3 id="변수-공간과-변수의-재사용"><a href="#변수-공간과-변수의-재사용" class="headerlink" title="변수 공간과 변수의 재사용"></a>변수 공간과 변수의 재사용</h3><h3 id="Tensor-연산"><a href="#Tensor-연산" class="headerlink" title="Tensor 연산"></a>Tensor 연산</h3><h3 id="자동-형변환"><a href="#자동-형변환" class="headerlink" title="자동 형변환"></a>자동 형변환</h3><h3 id="미분"><a href="#미분" class="headerlink" title="미분"></a>미분</h3><h3 id="TensorFlow를-이용한-선형회귀"><a href="#TensorFlow를-이용한-선형회귀" class="headerlink" title="TensorFlow를 이용한 선형회귀"></a>TensorFlow를 이용한 선형회귀</h3><h3 id="퍼셉트론"><a href="#퍼셉트론" class="headerlink" title="퍼셉트론"></a>퍼셉트론</h3><h3 id="최적화"><a href="#최적화" class="headerlink" title="최적화"></a>최적화</h3><h3 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h3><h4 id="텐서보드용-로그-생성"><a href="#텐서보드용-로그-생성" class="headerlink" title="텐서보드용 로그 생성"></a>텐서보드용 로그 생성</h4><h4 id="텐서보드-가동"><a href="#텐서보드-가동" class="headerlink" title="텐서보드 가동"></a>텐서보드 가동</h4><p><strong>연습문제 20.1.2</strong></p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/datascienceschool/">datascienceschool</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/fastcampus-Dr-Kim-datascienceschool/">fastcampus, Dr.Kim, datascienceschool</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-FC-Tensorflow-패키지-소개" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/01/22/FC-Tensorflow-패키지-소개/">[FC] Tensorflow 패키지 소개</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/01/22/FC-Tensorflow-패키지-소개/" class="article-date">
	  <time datetime="2019-01-22T04:26:29.000Z" itemprop="datePublished">January 22, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>패스트캠퍼스 데이터사이언스스쿨의 김도형 박사님 수업을 듣고 강의자료를 요약한 글입니다. 개인적으로 참고하기 위한 요약노트이니 보다 자세한 내용을 원하시는 분은 <a href="https://datascienceschool.net" target="_blank" rel="noopener">https://datascienceschool.net</a> 에 올라와 있는 강의자료를 참고하시기 바랍니다.</p>
<hr>
<h3 id="TensorFlow-기본-사용법"><a href="#TensorFlow-기본-사용법" class="headerlink" title="TensorFlow 기본 사용법"></a>TensorFlow 기본 사용법</h3><ol>
<li>텐서 정의</li>
<li>텐서 연산 정의</li>
<li>세션 정의</li>
<li>세션 사용</li>
</ol>
<h3 id="그래프와-세션"><a href="#그래프와-세션" class="headerlink" title="그래프와 세션"></a>그래프와 세션</h3><ul>
<li>텐서플로는 모든 연산을 자체적인 CPU가 아닌 외부에 분산된 GPU에서 처리한다고 가정 ➜ 컴퓨터 자체적으로 이뤄지는 연산은 없음</li>
<li>텐서(Tensor) 계산 과정은 모두 <strong>그래프(Graph)</strong>라는 객체 내에 저장됨</li>
<li>그래프를 계산하려면 외부 컴퓨터에 이 그래프 정보를 전달하고 그 결과값을 받아야 하는데, 이 통신과정은 <strong>세션(Session)</strong>이라는 객체가 담당</li>
<li>모든 텐서 계산은 해당하는 텐서를 포함하는 그래프를 세션 객체에 전달해 원격 실행한 후에 값을 볼 수 있음</li>
</ul>
<h3 id="그래프"><a href="#그래프" class="headerlink" title="그래프"></a>그래프</h3><ul>
<li>그래프의 구성<ul>
<li>노드(node): 상수형 텐서, 변수형 텐서, 연산(operation)</li>
<li>간선(edge): 노드로부터 출력되는 텐서 자료형</li>
</ul>
</li>
<li>따라서 텐서 자료형을 만들거나 연산자를 연결하면 모두 그래프에 들어가야 함</li>
<li>그래프를 명시적으로 지정하지 않으면 기본 그래프(default graph)에 할당</li>
<li>현재 기본 그래프에 대한 정보 얻기</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(tf.get_default_graph())</span><br></pre></td></tr></table></figure>
<ul>
<li><code>tf.Graph</code> 클래스로 명시적으로 그래프 생성하기</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_graph = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> my_graph.as_default():</span><br><span class="line">    my_x = tf.constant(<span class="number">3</span>)</span><br><span class="line">    my_y = my_x ** <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h3 id="세션"><a href="#세션" class="headerlink" title="세션"></a>세션</h3><ul>
<li>Theano의 함수(function)와 비슷한 역할</li>
<li>실제로 계산 그래프를 생성하고 값을 계산하기 위한 환경을 제공</li>
<li>Theano의 함수와 달리 세션 생성과 실행 시작, 종료를 다음과 같은 방법으로 명시해야 함<ul>
<li>세션 생성: Session 객체 생성. 분산 환경에서는 계산 노드와의 연결을 만듦</li>
<li>세션 사용: run 메서드에 그래프를 입력하면 출력 값을 계산하여 반함. 분산 환경에서는 계산 노드로 그래프를 보내 계산을 수행</li>
<li>세션 종료: close 메서드. with 문을 사용하면 명시적으로 호출하지 않아도 됨</li>
</ul>
</li>
<li>두 개의 상수형 텐서를 생성하고 세션을 통해 실행하기</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant(<span class="number">3</span>)</span><br><span class="line">y = x**<span class="number">2</span></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(x))</span><br><span class="line">print(sess.run(y))</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">9</span><br></pre></td></tr></table></figure>
<ul>
<li>with 구문을 사용한 세션 구현 ➜ with 블럭을 나갈 때 자동으로 close 메서드가 호출됨</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    out = sess.run([x, y])</span><br><span class="line">    print(out)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[3, 9]</span><br></pre></td></tr></table></figure>
<ul>
<li>직접 그래프를 만들 경우 ➜ 세션 생성시 해당 그래프를 인수로 주어야 함. (하나의 세션은 하나의 그래프만 받을 수 있음)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session(graph=my_graph) <span class="keyword">as</span> sess:</span><br><span class="line">    out = sess.run([my_x, my_y])</span><br><span class="line">    print(out)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[3, 9]</span><br></pre></td></tr></table></figure>
<h3 id="인터랙티브-세션과-eval-메서드"><a href="#인터랙티브-세션과-eval-메서드" class="headerlink" title="인터랙티브 세션과 eval() 메서드"></a>인터랙티브 세션과 <code>eval()</code> 메서드</h3><ul>
<li>텐서플로는 간단한 작업이라도 세션을 통해야지만 가능함</li>
<li>이런 불편을 해소하기 위해 파이썬 콘솔이나 주피터노트북을 사용하는 경우 인터랙티브 세션(Interactive Session)을 제공</li>
<li>인터랙티브 세션을 생성한 후에는 텐서의 eval() 메서드를 호출하기만 하면 명시적으로 세션을 지정하지 않더라도 자동으로 세션이 호출되어 텐서의 값이 출력됨</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">interactive_sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">x.eval(), y.eval()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(3, 9)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>as_default()</code> 메서드로 with문 안에서 인터랙티브 세션처럼 사용하기</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session().as_default():</span><br><span class="line">    print([x.eval(), y.eval()])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[3, 9]</span><br></pre></td></tr></table></figure>
<h3 id="텐서-자료형"><a href="#텐서-자료형" class="headerlink" title="텐서 자료형"></a>텐서 자료형</h3><ul>
<li>Tensor 클래스로 구현된 텐서는 NumPy의 다차원배열 ndarray 클래스처럼 다차원 배열 정보를 다루기 위한 자료형</li>
<li>ndarray와 다른 점은 ndarray는 직접 데이터를 저장하기 위한 자료형이지만 Tensor 클래스는 텐서플로의 계산 그래프 안에서 다차원 데이터를 표현하는 객체라는 차이</li>
</ul>
<h3 id="텐서-자료형의-종류"><a href="#텐서-자료형의-종류" class="headerlink" title="텐서 자료형의 종류"></a>텐서 자료형의 종류</h3><ul>
<li>상수형(Constant): 미리 주어진 값으로 고정된 텐서</li>
<li>변수형(Variable): 세션 내에서 값이 바뀔 수 있는 텐서</li>
<li>플레이스홀더(Placeholder): 고정된 값을 갖지만 값이 미리 주어지지 않고 나중에 넣을 수 있음</li>
</ul>
<h3 id="상수형-텐서"><a href="#상수형-텐서" class="headerlink" title="상수형 텐서"></a>상수형 텐서</h3><ul>
<li>숫자나 배열을 <code>tf.constant()</code>를 이용해 상수형 텐서 객체 생성하기</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">c_tensor = tf.constant(c)</span><br><span class="line">print(c_tensor, type(c_tensor))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(&quot;Const_1:0&quot;, shape=(7,), dtype=int32) &lt;class &apos;tensorflow.python.framework.ops.tensor&apos;&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>다양한 메서드를 활용해 상수형 텐서 생성하기<ul>
<li>대부분의 메서드에서는 배열의 크기를 지정하는 shape 또는 데이터 자료형을 지정하는 dtype 인수를 받음</li>
<li>shape 인수는 (행 넘버, 열 넘버) 형태의 튜플이나 리스트로 전달함</li>
<li>dtype 인수는 지정하지 않을 경우 tf.float32 자료형 즉, 32비트 부동소수점 자료형을 사용함</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c_tensor = tf.constant(c)</span><br><span class="line">print(c_tensor)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Tensor(&quot;Const_9:0&quot;, shape=(7,), dtype=int32)</span><br></pre></td></tr></table></figure>
<p><code>tf.zeros(shape)</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zero_tensor = tf.zeros((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">zero_tensor.eval()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0., 0., 0.],</span><br><span class="line">       [0., 0., 0.]], dtype=float32)</span><br></pre></td></tr></table></figure>
<p><code>tf.ones(shape)</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ones_tensor = tf.ones((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">ones_tensor.eval()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[1., 1., 1.],</span><br><span class="line">       [1., 1., 1.]], dtype=float32)</span><br></pre></td></tr></table></figure>
<p><code>tf.fill(shape, value)</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">filled_tensor = tf.fill((<span class="number">2</span>, <span class="number">3</span>), <span class="number">2</span>)</span><br><span class="line">filled_tensor.eval()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[2, 2, 2],</span><br><span class="line">       [2, 2, 2]], dtype=int32)</span><br></pre></td></tr></table></figure>
<p><code>tf.zeros_like(tensor)</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.zeros_like(filled_tensor).eval()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0, 0, 0],</span><br><span class="line">       [0, 0, 0]], dtype=int32)</span><br></pre></td></tr></table></figure>
<p><code>tf.ones_like(tensor)</code><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.ones_like(filled_tensor).eval()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0, 0, 0],</span><br><span class="line">       [0, 0, 0]], dtype=int32)</span><br></pre></td></tr></table></figure>
<ul>
<li>range 메서드를 사용해 열(sequence)로 구성된 상수형 텐서 만들기</li>
</ul>
<p><code>tf.range(start, limit=None, delta=1)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.range(<span class="number">5</span>).eval()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0, 1, 2, 3, 4], dtype=int32)</span><br></pre></td></tr></table></figure>
<ul>
<li>linspace 메서드를 사용해 열(sequence)로 구성된 상수형 텐서 만들기<ul>
<li>start 값이 부동소수점이 되도록 함</li>
</ul>
</li>
</ul>
<p><code>tf.linspace(start, stop, num)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.linspace(<span class="number">0.0</span>, <span class="number">5</span>, <span class="number">11</span>).eval()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. ],</span><br><span class="line">      dtype=float32)</span><br></pre></td></tr></table></figure>
<ul>
<li>랜덤한 값을 가지는 상수형 텐서 자료형 만들기</li>
</ul>
<p><code>tf.random_uniform(shape, minval=0, maxval=None, seed=None)</code><br><br><code>tf.random_normal</code><br><br><code>tf.truncated_normal</code><br><br><code>tf.random_shuffle</code><br><br><code>tf.random_crop</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.random_uniform((<span class="number">2</span>, <span class="number">3</span>), seed=<span class="number">0</span>).eval()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.10086262, 0.9701668 , 0.8487642 ],</span><br><span class="line">       [0.04828131, 0.04852307, 0.77747464]], dtype=float32)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>convert_to_tensor</code> 함수로 NumPy ndarray 자료형 변환하기</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np_array = np.arange(<span class="number">10</span>)</span><br><span class="line">tf.convert_to_tensor(np_array).eval()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span><br></pre></td></tr></table></figure>
<h3 id="플레이스홀더"><a href="#플레이스홀더" class="headerlink" title="플레이스홀더"></a>플레이스홀더</h3><ul>
<li><code>tf.placeholder</code> 명령으로 구현</li>
<li>플레이스홀더(Placeholder) 텐서 자료형은 상수형 텐서와 같은 역할을 하지만 크기만 설정하고 값은 미리 주지 않음</li>
<li>플레이스홀더의 값은 session을 사용한 그래프의 연산 중에 설정할 수 있음</li>
<li>신경망 모형의 경우 대부분 배치(batch) 단위의 학습이 이루어지기 때문에 학습용 데이터는 플레이스 홀더에 넣음</li>
<li>플레이스홀더는 데이터의 타입과 크기를 인수로 설정해 생성하고, session을 실행할 때 feed_dict 인수로 플레이스홀더에 데이터를 지정함</li>
<li>플레이스홀더에 들어가는 데이터 크기의 달라질 때는 shape 인수를 설정할 때 달라지는 차원의 값을 정수가 아닌 None으로 설정</li>
<li>다음은 플레이스홀더 x의 열의 갯수는 10개이지만 행의 갯수는 미정인 경우</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(dtype=tf.int32, shape=(<span class="keyword">None</span>, <span class="number">10</span>))  <span class="comment"># 행의 갯수는 미정, 열의 갯수는 10개</span></span><br><span class="line">value = np.arange(<span class="number">30</span>).reshape(<span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    x_assigned = sess.run(x, feed_dict=&#123;x: value&#125;)</span><br><span class="line">    print(x_assigned)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[ 0  1  2  3  4  5  6  7  8  9]</span><br><span class="line"> [10 11 12 13 14 15 16 17 18 19]</span><br><span class="line"> [20 21 22 23 24 25 26 27 28 29]]</span><br></pre></td></tr></table></figure>
<h3 id="변수형-텐서"><a href="#변수형-텐서" class="headerlink" title="변수형 텐서"></a>변수형 텐서</h3><ul>
<li><code>tf.Variable</code> 클래스로 구현</li>
<li>session 내에서 값이 변할 수 있음</li>
<li>변수형 텐서를 선언하려면 초기값으로 설정할 값을 입력</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var = tf.Variable(tf.zeros((<span class="number">2</span>, <span class="number">3</span>)))</span><br><span class="line">var</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.variable &apos;variable:0&apos; shape=&quot;(2,&quot; 3) dtype=&quot;float32_ref&quot;&gt;</span><br></pre></td></tr></table></figure>
<h3 id="초기화-연산자"><a href="#초기화-연산자" class="headerlink" title="초기화 연산자"></a>초기화 연산자</h3><ul>
<li>변수형 텐서를 생성할 때 입력한 초기값은 <code>initial_value</code> 라는 속성에 저장되고 아직 해당 변수형 텐서의 값으로 할당되어 있지 않음 ➜ 이 상태에서 바로 <code>eval</code>을 실행하면 에러 발생</li>
<li>초기화 연산자는 세션에 변수형 텐서가 들어간 다음에 변수형 텐서의 초기값을 변수형 텐서의 값으로 할당하는 추가 작업을 수행</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var.initial_value</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;tf.tensor &apos;zeros_1:0&apos; shape=&quot;(2,&quot; 3) dtype=&quot;float32&quot;&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>각 변수에는 <code>initializer</code>라는 이름의 초기화 메서드가 있어 이를 세션에서 실행시키면 초기값이 변수로 할당됨</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">interactive_sess.run(var.initializer)</span><br><span class="line">var.eval()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0., 0., 0.],</span><br><span class="line">       [0., 0., 0.]], dtype=float32)</span><br></pre></td></tr></table></figure>
<ul>
<li>그래프 내의 모든 변수의 초기화 연산을 한꺼번에 수행하려면 <code>tf.global_variables_initializer()</code> 함수를 사용</li>
<li>모든 변수를 생성하기 전에 <code>tf.global_variable_initializer()</code> 함수를 실행하면면 나중에 생성된 변수에 대해서는 초기화 연산이 이루지지 않아 에러가 발생</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">global_init = tf.global_variables_initializer()</span><br><span class="line">interactive_sess.run(global_init)</span><br><span class="line">print(var.eval())</span><br><span class="line">var1 = var + <span class="number">2</span></span><br><span class="line">print(var1.eval())</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[0. 0. 0.]</span><br><span class="line"> [0. 0. 0.]]</span><br><span class="line">[[2. 2. 2.]</span><br><span class="line"> [2. 2. 2.]]</span><br></pre></td></tr></table></figure>
<h3 id="name-속성과-이름-공간"><a href="#name-속성과-이름-공간" class="headerlink" title="name 속성과 이름 공간"></a>name 속성과 이름 공간</h3><ul>
<li>모든 텐서는 <code>op.name</code> 속성에 이름(name) 문자열을 가지고 있음</li>
<li>텐서를 생성할 때 name 인수를 사용하면 각 텐서에 이름을 수동으로 할당할 수 있음</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v1 = tf.Variable(<span class="number">2</span>, name=<span class="string">"v1"</span>)</span><br><span class="line">v2 = tf.add(v1, <span class="number">3</span>, name=<span class="string">"v2"</span>)</span><br><span class="line">print(v1.op.name)</span><br><span class="line">print(v2.op.name)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">v1</span><br><span class="line">v2</span><br></pre></td></tr></table></figure>
<ul>
<li>name 속성을 주지 않으면 자동 생성</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v3 = tf.constant(<span class="number">1</span>)</span><br><span class="line">v4 = tf.Variable(<span class="number">1</span>)</span><br><span class="line">print(v3.op.name)</span><br><span class="line">print(v4.op.name)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Const_3</span><br><span class="line">Variable_1</span><br></pre></td></tr></table></figure>
<ul>
<li>노드 이름은 중복되지 않아야 함</li>
<li>이름 공간(name scope)를 사용해 중복 방지</li>
<li><code>tf.name_scope()</code> 문맥 안에서 변수를 생성하면 이름 앞에 이름공간 문자열이 추가됨</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"scope1"</span>):</span><br><span class="line">    v5 = tf.Variable(<span class="number">2</span>, name=<span class="string">"v5"</span>)</span><br><span class="line">    v6 = tf.add(v5, <span class="number">3</span>, name=<span class="string">"v6"</span>)</span><br><span class="line">print(v5.op.name)</span><br><span class="line">print(v6.op.name)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scope1/v5</span><br><span class="line">scope1/v6</span><br></pre></td></tr></table></figure>
<hr>
<p>강의자료 분량이 많아 ‘[FC] Tensorflow 패키지 소개(2)’에서 이어가겠습니다. </p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/datascienceschool/">datascienceschool</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/fastcampus-Dr-Kim-datascienceschool/">fastcampus, Dr.Kim, datascienceschool</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/3/">Next</a>
  </nav>

</section>
          <aside id="sidebar">
  
    <div class="widget-wrap" style="margin: 20px 0;">
	<div id="search-form-wrap">

    <form class="search-form">
        <label style="width: 75%;">
            <span class="screen-reader-text">Search for:</span>
            <input type="search" class="search-field" style="height: 42px;" placeholder=" Search…" value="" name="s" title="Search for:">
        </label>
        <input type="submit" class="search-form-submit" value="Search">
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something...">
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
</div>
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Connect With Us</h3>
    <div class="widget widget_athemes_social_icons">

    	<ul class="clearfix widget-social-icons">   
    	
          
     			  <li><a href="https://github.com/foxsayy" title="Github"><i class="fa fa-github" aria-hidden="true"></i></a></li> 
          
   		
          
     			  <li><a href="https://www.instagram.com/roh.sng.hwan/?hl=ko" title="Instagram"><i class="fa fa-instagram" aria-hidden="true"></i></a></li> 
          
   		
          
            <li><a href="mailto:roh.sng.hwan@gmail.com?subject=请联系我&body=我能帮你什么" title="email"><i class="fa fa-envelope" aria-hidden="true"></i></a></li> 
          
   		
   		</ul>


   		<!--
   		<ul class="clearfix widget-social-icons">   		
   		<li class="widget-si-twitter"><a href="http://twitter.com" title="Twitter"><i class="ico-twitter"></i></a></li> 
		<li class="widget-si-facebook"><a href="http://facebook.com" title="Facebook"><i class="ico-facebook"></i></a></li>
			<li class="widget-si-gplus"><a href="http://plus.google.com" title="Google+"><i class="ico-gplus"></i></a></li>
			<li class="widget-si-pinterest"><a href="http://pinterest.com" title="Pinterest"><i class="ico-pinterest"></i></a></li>
			<li class="widget-si-flickr"><a href="http://flickr.com" title="Flickr"><i class="ico-flickr"></i></a></li>
			<li class="widget-si-instagram"><a href="http://instagram.com" title="Instagram"><i class="ico-instagram"></i></a></li>
		</ul> -->

    </div>
  </div>


  
    
  <div class="widget_athemes_tabs">
    <ul id="widget-tab" class="clearfix widget-tab-nav">
      <li class="active"><a>Recent Posts</a></li>
    </ul>
    <div class="widget">
      <ul>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/02/20/Python-딕셔너리-key와-value-뒤집기/">[Python] 딕셔너리 key와 value 뒤집기</a></h6>
              <span>February 20, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/02/15/Deep-Learning-with-Python-Ch-06/">Deep Learning with Python - Ch.06</a></h6>
              <span>February 15, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/02/15/Deep-Learning-with-Python-Ch-05/">Deep Learning with Python - Ch.05</a></h6>
              <span>February 15, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/02/14/Deep-Learning-with-Python-Ch-04/">Deep Learning with Python - Ch.04</a></h6>
              <span>February 14, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/02/06/Deep-Learning-with-Python-Ch-03/">Deep Learning with Python - Ch.03</a></h6>
              <span>February 6, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/01/28/Deep-Learning-with-Python-Ch-02/">Deep Learning with Python - Ch.02</a></h6>
              <span>January 28, 2019</span>
            </div>

          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/DSBooks/">DSBooks</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/MySQL/">MySQL</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NoSQL/">NoSQL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/The-Economist/">The Economist</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/books/">books</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/datascienceschool/">datascienceschool</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/패턴-인식과-머신-러닝/">패턴 인식과 머신 러닝</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>

    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">16</span></li></ul>
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2019 THE DATASCIENTIST All Rights Reserved.
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hiero" target="_blank">hiero</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var contentdiv = document.getElementById("content");

    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
</script>

<!-- Custome JS -->
<script src="/js/my.js"></script>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories/index.html" class="mobile-nav-link">Categories</a>
  
    <a href="/log/index.html" class="mobile-nav-link">Log</a>
  
    <a href="/about/index.html" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.js"></script>


<script src="/js/scripts.js"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>








  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
