<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">

  <!-- PACE Progress Bar START -->
  
    <script src="https://raw.githubusercontent.com/HubSpot/pace/v1.0.2/pace.min.js"></script>
    <link rel="stylesheet" href="https://github.com/HubSpot/pace/raw/master/themes/orange/pace-theme-flash.css">
  
  

  <!-- PACE Progress Bar START -->

  
  <title>THE DATASCIENTIST</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  
  <meta property="og:type" content="website">
<meta property="og:title" content="THE DATASCIENTIST">
<meta property="og:url" content="http://foxsayy.github.io/index.html">
<meta property="og:site_name" content="THE DATASCIENTIST">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="THE DATASCIENTIST">
  
    <link rel="alternate" href="/atom.xml" title="THE DATASCIENTIST" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="https://cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/hiero.css">
  <link rel="stylesheet" href="/css/glyphs.css">
  

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/my.css">
  <!-- Google Adsense -->
  
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-0123456789ABCDEF",
          enable_page_level_ads: true
      });
  </script>
  
</head>
</html>
<script>
var themeMenus = {};

  themeMenus["/"] = "Home"; 

  themeMenus["/archives"] = "Archives"; 

  themeMenus["/categories/index.html"] = "Categories"; 

  themeMenus["/log/index.html"] = "log"; 

  themeMenus["/about/index.html"] = "About"; 

</script>


  <body>


  <header id="allheader" class="site-header" role="banner">
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="THE DATASCIENTIST" rel="home"> THE DATASCIENTIST </a>
            
          </h1>

          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>
            <div class="clearfix sf-menu">

              <ul id="main-nav" class="nmenu sf-js-enabled">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">Home</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">Archives</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories/index.html">Categories</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/log/index.html">log</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about/index.html">About</a> </li>
                    
              </ul>
            </div>
          </nav>


      </div>
  </div>
</header>


  <div id="originBgDiv" style="background: #fff; width: 100%;">

      <div style="max-height:600px; overflow: hidden;  display: flex; display: -webkit-flex; align-items: center;">
        <img id="originBg" width="100%" alt="" src="">
      </div>

  </div>

  <script>
  function setAboutIMG(){
      var imgUrls = "css/images/pose.jpg,https://source.unsplash.com/collection/954550/1920x1080".split(",");
      var random = Math.floor((Math.random() * imgUrls.length ));
      if (imgUrls[random].startsWith('http') || imgUrls[random].indexOf('://') >= 0) {
        document.getElementById("originBg").src=imgUrls[random];
      } else {
        document.getElementById("originBg").src='/' + imgUrls[random];
      }
  }
  bgDiv=document.getElementById("originBgDiv");
  if(location.pathname.match('about')){
    setAboutIMG();
    bgDiv.style.display='block';
  }else{
    bgDiv.style.display='none';
  }
  </script>



  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main">
  
    <article id="post-Deep-Learning-with-Python-Ch-03" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/02/06/Deep-Learning-with-Python-Ch-03/">Deep Learning with Python - Ch.03</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/02/06/Deep-Learning-with-Python-Ch-03/" class="article-date">
	  <time datetime="2019-02-05T17:02:23.000Z" itemprop="datePublished">February 6, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p><a href="https://ridibooks.com/v2/Detail?id=754024868&amp;_s=search&amp;_q=%EC%BC%80%EB%9D%BC%EC%8A%A4" target="_blank" rel="noopener">케라스 창시자에게 배우는 딥러닝</a>을 실습하면서 정리한 포스트입니다. 코드 예제와 코드 설명은 <a href="https://github.com/rickiepark/deep-learning-with-python-notebooks" target="_blank" rel="noopener">역자 깃허브</a>에서 받아볼 수 있습니다. 출판물이고 개인적으로만 참고하기 위한 요약노트이다 보니 설명이 불친절한 점은 양해 바랍니다. 보다 자세한 내용을 원하시는 분은 위 링크의 책을 참고하시기 바랍니다.</p>
<hr>
<h2 id="3장-신경망-시작하기"><a href="#3장-신경망-시작하기" class="headerlink" title="3장. 신경망 시작하기"></a>3장. 신경망 시작하기</h2><h3 id="신경망-구조"><a href="#신경망-구조" class="headerlink" title="신경망 구조"></a>신경망 구조</h3><ul>
<li>네트워크(모델)를 구성하는 층</li>
<li>입력데이터, 타겟</li>
<li>손실 함수: 피드백 신호를 정의</li>
<li>옵티마이저: 학습 진행 방식을 결정</li>
</ul>
<h4 id="층-딥러닝의-구성-단위"><a href="#층-딥러닝의-구성-단위" class="headerlink" title="층: 딥러닝의 구성 단위"></a>층: 딥러닝의 구성 단위</h4><ul>
<li>층: 하나 이상의 텐서를 입력받아 하나 이상의 텐서를 출력하는 데이터 처리 모듈</li>
<li>대부분 가중치라는 층의 상태를 가짐(상태가 없는 층도 존재)</li>
<li>가중치는 확률적 경사 하강법에 의해 학습되는 하나 이상의 텐서</li>
<li>층마다 적절한 텐서 포맷과 데이터 처리 방식이 다름<ul>
<li>벡터 데이터(2D 텐서) : 밀집 연결 층</li>
<li>시퀀스 데이터(3D 텐서) : LSTM 같은 순환 층</li>
<li>이미지 데이터(4D 텐서) : Conv2D 클래스. 2D 합성곱 층</li>
</ul>
</li>
<li>케라스는 호환 가능한 층(호환성: 각 층이 특정 크기의 입력 텐서만 받고 특정 크기의 출력 텐서를 반환)을 엮어 데이터 변환 파이프라인을 구성해 딥러딩 모델을 만듦</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 첫 번째 차원이 784인 2D 텐서만 입력으로 받는 층</span></span><br><span class="line"><span class="comment"># 첫 번째 차원 크기가 32로 변환된 텐서를 출력</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line">layer = layers.Dense(<span class="number">32</span>, input_shape=(<span class="number">784</span>,))</span><br></pre></td></tr></table></figure>
<ul>
<li>위 층에는 32차원의 벡터를 입력으로 받는 하위 층이 연결돼야 함</li>
<li>케라스가 모델에 추가된 층을 자동으로 상위 층에 맞춰줌</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>, input_shape=(<span class="number">784</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>두 번째 층에는 input_shape 매개변수를 지정하지 않음(앞 층의 출력 크기를 입력 크기로 자동으로 채택)</li>
</ul>
<h4 id="모델-층의-네트워크"><a href="#모델-층의-네트워크" class="headerlink" title="모델: 층의 네트워크"></a>모델: 층의 네트워크</h4><ul>
<li>딥러닝 모델은 층으로 만든 Directed Acyclic Graph(DAG)</li>
<li>자주 등장하는 네트워크 구조<ul>
<li>branch가 2개인 네트워크</li>
<li>출력이 여러 개인 네트워크</li>
<li>inception 블록</li>
</ul>
</li>
<li>네트워크 구조는 가설 공간(가능성 있는 공간)을 정의</li>
<li>네트워크 구조 선택 : 가설 공간을 입력 데이터에서 출력 데이터로 매핑하는 일련의 특정 텐서 연산으로 제한</li>
<li>딱 맞는 네트워크 구조 찾기는 과학보다 예술</li>
<li>네트워크 구조 정의 후에는 손실함수와 옵티마이저를 선택해야 함</li>
</ul>
<h4 id="손실-함수와-옵티마이저-학습-과정을-조절하는-열쇠"><a href="#손실-함수와-옵티마이저-학습-과정을-조절하는-열쇠" class="headerlink" title="손실 함수와 옵티마이저: 학습 과정을 조절하는 열쇠"></a>손실 함수와 옵티마이저: 학습 과정을 조절하는 열쇠</h4><ul>
<li>손실 함수: 훈련하는 동안 최소화될 값. 문제에 대한 성공 지표</li>
<li>옵티마이저: 손실 함수를 기반으로 네트워크가 어떻게 업데이트될지 결정.</li>
<li>출력 여러 개를 내는 신경망은 여러 개의 손실 함수를 가질 수 있음</li>
<li>But, 경사 하강법 과정은 하나의 스칼라 손실 값을 기준으로 함</li>
<li>따라서 손실이 여러 개인 네트워크에서는 모든 손실이 (평균을 내서) 하나의 스칼라 양으로 합쳐짐</li>
<li>문제에 따라 올바른 목적 함수 선택해야<ul>
<li>2개의 클래스 분류 문제 : binary crossentropy</li>
<li>여러개 클래스 분류 문제 : categorical crossentropy</li>
<li>회귀 문제 : 평균 제곱 오차</li>
<li>시퀀스 학습 문제 : Connection Temporal Classification</li>
<li>완전히 새로운 연구: 독자적인 목적 함수</li>
</ul>
</li>
</ul>
<h3 id="케라스-소개"><a href="#케라스-소개" class="headerlink" title="케라스 소개"></a>케라스 소개</h3><ul>
<li>생략</li>
</ul>
<h3 id="딥러닝-컴퓨터-셋팅"><a href="#딥러닝-컴퓨터-셋팅" class="headerlink" title="딥러닝 컴퓨터 셋팅"></a>딥러닝 컴퓨터 셋팅</h3><ul>
<li>생략</li>
</ul>
<h3 id="영화-리뷰-분류-이진-분류-예제"><a href="#영화-리뷰-분류-이진-분류-예제" class="headerlink" title="영화 리뷰 분류: 이진 분류 예제"></a>영화 리뷰 분류: 이진 분류 예제</h3><p>리뷰 텍스트를 기반으로 영화 리뷰를 긍정과 부정으로 분류</p>
<h4 id="IMDB-데이터셋"><a href="#IMDB-데이터셋" class="headerlink" title="IMDB 데이터셋"></a>IMDB 데이터셋</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>num_words=10000 : 트레이닝셋에서 가장 많이 등장하는 단어 1만 개만 사용</li>
<li>labels는 0(부정)과 1(긍정)을 나타내는 리스트</li>
</ul>
<h4 id="데이터-준비"><a href="#데이터-준비" class="headerlink" title="데이터 준비"></a>데이터 준비</h4><ul>
<li>숫자 리스트를 신경망에 넣기 위해 텐서로 바꾸는 두 가지 방법<ul>
<li>리스트에 padding을 추가하고 (samples, sequence-length) 크기의 정수 텐서로 변환. 신경망 첫 번째 층으로 사용</li>
<li>리스트를 원핫인코딩해 0, 1 벡터로 변환. (아래 예시)</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 정수 시퀀스를 이진 행렬로 인코딩</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    <span class="comment"># (시퀀스 길이, 차원) 크기의 0행렬 만들기</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># results[i]에서 특정 인덱스 위치를 1로 바꾸기</span></span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터를 벡터로 변환</span></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br></pre></td></tr></table></figure>
<p>x_train.shape, x_test.shape는 각각 (25000, 10000) 모양이 됨</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 라벨을 벡터로 바꾸기</span></span><br><span class="line">y_train = np.asarray(train_labels).astype(<span class="string">'float32'</span>)</span><br><span class="line">y_test = np.asarray(test_labels).astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="신경망-모델-만들기"><a href="#신경망-모델-만들기" class="headerlink" title="신경망 모델 만들기"></a>신경망 모델 만들기</h4><ul>
<li>입력 데이터는 벡터, 라벨은 1 or 0의 스칼라</li>
<li>이런 문제에 잘 작동하는 네트워크는 relu 활성화 함수를 사용한 완전 연결 층(Dense(16, activation=’relu’))을 그냥 쌓은 것</li>
<li>16은 은닉 유닛의 수. 하나의 은닉 유닛은 층이 나타내는 표현 공간에서 하나의 차원이 됨.</li>
<li><code>output = relu(dot(W, input) + b)</code></li>
<li>16개 은닉 유닛이 있다는 건 가중치 행렬 W의 크기가 (input_dimension, 16)이라는 의미. 입력 데이터와 W를 점곱하면 입력 데이터가 16차원으로 표현된 공간으로 투영됨(+ 편향 벡터 b를 더하고 relu 연산 적용)</li>
<li>표현공간의 차원: ‘신경망이 내재된 표현을 학습할 때 가질 수 있는 자유도’</li>
<li>중간의 은닉 층은 활성화 함수로 relu를, 마지막 층은 확률을 출력하기 위해 시그모이드 활성화 함수를 사용.</li>
<li>relu는 음수를 0으로 만드는 함수. 시그모이드는 임의의 값을 [0, 1] 사이로 압축-&gt; 출력 값을 확률처럼 해석 가능</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 위 신경망의 케라스 구현</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>이진 분류 문제고 신경망 출력이 확률 -&gt; binary_crossentropy 나 mean_squared_error</li>
<li>binary_crossentropy<ul>
<li>확률을 출력하는 모델 사용 시 최선의 선택</li>
<li>크로스엔트로피: 확률 분포 간의 차이를 측정(여기선 원본 분포와 예측 분포 사이를 측정)</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델 컴파일하기</span></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>옵티마이저의 매개변수를 바꾸거나 손실함수, 측정함수를 직접 만들어야 할 경우는 아래와 같이 설정</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 옵티마이저 설정하기</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line">model.compile(optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 손실, 측정함수 객체로 지정하기</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> losses</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">             loss=losses.binary_crossentropy,</span><br><span class="line">             metrics=[metrics.binary_accuracy])</span><br></pre></td></tr></table></figure>
<h4 id="훈련-검증"><a href="#훈련-검증" class="headerlink" title="훈련 검증"></a>훈련 검증</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 원본 훈련 데이터에서 1만개 샘플 떼어내 검증 셋 만들기</span></span><br><span class="line">x_val = x_train[:<span class="number">10000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">10000</span>:]</span><br><span class="line">v_val = y_train[:<span class="number">10000</span>]</span><br><span class="line">partial_y_train = y_train[<span class="number">10000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 512개 샘플씩 미니 배치를 만들어 20번의 에포크 동안 훈련</span></span><br><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                    partial_y_train,</span><br><span class="line">                    epochs=<span class="number">20</span>,</span><br><span class="line">                    batch_size=<span class="number">512</span>,</span><br><span class="line">                    validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">Train on 15000 samples, validate on 10000 samples</span><br><span class="line">Epoch 1/20</span><br><span class="line">15000/15000 [==============================] - 3s 200us/step - loss: 0.5084 - acc: 0.7810 - val_loss: 0.3798 - val_acc: 0.8683</span><br><span class="line">Epoch 2/20</span><br><span class="line">15000/15000 [==============================] - 1s 95us/step - loss: 0.3005 - acc: 0.9043 - val_loss: 0.3002 - val_acc: 0.8901</span><br><span class="line">Epoch 3/20</span><br><span class="line">15000/15000 [==============================] - 1s 95us/step - loss: 0.2179 - acc: 0.9289 - val_loss: 0.3083 - val_acc: 0.8711</span><br><span class="line">Epoch 4/20</span><br><span class="line">15000/15000 [==============================] - 2s 124us/step - loss: 0.1750 - acc: 0.9437 - val_loss: 0.2843 - val_acc: 0.8835</span><br><span class="line">Epoch 5/20</span><br><span class="line">15000/15000 [==============================] - 1s 95us/step - loss: 0.1426 - acc: 0.9542 - val_loss: 0.2842 - val_acc: 0.8870</span><br><span class="line">Epoch 6/20</span><br><span class="line">15000/15000 [==============================] - 1s 94us/step - loss: 0.1150 - acc: 0.9653 - val_loss: 0.3154 - val_acc: 0.8772</span><br><span class="line">Epoch 7/20</span><br><span class="line">15000/15000 [==============================] - 1s 95us/step - loss: 0.0978 - acc: 0.9709 - val_loss: 0.3129 - val_acc: 0.8846</span><br><span class="line">Epoch 8/20</span><br><span class="line">15000/15000 [==============================] - 1s 94us/step - loss: 0.0807 - acc: 0.9765 - val_loss: 0.3857 - val_acc: 0.8650</span><br><span class="line">Epoch 9/20</span><br><span class="line">15000/15000 [==============================] - 2s 107us/step - loss: 0.0660 - acc: 0.9820 - val_loss: 0.3636 - val_acc: 0.8782</span><br><span class="line">Epoch 10/20</span><br><span class="line">15000/15000 [==============================] - 2s 134us/step - loss: 0.0561 - acc: 0.9849 - val_loss: 0.3844 - val_acc: 0.8793</span><br><span class="line">Epoch 11/20</span><br><span class="line">15000/15000 [==============================] - 2s 137us/step - loss: 0.0436 - acc: 0.9899 - val_loss: 0.4151 - val_acc: 0.8783</span><br><span class="line">Epoch 12/20</span><br><span class="line">15000/15000 [==============================] - 2s 117us/step - loss: 0.0379 - acc: 0.9920 - val_loss: 0.4542 - val_acc: 0.8684</span><br><span class="line">Epoch 13/20</span><br><span class="line">15000/15000 [==============================] - 2s 109us/step - loss: 0.0300 - acc: 0.9929 - val_loss: 0.4703 - val_acc: 0.8728</span><br><span class="line">Epoch 14/20</span><br><span class="line">15000/15000 [==============================] - 2s 125us/step - loss: 0.0247 - acc: 0.9945 - val_loss: 0.5042 - val_acc: 0.8718</span><br><span class="line">Epoch 15/20</span><br><span class="line">15000/15000 [==============================] - 2s 132us/step - loss: 0.0192 - acc: 0.9964 - val_loss: 0.5316 - val_acc: 0.8704</span><br><span class="line">Epoch 16/20</span><br><span class="line">15000/15000 [==============================] - 2s 109us/step - loss: 0.0164 - acc: 0.9969 - val_loss: 0.5650 - val_acc: 0.8690</span><br><span class="line">Epoch 17/20</span><br><span class="line">15000/15000 [==============================] - 1s 99us/step - loss: 0.0125 - acc: 0.9981 - val_loss: 0.5973 - val_acc: 0.8668</span><br><span class="line">Epoch 18/20</span><br><span class="line">15000/15000 [==============================] - 2s 108us/step - loss: 0.0108 - acc: 0.9983 - val_loss: 0.6285 - val_acc: 0.8670</span><br><span class="line">Epoch 19/20</span><br><span class="line">15000/15000 [==============================] - 2s 109us/step - loss: 0.0079 - acc: 0.9991 - val_loss: 0.7197 - val_acc: 0.8553</span><br><span class="line">Epoch 20/20</span><br><span class="line">15000/15000 [==============================] - 1s 100us/step - loss: 0.0048 - acc: 0.9998 - val_loss: 0.6812 - val_acc: 0.8674</span><br></pre></td></tr></table></figure>
<ul>
<li>model.fit() 메서드는 History 객체를 반환</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 훈련과 검증 손실</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">history_dict = history.history</span><br><span class="line">loss = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'훈련과 검증 손실'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/Deep Learning with Python - Ch.03_01.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 훈련과 검증 정확도</span></span><br><span class="line">plt.clf()</span><br><span class="line">acc = history_dict[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history_dict[<span class="string">'val_acc'</span>]</span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'훈련과 검증 정확도'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/Deep Learning with Python - Ch.03_02.png" alt=""></p>
<ul>
<li>훈련 손실은 에포크마다 감소, 훈련 정확도는 에포크마다 증가</li>
<li>트레이닝셋에서 잘 작동하지만 테스트셋에서는 아님(overfitting 됐기 때문)</li>
<li>오버피팅을 막기 위해 세번째 에포크 이후 훈련을 중지</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 처음부터 다시 훈련하기</span></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">             loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">4</span>, batch_size=<span class="number">512</span>)</span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/4</span><br><span class="line">25000/25000 [==============================] - 3s 102us/step - loss: 0.4749 - acc: 0.8216</span><br><span class="line">Epoch 2/4</span><br><span class="line">25000/25000 [==============================] - 2s 76us/step - loss: 0.2659 - acc: 0.9096</span><br><span class="line">Epoch 3/4</span><br><span class="line">25000/25000 [==============================] - 2s 70us/step - loss: 0.1983 - acc: 0.9298</span><br><span class="line">Epoch 4/4</span><br><span class="line">25000/25000 [==============================] - 2s 67us/step - loss: 0.1678 - acc: 0.9403</span><br><span class="line">25000/25000 [==============================] - 3s 140us/step</span><br><span class="line">[0.3244189430713654, 0.87316]</span><br></pre></td></tr></table></figure>
<ul>
<li>87%의 정확도 달성</li>
</ul>
<h4 id="훈련된-모델로-새로운-데이터에-대해-예측하기"><a href="#훈련된-모델로-새로운-데이터에-대해-예측하기" class="headerlink" title="훈련된 모델로 새로운 데이터에 대해 예측하기"></a>훈련된 모델로 새로운 데이터에 대해 예측하기</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.predict(x_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">array([[0.23489225],</span><br><span class="line">       [0.99956626],</span><br><span class="line">       [0.95799285],</span><br><span class="line">       ...,</span><br><span class="line">       [0.16514498],</span><br><span class="line">       [0.11655141],</span><br><span class="line">       [0.74928373]], dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="뉴스-기사-분류-다중-분류-문제"><a href="#뉴스-기사-분류-다중-분류-문제" class="headerlink" title="뉴스 기사 분류: 다중 분류 문제"></a>뉴스 기사 분류: 다중 분류 문제</h3><ul>
<li>로이터 뉴스를 46개 토픽으로 분류하는 신경망 만들기</li>
<li>각 데이터가 하나의 카테고리로 분류되는 단일 레이블 다중 분류 문제</li>
<li>각 데이터가 여러 개의 카테고리에 속할 수 있다면 다중 레이블 다중 분류 문제</li>
</ul>
<h4 id="로이터-데이터셋"><a href="#로이터-데이터셋" class="headerlink" title="로이터 데이터셋"></a>로이터 데이터셋</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 케라스에서 데이터셋 불러오기</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> reuters</span><br><span class="line"></span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>num_words=10000</code> : 데이터에서 가장 자주 등장하는 단어 10000개로 제한</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 샘플 수 확인하기</span></span><br><span class="line">len(train_data), len(test_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(8982, 2246)</span><br></pre></td></tr></table></figure>
<ul>
<li>트레이닝셋 8982개, 테스트셋 2246개</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">[1,</span><br><span class="line"> 227,</span><br><span class="line"> 2406,</span><br><span class="line"> 91,</span><br><span class="line"> 2,</span><br><span class="line"> 125,</span><br><span class="line"> 2855,</span><br><span class="line"> 21,</span><br><span class="line"> 4,</span><br><span class="line"> 3976,</span><br><span class="line"> 76,</span><br><span class="line"> 7,</span><br><span class="line"> 4,</span><br><span class="line"> 757,</span><br><span class="line"> 481,</span><br><span class="line"> 3976,</span><br><span class="line"> 790,</span><br><span class="line"> 5259,</span><br><span class="line"> 5654,</span><br><span class="line"> 9,</span><br><span class="line"> 111,</span><br><span class="line"> 149,</span><br><span class="line"> 8,</span><br><span class="line"> 7,</span><br><span class="line"> 10,</span><br><span class="line"> 76,</span><br><span class="line"> 223,</span><br><span class="line"> 51,</span><br><span class="line"> 4,</span><br><span class="line"> 417,</span><br><span class="line"> 8,</span><br><span class="line"> 1047,</span><br><span class="line"> 91,</span><br><span class="line"> 6917,</span><br><span class="line"> 1688,</span><br><span class="line"> 340,</span><br><span class="line"> 7,</span><br><span class="line"> 194,</span><br><span class="line"> 9411,</span><br><span class="line"> 6,</span><br><span class="line"> 1894,</span><br><span class="line"> 21,</span><br><span class="line"> 127,</span><br><span class="line"> 2151,</span><br><span class="line"> 2394,</span><br><span class="line"> 1456,</span><br><span class="line"> 6,</span><br><span class="line"> 3034,</span><br><span class="line"> 4,</span><br><span class="line"> 329,</span><br><span class="line"> 433,</span><br><span class="line"> 7,</span><br><span class="line"> 65,</span><br><span class="line"> 87,</span><br><span class="line"> 1127,</span><br><span class="line"> 10,</span><br><span class="line"> 8219,</span><br><span class="line"> 1475,</span><br><span class="line"> 290,</span><br><span class="line"> 9,</span><br><span class="line"> 21,</span><br><span class="line"> 567,</span><br><span class="line"> 16,</span><br><span class="line"> 1926,</span><br><span class="line"> 24,</span><br><span class="line"> 4,</span><br><span class="line"> 76,</span><br><span class="line"> 209,</span><br><span class="line"> 30,</span><br><span class="line"> 4033,</span><br><span class="line"> 6655,</span><br><span class="line"> 5654,</span><br><span class="line"> 8,</span><br><span class="line"> 4,</span><br><span class="line"> 60,</span><br><span class="line"> 8,</span><br><span class="line"> 4,</span><br><span class="line"> 966,</span><br><span class="line"> 308,</span><br><span class="line"> 40,</span><br><span class="line"> 2575,</span><br><span class="line"> 129,</span><br><span class="line"> 2,</span><br><span class="line"> 295,</span><br><span class="line"> 277,</span><br><span class="line"> 1071,</span><br><span class="line"> 9,</span><br><span class="line"> 24,</span><br><span class="line"> 286,</span><br><span class="line"> 2114,</span><br><span class="line"> 234,</span><br><span class="line"> 222,</span><br><span class="line"> 9,</span><br><span class="line"> 4,</span><br><span class="line"> 906,</span><br><span class="line"> 3994,</span><br><span class="line"> 8519,</span><br><span class="line"> 114,</span><br><span class="line"> 5758,</span><br><span class="line"> 1752,</span><br><span class="line"> 7,</span><br><span class="line"> 4,</span><br><span class="line"> 113,</span><br><span class="line"> 17,</span><br><span class="line"> 12]</span><br></pre></td></tr></table></figure>
<ul>
<li>각 샘플은 정수 리스트</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 텍스트로 디코딩</span></span><br><span class="line">word_index = reuters.get_word_index()</span><br><span class="line">reverse_word_index = dict((value, key) <span class="keyword">for</span> (key, value) <span class="keyword">in</span> word_index.items())</span><br><span class="line">decoded_newswire = <span class="string">' '</span>.join([reverse_word_index.get(i - <span class="number">3</span>, <span class="string">'?'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="number">-1</span>]])</span><br><span class="line">    <span class="comment"># 0, 1, 2는 각각 '패딩, 문서 시작, 사전에없음' 인덱스이므로 3을 뺌</span></span><br><span class="line">decoded_newswire</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;? currency fluctuations may ? their influence on the bullion market in the near future bullion bankers samuel montagu and co ltd said in a market report but the firm said silver may lag behind gold in any reactions to movements on foreign exchanges opec&apos;s failure to address the recent decline in oil prices remains a worrying factor however and on balance it appears that the market should be approached cautiously montagu said the bank said the us economy has shown no ? long term improvement and that both latin american debt and the iranian arms affair could undermine confidence in the dollar reuter 3&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_labels.min(), train_labels.max()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(0, 45)</span><br></pre></td></tr></table></figure>
<ul>
<li>라벨은 0과 45 사이의 정수</li>
</ul>
<h4 id="데이터-준비-1"><a href="#데이터-준비-1" class="headerlink" title="데이터 준비"></a>데이터 준비</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터를 벡터로 변환</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span>        </span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터를 벡터로 변환</span></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br></pre></td></tr></table></figure>
<ul>
<li>라벨을 벡터로 바꾸는 두 가지 방법<ul>
<li>라벨의 리스트를 정수 텐서로 변환한따</li>
<li>원 핫 인코딩(범주형 인코딩)(아래 예시)</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 케라스 내장 함수를 사용한 원핫인코딩</span></span><br><span class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</span><br><span class="line"></span><br><span class="line">one_hot_train_labels = to_categorical(train_labels)</span><br><span class="line">one_hot_test_labels = to_categorical(test_labels)</span><br></pre></td></tr></table></figure>
<h4 id="모델-구성"><a href="#모델-구성" class="headerlink" title="모델 구성"></a>모델 구성</h4><ul>
<li>출력 클래스는 46개. 규모가 작은 층을 사용하면 병목현상으로 유용한 정보를 잃게될 수 있다. 따라서 아래에선 64개 유닛을 사용한다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>마지막 Dense 층의 크기는 46. (각 입력 샘플에 대해 46차원의 벡터를 출력)</li>
<li>마지막 층의 softmax 활성화 함수 : 각 입력 샘플마다 46개 출력 클래스에 대한 확률 분포를 출력. 46개의 값을 더하면 1이 됨</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델 컴파일하기</span></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">             loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">             metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>카테고리컬 크로스엔트로피 손실함수는 두 확률 분포 사이의 거리를 측정</li>
<li>여기선 네트워크가 출력한 확률 분포와 진짜 라벨의 분포 사이의 거리를 측정</li>
<li>두 분포 사이의 거리를 좁힐수록 진짜 라벨에 가까운 출력을 내도록 훈련시킨다</li>
</ul>
<h4 id="훈련-검증-1"><a href="#훈련-검증-1" class="headerlink" title="훈련 검증"></a>훈련 검증</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 테스트셋에서 1000개 샘플을 따로 떼어내 검증셋 준비하기</span></span><br><span class="line">x_val = x_train[:<span class="number">1000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">1000</span>:]</span><br><span class="line">y_val = one_hot_train_labels[:<span class="number">1000</span>]</span><br><span class="line">partial_y_train = one_hot_train_labels[<span class="number">1000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델 트레이닝(에포크 20번)</span></span><br><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                   partial_y_train,</span><br><span class="line">                   epochs=<span class="number">20</span>,</span><br><span class="line">                   batch_size=<span class="number">512</span>,</span><br><span class="line">                   validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 훈련과 검증 손실</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'훈련 손실'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'검증 손실'</span>)</span><br><span class="line">plt.title(<span class="string">'훈련과 검증 손실'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/Deep Learning with Python - Ch.03_03.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 훈련과 검증 정확도</span></span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'훈련 정확도'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'검증 정확도'</span>)</span><br><span class="line">plt.title(<span class="string">'훈련과 검증 정확도'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'에포크'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'정확도'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/Deep Learning with Python - Ch.03_04.png" alt=""></p>
<ul>
<li>아홉 번째 에포크 이후 오버피팅이 시작됨. 에포크 9로 새로운 모델 훈련하고 테스트셋에서 평가</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(partial_x_train,</span><br><span class="line">          partial_y_train,</span><br><span class="line">          epochs=<span class="number">9</span>,</span><br><span class="line">          batch_size=<span class="number">512</span>,</span><br><span class="line">          validation_data=(x_val, y_val))</span><br><span class="line">results = model.evaluate(x_test, one_hot_test_labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">results</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 78%의 정확도</span><br><span class="line">[0.9839374910797907, 0.7858414960459524]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">test_labels_copy = copy.copy(test_labels)</span><br><span class="line">np.random.shuffle(test_labels_copy)</span><br><span class="line">float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.19100623330365094</span><br></pre></td></tr></table></figure>
<ul>
<li>무작위 분류시의 19%에 비하면 좋은 결과!</li>
</ul>
<h4 id="새로운-데이터에-대해-예측하기"><a href="#새로운-데이터에-대해-예측하기" class="headerlink" title="새로운 데이터에 대해 예측하기"></a>새로운 데이터에 대해 예측하기</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 테스트셋 예측하기</span></span><br><span class="line">predictions = model.predict(x_test)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>predictions</code><ul>
<li>각 항목은 길이가 46인 벡터</li>
<li>이 벡터의 원소의 합은 1</li>
<li>가장 큰 값이 가장 확률이 높은 클래스(<code>np.argmax</code> 사용)</li>
</ul>
</li>
</ul>
<h4 id="레이블과-손실을-다루는-다른-방법"><a href="#레이블과-손실을-다루는-다른-방법" class="headerlink" title="레이블과 손실을 다루는 다른 방법"></a>레이블과 손실을 다루는 다른 방법</h4><ul>
<li>라벨을 정수 텐서로 변환해서 인코딩 할때는 손실 함수 하나만 바꾸면 된다<ul>
<li><code>loss=sparse_categorical_crossentropy</code></li>
</ul>
</li>
</ul>
<h3 id="주택-가격-예측-회귀-문제"><a href="#주택-가격-예측-회귀-문제" class="headerlink" title="주택 가격 예측: 회귀 문제"></a>주택 가격 예측: 회귀 문제</h3>
      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/DSBooks/">DSBooks</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Keras-Deep-Learning-with-Python/">Keras, Deep Learning with Python</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Deep-Learning-with-Python-Ch-02" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/01/28/Deep-Learning-with-Python-Ch-02/">Deep Learning with Python - Ch.02</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/01/28/Deep-Learning-with-Python-Ch-02/" class="article-date">
	  <time datetime="2019-01-28T07:04:24.000Z" itemprop="datePublished">January 28, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://ridibooks.com/v2/Detail?id=754024868&amp;_s=search&amp;_q=%EC%BC%80%EB%9D%BC%EC%8A%A4" target="_blank" rel="noopener">케라스 창시자에게 배우는 딥러닝</a>을 실습하면서 정리한 포스트입니다. 코드 예제와 코드 설명은 <a href="https://github.com/rickiepark/deep-learning-with-python-notebooks" target="_blank" rel="noopener">역자 깃허브</a>에서 받아볼 수 있습니다. 출판물이고 개인적으로만 참고하기 위한 요약노트이다 보니 설명이 불친절한 점은 양해 바랍니다. 보다 자세한 내용을 원하시는 분은 위 링크의 책을 참고하시기 바랍니다.</p>
<hr>
<h2 id="2장-시작-전에-신경망의-수학적-구성-요소"><a href="#2장-시작-전에-신경망의-수학적-구성-요소" class="headerlink" title="2장. 시작 전에: 신경망의 수학적 구성 요소"></a>2장. 시작 전에: 신경망의 수학적 구성 요소</h2><h3 id="신경망과의-첫-만남"><a href="#신경망과의-첫-만남" class="headerlink" title="신경망과의 첫 만남"></a>신경망과의 첫 만남</h3><ul>
<li>MNIST 예제</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 케라스에서 MNIST 데이터셋 불러오기</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 신경망 구조</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line">network = models.Sequential()</span><br><span class="line">network.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span> * <span class="number">28</span>,)))</span><br><span class="line">network.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>신경망의 핵심 구성 요소인 층(layer)은 일종의 데이터 처리 필터</li>
<li>어떤 데이터가 들어가면 더 유용한 형태로 출력됨(층은 주어진 문제에 더 의미 있는 표현을 입력된 데이터로부터 추출함)</li>
<li>딥러닝 모델은 데이터 정제 필터(층)가 연속되어 있는 데이터 프로세싱을 위한 여과기와 같음</li>
<li>위 예시는 조밀하게 연결된 신경망 층인 Dense 층 2개가 연속된 구조의 신경망 구조</li>
<li>두 번째 층은 10개의 확률 점수가 들어 있는 배열(모두 더하면 1)을 반환하는 소프트맥스 층</li>
<li>각 점수는 현재 숫자 이미지가 10개의 숫자 클래스 중 하나에 속할 확률임</li>
<li>신경망이 훈련 준비를 마치기 위해서 컴파일 단계에 포함될 세 가지가 더 필요<ul>
<li>손실 함수 : 훈련 데이터에서 신경망의 성능을 측정하는 방법. 네트워크가 옳은 방향으로 학습될 수 있도록 도움</li>
<li>옵티마이저: 입력된 데이터와 손실 함수를 기반으로 네트워크를 업데이트하는 메커니즘입니다.</li>
<li>훈련과 테스트 과정을 모니터링할 지표 : 여기에서는 정확도(정확히 분류된 이미지의 비율)만 고려하겠습니다.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 컴파일 단계</span></span><br><span class="line">network.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">               loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">               metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>트레이닝 시작 전에 데이터를 네트워크에 맞는 크기로 바꾸고 모든 값을 0과 1 사이로 스케일을 조정</li>
<li>MNIST 이미지는 [0, 255] 사이의 값인 unit8 타입의 (60000, 28, 28) 크기의 배열로 저장돼 있음. 이를 0과 1 사이 값을 가지는 float32 타입의 (60000, 28*28) 크기인 배열로 변경</li>
<li>categorical_crossentropy: 손실 함수. 가중치 텐서를 학습하기 위한 피드백 신호로 사용. 훈련하는 동안 최소화됨</li>
<li>rmsprop : 경사 하강법 적용 방식은 이 옵티마이저에 의해 결정</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 이미지 데이터 준비하기</span></span><br><span class="line">train_images = train_images.reshape((<span class="number">60000</span>, <span class="number">28</span> * <span class="number">28</span>))</span><br><span class="line">train_images = train_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br><span class="line"></span><br><span class="line">test_images = test_images.reshape((<span class="number">10000</span>, <span class="number">28</span> * <span class="number">28</span>))</span><br><span class="line">test_images = test_images.astype(<span class="string">'float32'</span>) / <span class="number">255</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 레이블 준비_범주형으로 인코딩</span></span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line">train_labels = to_categorical(train_labels)</span><br><span class="line">test_labels = to_categorical(test_labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fit 메서드로 트레이닝 데이터에 모델을 학습시킴</span></span><br><span class="line">network.fit(train_images, train_labels, epochs=<span class="number">5</span>, batch_size=<span class="number">128</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>네트워크가 128개 샘플씩 미니 배치로 훈련 데이터를 5번 반복</li>
<li>5번의 에포크 동안 네트워크는 2345번의 그래디언트 업데이트를 수행(에포크당 469번)</li>
<li>훈련 데이터에 대한 네트워크의 손실과 정확도 정보가 출력됨</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/5</span><br><span class="line">60000/60000 [==============================] - 3s 54us/step - loss: 0.2556 - acc: 0.9261</span><br><span class="line">Epoch 2/5</span><br><span class="line">60000/60000 [==============================] - 3s 51us/step - loss: 0.1042 - acc: 0.9688</span><br><span class="line">Epoch 3/5</span><br><span class="line">60000/60000 [==============================] - 3s 51us/step - loss: 0.0685 - acc: 0.9796</span><br><span class="line">Epoch 4/5</span><br><span class="line">60000/60000 [==============================] - 3s 52us/step - loss: 0.0499 - acc: 0.9845</span><br><span class="line">Epoch 5/5</span><br><span class="line">60000/60000 [==============================] - 3s 52us/step - loss: 0.0369 - acc: 0.9888</span><br><span class="line">&lt;keras.callbacks.History at 0x137a2e860&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 테스트셋에서 모델이 잘 작동하는지 확인하기</span></span><br><span class="line">test_loss, test_acc = network.evaluate(test_images, test_labels)</span><br><span class="line">print(<span class="string">'test_acc:'</span>, test_acc)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10000/10000 [==============================] - 0s 38us/step</span><br><span class="line">test_acc: 0.9798</span><br></pre></td></tr></table></figure>
<p>잘 작동한다!</p>
<ul>
<li>테스트셋의 정확도는 97.8%. 트레이닝셋 정확도와 차이가 나는 이유는 <strong>오버피팅</strong> 때문임</li>
</ul>
<h3 id="신경망을-위한-데이터-표현"><a href="#신경망을-위한-데이터-표현" class="headerlink" title="신경망을 위한 데이터 표현"></a>신경망을 위한 데이터 표현</h3><ul>
<li><strong>텐서</strong> : 데이터(숫자)를 위한 컨테이너</li>
<li>최근의 모든 머신러닝 시스템은 텐서를 기본 데이터 구조로 사용</li>
</ul>
<h4 id="스칼라-0D-텐서"><a href="#스칼라-0D-텐서" class="headerlink" title="스칼라(0D 텐서)"></a>스칼라(0D 텐서)</h4><ul>
<li>하나의 숫자만 담고 있는 텐서</li>
<li>넘파이에서는 float32나 float64 타입의 숫자가 스칼라 텐서</li>
<li>ndim 속성을 사용하면 넘파이 배열의 축 개수를 확인 가능</li>
<li>스칼라 텐서의 축 개수는 0(ndim == 0)</li>
<li>텐서의 축 개수를 랭크(rank)라고도 부름</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 스칼라 텐서(0D 텐서))</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.array(<span class="number">12</span>)</span><br><span class="line">x, x.ndim</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(array(12), 0)</span><br></pre></td></tr></table></figure>
<h4 id="벡터-1D-텐서"><a href="#벡터-1D-텐서" class="headerlink" title="벡터(1D 텐서)"></a>벡터(1D 텐서)</h4><ul>
<li>숫자의 배열</li>
<li>하나의 축을 가짐</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 벡터(1D 텐서)</span><br><span class="line">x = np.array([12, 3, 6, 14, 7])</span><br><span class="line">x, x.ndim</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(array([12,  3,  6, 14,  7]), 1)</span><br></pre></td></tr></table></figure>
<ul>
<li>위 예에서 x는 5개의 원소를 가지고 있으므로 5차원 벡터<ul>
<li>5D 벡터: 하나의 축을 따라 5개의 차원을 가진 것</li>
<li>5D 텐서: 5개의 축을 가진 것</li>
</ul>
</li>
<li>차원수(dimensionality)는 특정 축을 따라 놓인 원소의 개수(5D 벡터와 같은 경우)이거나 텐서의 축 개수(5D 텐서와 같은 경우)이거나 텐ㅅ의 축 개수(5D 텐서와 같은 경우)를 의미하므로 가끔 혼동하기 쉽다</li>
<li>후자의 경우 랭크 5인 텐서라고 말하는게 보다 정확(텐서의 랭크가 축의 개수)</li>
</ul>
<h4 id="행렬-2D-텐서"><a href="#행렬-2D-텐서" class="headerlink" title="행렬(2D 텐서)"></a>행렬(2D 텐서)</h4><ul>
<li>벡터의 배열이 행렬 또는 2D 텐서</li>
<li>행렬에는 행과 열 2개의 축이 있음</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">5</span>, <span class="number">78</span>, <span class="number">2</span>, <span class="number">34</span>, <span class="number">0</span>],</span><br><span class="line">              [<span class="number">6</span>, <span class="number">79</span>, <span class="number">3</span>, <span class="number">35</span>, <span class="number">1</span>],</span><br><span class="line">              [<span class="number">7</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="number">36</span>, <span class="number">2</span>]])</span><br><span class="line">x.ndim</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure>
<h4 id="3D-텐서와-고차원-텐서"><a href="#3D-텐서와-고차원-텐서" class="headerlink" title="3D 텐서와 고차원 텐서"></a>3D 텐서와 고차원 텐서</h4><ul>
<li>행렬들을 하나의 새로운 배열로 합치면 숫자로 채워진 직육면체 형태인 3D 텐서</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3D 텐서</span></span><br><span class="line">x = np.array([[[<span class="number">2</span>, <span class="number">45</span>, <span class="number">2</span>, <span class="number">34</span>, <span class="number">0</span>],</span><br><span class="line">               [<span class="number">5</span>, <span class="number">34</span>, <span class="number">5</span>, <span class="number">36</span>, <span class="number">1</span>],</span><br><span class="line">               [<span class="number">7</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="number">36</span>, <span class="number">2</span>]],</span><br><span class="line">              [[<span class="number">5</span>, <span class="number">78</span>, <span class="number">2</span>, <span class="number">34</span>, <span class="number">0</span>],</span><br><span class="line">               [<span class="number">6</span>, <span class="number">79</span>, <span class="number">3</span>, <span class="number">35</span>, <span class="number">1</span>],</span><br><span class="line">               [<span class="number">7</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="number">37</span>, <span class="number">2</span>]],</span><br><span class="line">              [[<span class="number">5</span>, <span class="number">89</span>, <span class="number">3</span>, <span class="number">35</span>, <span class="number">1</span>],</span><br><span class="line">               [<span class="number">7</span>, <span class="number">80</span>, <span class="number">4</span>, <span class="number">36</span>, <span class="number">2</span>],</span><br><span class="line">               [<span class="number">8</span>, <span class="number">72</span>, <span class="number">2</span>, <span class="number">40</span>, <span class="number">5</span>]]])</span><br><span class="line">x.ndim</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></table></figure>
<ul>
<li>3D 텐서들을 하나의 배열로 합치면 4D 텐서</li>
<li>딥러닝에선 보통 0D에서 4D까지의 텐서를 다룬다</li>
<li>동영상 데이터를 다룰 때는 5D 텐서까지 가기도</li>
</ul>
<h4 id="핵심-속성"><a href="#핵심-속성" class="headerlink" title="핵심 속성"></a>핵심 속성</h4><ul>
<li>텐서의 핵심 속성 3가지<ul>
<li>축의 개수(rank) : 3D 텐서에는 3개의 축, 행렬에는 2개의 축</li>
<li>크기(shape) : 텐서의 각 축을 따라 얼마나 많은 차원이 있는지 나타낸 파이썬의 튜플.</li>
<li>데이터 타입 : float32, float64. uint8 등</li>
</ul>
</li>
</ul>
<h4 id="배치-데이터"><a href="#배치-데이터" class="headerlink" title="배치 데이터"></a>배치 데이터</h4><ul>
<li>일반적으로 데이터 텐서의 첫 번째 축(0번 축)은 샘플 축(MNIST 예제에서는 숫자 이미지가 샘플)</li>
<li>딥러닝 모델은 데이터를 작은 batch로 나눠서 처리</li>
<li>이런 batch 데이터를 다룰 땐 0번 축을 배치 축 또는 배치 차원이라 부른다</li>
</ul>
<h4 id="텐서의-실제-사례"><a href="#텐서의-실제-사례" class="headerlink" title="텐서의 실제 사례"></a>텐서의 실제 사례</h4><ul>
<li>벡터 데이터<ul>
<li>(샘플들, 피처들) 크기의 2D 텐서</li>
<li>ex) 사람의 나이, 우편번호, 소득으로 구성된 인구 통계 데이터. 각 사람은 3개의 값을 가진 벡터로 구성. 10만명이 포함된 전체 데이터셋은 (100000, 3) 크기의 텐서에 저장  </li>
</ul>
</li>
<li>시계열 데이터<ul>
<li>(샘플들, timsteps, 피처들) 크기의 3D 텐서</li>
<li>시간이 중요할 때는 시간 축을 포함해 3D 텐서로 저장</li>
<li>관례적으로 시간 축은 항상 1번(두 번째) 축</li>
<li>ex) 주식 가격 데이터셋. 1분마다 현재 주식 가격, 지난 1분 동안 최고가와 최소가를 저장. 1분마다 데이터는 3D 벡터로 인코딩. 하루(390분) 거래는 (390, 3) 크기의 2D 텐서로 인코딩. 250일치 데이터는 (250, 390, 3) 크기의 3D 텐서로 저장될 수 있음.     </li>
</ul>
</li>
<li>이미지<ul>
<li>(샘플들, 높이, 너비, 컬러 채널) 크기의 4D 텐서</li>
<li>ex) 256 x 256 크기의 흑백 이미지에 대한 128개의 배치는 (128, 256, 256, 1) 크기의 텐서에 저장 가능. 컬러라면 (128, 256, 256, 3)</li>
</ul>
</li>
<li>동영상<ul>
<li>(samples, frames, height, width, channels) 크기의 5D 텐서</li>
<li>ex) 60초짜리 144 x 256 유튜브 영상을 초당 4프레임으로 샘플링하면 240프레임. 이 영상을 4개 가진 배치는 (4, 240, 144, 256, 3) 크기의 텐서에 저장.</li>
</ul>
</li>
</ul>
<h3 id="신경망의-톱니바퀴-텐서-연산"><a href="#신경망의-톱니바퀴-텐서-연산" class="headerlink" title="신경망의 톱니바퀴 : 텐서 연산"></a>신경망의 톱니바퀴 : 텐서 연산</h3><ul>
<li>케라스 층 생성하기<ul>
<li><code>keras.layers.Dense(512, activation=&#39;relu&#39;)</code></li>
<li>2D 텐서를 입력받고 입력 텐서의 새로운 표현인 또 다른 2D 텐서를 반환하는 함수로 볼 수 있음</li>
<li><code>output = relu(dot(W, input) + b)</code> 함수와 같음</li>
<li>이 함수에는 3개의 텐서 연산이 있음. 입력 텐서와 W의 dot, dot의 결과인 2D 텐서와 벡터 b 사이의 덧셈, relu 연산</li>
</ul>
</li>
</ul>
<h4 id="원소별-연산"><a href="#원소별-연산" class="headerlink" title="원소별 연산"></a>원소별 연산</h4><ul>
<li>relu 함수와 덧셈은 원소별 연산.</li>
<li>각 원소에 독립적으로 적용됨</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파이썬으로 구현한 원소별 연산_relu</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len((x.shape) == <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    x = x.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">            x[i, j] = max(x[i, j], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 파이썬으로 구현한 원소별 연산_덧셈</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_add</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">2</span></span><br><span class="line">    <span class="keyword">assert</span> x.shape == y.shape</span><br><span class="line"></span><br><span class="line">    x = x.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">            x[i, j] += y[i, j]</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="브로드캐스팅"><a href="#브로드캐스팅" class="headerlink" title="브로드캐스팅"></a>브로드캐스팅</h4><ul>
<li>크기가 다른 두 텐서가 더해진다면? 실행 가능하다면 작은 텐서가 큰 텐서 크기에 맞춰 브로드캐스팅됨<ol>
<li>큰 텐서의 ndim에 맞게 작은 텐서에 브로드캐스팅 축이 추가됨</li>
<li>작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복됨</li>
</ol>
</li>
<li>ex) X.shape = (32, 10), y.shape = (10,)<ol>
<li>y에 비어 있는 축을 추가해 (1, 10)으로</li>
<li>y를 이 축에 32번 반복하면 텐서 Y.shape는 (32, 10)</li>
<li>크기가 같아져서 더할 수 있음</li>
</ol>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 단순 구현</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_add_matrix_and_vector</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">2</span></span><br><span class="line">    <span class="keyword">assert</span> len(y.shape) == <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> x.shape[<span class="number">1</span>] == y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    x = x.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">            x[i, j] += y[j]</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 크기가 다른 두 텐서에 브로드캐스팅으로 원소별 maximum 연산 적용</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.random.random((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">10</span>))</span><br><span class="line">y = np.random.random((<span class="number">32</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">z = np.maximum(x, y)</span><br></pre></td></tr></table></figure>
<h4 id="tensor-product"><a href="#tensor-product" class="headerlink" title="tensor product"></a>tensor product</h4><ul>
<li>원소별 연산과 반대로 입력 텐서의 원소들을 결합시킴</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 점곱 연산</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_vector_dot</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> len(y.shape) == <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> x.shape[<span class="number">0</span>] == y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    z = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        z += x[i] * y[i]</span><br><span class="line">    <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 행렬 x와 벡터 y의 점곱</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">naive_matrix_vector_dot</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x.shape) == <span class="number">2</span></span><br><span class="line">    <span class="keyword">assert</span> len(y.shape) == <span class="number">1</span></span><br><span class="line">    <span class="keyword">assert</span> x.shape[<span class="number">1</span>] == y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    z = np.zeros(x.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">            z[i] += x[i, j] * y[j]</span><br><span class="line">    <span class="keyword">return</span> z</span><br></pre></td></tr></table></figure>
<h4 id="텐서-크기-변환-tensor-reshaping"><a href="#텐서-크기-변환-tensor-reshaping" class="headerlink" title="텐서 크기 변환(tensor reshaping)"></a>텐서 크기 변환(tensor reshaping)</h4><ul>
<li>특정 크기에 맞게 열과 행을 재배열</li>
<li>행과 열을 바꾸는 전치도 자주 사용</li>
</ul>
<h4 id="텐서-연산의-기하학적-해석"><a href="#텐서-연산의-기하학적-해석" class="headerlink" title="텐서 연산의 기하학적 해석"></a>텐서 연산의 기하학적 해석</h4><ul>
<li>아핀 변환, 회전, 스케일링 등 기본적인 기하학적 연산은 텐선으로 표현 가능</li>
</ul>
<h4 id="딥러닝의-기하학적-해석"><a href="#딥러닝의-기하학적-해석" class="headerlink" title="딥러닝의 기하학적 해석"></a>딥러닝의 기하학적 해석</h4><ul>
<li>빨간색 파란색 2장의 색종이를 겹친 다음 뭉쳐서 작은 공을 만든다고 가정</li>
<li>종이공: 입력 데이터, 색종이: 분류 문제의 데이터 클래스</li>
<li>신경망의 역할: 종이 공을 펼쳐서 두 클래스가 분리되는 변환을 찾는 것</li>
</ul>
<h3 id="신경망의-엔진-그래디언트-기반-최적화"><a href="#신경망의-엔진-그래디언트-기반-최적화" class="headerlink" title="신경망의 엔진: 그래디언트 기반 최적화"></a>신경망의 엔진: 그래디언트 기반 최적화</h3><p><code>output = relu(dot(W, input) + b)</code></p>
<ul>
<li>텐서 W와 b는 층의 속성. 가중치 또는 훈련되는 파라미터.</li>
<li>이런 가중치에는 훈련 데이터를 신경망에 노출시켜 학습된 정보가 담겨 있음</li>
<li>초기에는 가중치 행렬이 작은 난수로 채워짐(무작위 초기화 단계)</li>
<li>피드백 신호에 따라 가중치가 점진적으로 조정됨(훈련 단계)</li>
<li>훈련 반복 루프<ol>
<li>트레이닝 샘플 x와 타깃 y의 배치를 추출</li>
<li>x를 사용해 네트워크를 실행(forward pass 단계)하고 y_pred 구하기</li>
<li>y와 y_pred 차이를 측정해 이 배치에 대한 네트워크 손실 계산</li>
<li>배치에 대한 손실이 감소되도록 네트워크 가중치 업데이트</li>
</ol>
</li>
<li>이 접근법은 모든 가중치 행렬의 원소바다 두 번의 forward pass를 계산해야 하므로 비효율적</li>
<li>신경망에 사용된 연산이 미분 가능하다는 점을 이용해 네트워크 가중치에 대한 손실의 그래디언트를 계산하는 게 더 좋은 방법</li>
</ul>
<h4 id="변화율"><a href="#변화율" class="headerlink" title="변화율"></a>변화율</h4><ul>
<li>derivative!</li>
</ul>
<h4 id="텐서-연산의-변화율-그래디언트"><a href="#텐서-연산의-변화율-그래디언트" class="headerlink" title="텐서 연산의 변화율: 그래디언트"></a>텐서 연산의 변화율: 그래디언트</h4><ul>
<li>다차원 입력, 즉 텐서를 입력으로 받는 함수에 변화율 개념을 확장시킨 것</li>
<li>y_pred = dot(W, x)</li>
<li>loss_value = loss(y_pred, y)</li>
<li>입력 데이터 x와 y가 고정돼 있다면 이 함수는 W를 손실 값에 매핑하는 함수로 볼 수 있음</li>
<li>loss_value = f(W)</li>
<li>W0(W의 현재값)에서 f의 변화율: W와 같은 크기의 텐서인 gradient(f)(W0)</li>
<li>이 텐서의 각 원소 gradient(f)(W0)[i, j]: W0[i, j]를 변경했을 때 loss_value가 바뀌는 방향과 크기를 나타냄</li>
<li>즉 gradient(f)(W0)가 W0에서 함수 f(W) = loss_value의 그래디언트</li>
<li>gradient(f)(W0)는 W0에서 f(W)의 기울기를 나타내는 텐서로 해석 가능</li>
<li>f(W)에서 그래디언트의 반대 방향으로 W를 움직이면 f(W) 값을 줄일 수 있음</li>
</ul>
<h4 id="확률적-경사-하강법"><a href="#확률적-경사-하강법" class="headerlink" title="확률적 경사 하강법"></a>확률적 경사 하강법</h4><ul>
<li>절충안: 미니 배치 확률적 경사 하강법(미니 배치 SGD)<ol>
<li>훈련샘플 배치 x와 타깃 y를 추출</li>
<li>x로 네트워크를 실행하고 y_pred 구하기</li>
<li>y와 y_pred 사이의 오차를 측정해 네트워크 손실 계산</li>
<li>네트워크의 파라미터에 대한 손실 함수의 그래디언트 계산(backward pass)</li>
<li>그래디언트 반대 방향으로 파라미터 이동</li>
</ol>
</li>
<li>트루 SGD<ul>
<li>반복마다 하나의 샘플과 하나의 타깃을 뽑음</li>
</ul>
</li>
<li>배치 SGD<ul>
<li>가용한 모든 데이터로 반복 실행</li>
<li>업데이트가 정확하지만 많은 비용</li>
</ul>
</li>
<li>SGD 변종들<ul>
<li>업데이트할 다음 가중치를 계산할 때 현재 그래디언트 값만 보지 않고 이전에 업데이트된 가중치를 여러 가지 다른 방식으로 고려</li>
<li>ex) 모멘텀을 사용한 SGD, Adagrad, RMSProp 등</li>
<li>이런 변종들을 최적화 방법 or 옵티마이저라 부름</li>
<li>모멘텀은 SGD에 있는 2개의 문제점인 수렴 속도와 지역 최솟값을 해결</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모멘텀 단순 구현</span></span><br><span class="line">past_velocity = <span class="number">0.</span></span><br><span class="line">momentum = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">while</span> loss &gt; <span class="number">0.1</span>:</span><br><span class="line">    w, loss, gradient = get_current_parameters()</span><br><span class="line">    velocity = momentum * past_velocity - learning_rate * gradient</span><br><span class="line">    w = w + momentum * velocity - learning_rate * gradient</span><br><span class="line">    past_velocity = velocity</span><br><span class="line">    update_parameter(w)</span><br></pre></td></tr></table></figure>
<h4 id="변화율-연결-역전파-알고리즘"><a href="#변화율-연결-역전파-알고리즘" class="headerlink" title="변화율 연결: 역전파 알고리즘"></a>변화율 연결: 역전파 알고리즘</h4><ul>
<li>3개의 텐서 연산 a, b, c와 가중치 행렬 W1, W2, W3로 구성된 네트워크 f의 예</li>
<li>f(W1, W2, W3) = a(W1, b(W2, c(W3)))</li>
<li>연쇄법칙을 신경망 그래디언트 계산에 적용한 역전파 알고리즘</li>
</ul>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/DSBooks/">DSBooks</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Keras-Deep-Learning-with-Python/">Keras, Deep Learning with Python</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Deep-Learning-with-Python-Ch-01" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/01/28/Deep-Learning-with-Python-Ch-01/">Deep Learning with Python - Ch.01</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/01/28/Deep-Learning-with-Python-Ch-01/" class="article-date">
	  <time datetime="2019-01-28T06:36:09.000Z" itemprop="datePublished">January 28, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p><a href="https://ridibooks.com/v2/Detail?id=754024868&amp;_s=search&amp;_q=%EC%BC%80%EB%9D%BC%EC%8A%A4" target="_blank" rel="noopener">케라스 창시자에게 배우는 딥러닝</a>을 실습하면서 정리한 포스트입니다. 코드 예제와 코드 설명은 <a href="https://github.com/rickiepark/deep-learning-with-python-notebooks" target="_blank" rel="noopener">역자 깃허브</a>에서 받아볼 수 있습니다. 출판물이고 개인적으로만 참고하기 위한 요약노트이다 보니 설명이 불친절한 점은 양해 바랍니다. 보다 자세한 내용을 원하시는 분은 위 링크의 책을 참고하시기 바랍니다.</p>
<hr>
<h2 id="1장-딥러닝이란-무엇인가"><a href="#1장-딥러닝이란-무엇인가" class="headerlink" title="1장 딥러닝이란 무엇인가?"></a>1장 딥러닝이란 무엇인가?</h2><h3 id="인공지능과-머신-러닝-딥러닝"><a href="#인공지능과-머신-러닝-딥러닝" class="headerlink" title="인공지능과 머신 러닝, 딥러닝"></a>인공지능과 머신 러닝, 딥러닝</h3><h4 id="인공지능"><a href="#인공지능" class="headerlink" title="인공지능"></a>인공지능</h4><ul>
<li>보통의 사람이 수행하는 지능적인 작업을 자동화하기 위한 연구 활동</li>
<li>인공지능 ⊃ 머신러닝 ⊃ 딥러닝</li>
<li>심볼릭 AI : “명시적인 규칙을 충분히 많이 만들어 지식을 다루면 인간 수준의 인공지능을 만들 수 있다” ☞ 하지만 명확한 규칙을 찾는게 쉽지 않음</li>
</ul>
<h4 id="머신러닝"><a href="#머신러닝" class="headerlink" title="머신러닝"></a>머신러닝</h4><ul>
<li>전통적 프로그래밍 : 규칙, 데이터를 넣어서 해답을 찾는다</li>
<li>머신러닝 : 데이터, 해답을 넣어서 규칙을 찾는다</li>
</ul>
<h4 id="데이터에서-표현을-학습하기"><a href="#데이터에서-표현을-학습하기" class="headerlink" title="데이터에서 표현을 학습하기"></a>데이터에서 표현을 학습하기</h4><ul>
<li>머신 러닝과 딥러닝의 핵심 문제는 의미 있는 데이터로의 변환</li>
<li>입력 데이터를 기반으로 기대 출력에 가깝게 만드는 유용한 표현을 학습하는 것</li>
</ul>
<h4 id="딥러닝에서-‘딥’이란"><a href="#딥러닝에서-‘딥’이란" class="headerlink" title="딥러닝에서 ‘딥’이란?"></a>딥러닝에서 ‘딥’이란?</h4><ul>
<li>딥러닝은 머신러닝의 특정한 한 분야로 연속된 층에서 점진적으로 의미 있는 표현을 배우는 데 강점이 있음</li>
<li>‘딥’이란 연속된 층으로 표현을 학습한다는 개념을 나타냄</li>
<li>딥러닝에선 기본 층을 겹겹이 쌓아 올려 구성한 신경망이라는 모델을 사용</li>
</ul>
<h4 id="그림-3개로-딥러닝의-작동-원리-이해하기"><a href="#그림-3개로-딥러닝의-작동-원리-이해하기" class="headerlink" title="그림 3개로 딥러닝의 작동 원리 이해하기"></a>그림 3개로 딥러닝의 작동 원리 이해하기</h4><ul>
<li>신경망은 가중치를 파라미터로 가진다</li>
<li>손실 함수가 신경망의 출력 품질을 측정한다</li>
<li>손실 점수를 피드백 신호로 사용하여 가중치를 조정한다</li>
</ul>
<h4 id="지금까지-딥러닝-성과"><a href="#지금까지-딥러닝-성과" class="headerlink" title="지금까지 딥러닝 성과"></a>지금까지 딥러닝 성과</h4><ul>
<li>이미지 분류, 음성, 필기 인식 등</li>
<li>여전히 할 수 있는 일을 알아가는 중. 형식 추론과 같은 다양한 문제에 적용하기 시작</li>
</ul>
<h4 id="단기간의-과대-선전을-믿지-말자"><a href="#단기간의-과대-선전을-믿지-말자" class="headerlink" title="단기간의 과대 선전을 믿지 말자"></a>단기간의 과대 선전을 믿지 말자</h4><ul>
<li>딥러닝이 할 수 있는 것과 할 수 없는 것에 대해 명확히 이해하자</li>
</ul>
<h4 id="AI에-대한-전망"><a href="#AI에-대한-전망" class="headerlink" title="AI에 대한 전망"></a>AI에 대한 전망</h4><ul>
<li>단기 기대는 비현실적일수도 있지만 장기 전망은 매우 밝다</li>
</ul>
<h3 id="딥러닝-이전-머신-러닝의-간략한-역사"><a href="#딥러닝-이전-머신-러닝의-간략한-역사" class="headerlink" title="딥러닝 이전: 머신 러닝의 간략한 역사"></a>딥러닝 이전: 머신 러닝의 간략한 역사</h3><h4 id="확률적-모델링"><a href="#확률적-모델링" class="headerlink" title="확률적 모델링"></a>확률적 모델링</h4><ul>
<li>통계학 이론을 데이터 분석에 응용한 것</li>
<li>나이브 베이즈 알고리즘 : 입력 데이터의 특성이 모두 독립적이라 가정하고 베이즈 정리를 적용하는 머신 러닝 분류 알고리즘</li>
<li>로지스틱 회귀 : 현대 머신 러닝의 “hello world”</li>
</ul>
<h4 id="초창기-신경망"><a href="#초창기-신경망" class="headerlink" title="초창기 신경망"></a>초창기 신경망</h4><ul>
<li>경사 하강법 최적화를 사용해 연쇄적으로 변수가 연결된 연산을 훈련하는 방법</li>
<li>최초의 성공적인 신경망 애플리케이션은 합성곱 신경망과 역전파를 연결해 손글씨 숫자 이미지를 분류하는 문제에 적용됨</li>
</ul>
<h4 id="커널-방법"><a href="#커널-방법" class="headerlink" title="커널 방법"></a>커널 방법</h4><ul>
<li>분류 알고리즘의 한 종류로 SVM(서포트 벡터 머신)이 가장 유명</li>
<li>SVM은 2개의 다른 범주에 속한 데이터 그룹 사이에 결정 경계(decision boundary)를 찾는다</li>
<li>decision boundary를 찾는 두 단계 과정<ul>
<li>decision boundary가 하나의 초평면(hyperplane)으로 표현될 수 있는 새로운 고차원 표현으로 데이터를 매핑</li>
<li>초평면과 각 클래스의 가장 가까운 데이터 포인트 사이의 거리가 최대가 되는 최선의 결정 경계(하나의 분할 초평면)를 찾는다 == 마진 최대화 단계</li>
</ul>
</li>
</ul>
<h4 id="결정트리-랜덤-포레스트-그래디언트-부스팅"><a href="#결정트리-랜덤-포레스트-그래디언트-부스팅" class="headerlink" title="결정트리, 랜덤 포레스트, 그래디언트 부스팅"></a>결정트리, 랜덤 포레스트, 그래디언트 부스팅</h4><ul>
<li>결정트리 : 플로차트 같은 구조를 가지며 입력 데이터 포인트를 분류하거나 주어진 입력에 대해 출력 값을 예측</li>
<li>랜덤 포레스트 : 서로 다른 결정 트리를 많이 만들고 그 출력을 앙상블하는 방법을 사용</li>
<li>그래디언트 부스팅 : 이전 모델에서 놓친 데이터 포인트를 보완하는 새로운 모델을 반복적으로 훈련해 모델을 향상시킴</li>
</ul>
<h4 id="다시-신경망으로"><a href="#다시-신경망으로" class="headerlink" title="다시 신경망으로"></a>다시 신경망으로</h4><ul>
<li>2011년 IDSIA의 댄 크리슨이 심층 신경망으로 이미지 분류 대회에서 우승했는데 이것이 현대적 딥러닝의 첫 번째 성공</li>
<li>2012년부터 심층 합성곱이 모든 컴퓨터 비전 작업의 주력 알고리즘이 됨</li>
</ul>
<h4 id="딥러닝의-특징"><a href="#딥러닝의-특징" class="headerlink" title="딥러닝의 특징"></a>딥러닝의 특징</h4><ul>
<li>딥러닝의 변환 능력은 모델이 모든 표현 층을 순차적이 아니라 동시에 공동으로 학습하게 만든다</li>
<li>딥러닝이 데이터로부터 학습하는 방법의 특징<ul>
<li>층을 거치면서 점진적으로 더 복잡한 표현이 만들어진다</li>
<li>이런 점진적인 중간 표현이 공동으로 학습된다</li>
</ul>
</li>
</ul>
<h4 id="머신러닝의-최근-동향"><a href="#머신러닝의-최근-동향" class="headerlink" title="머신러닝의 최근 동향"></a>머신러닝의 최근 동향</h4><ul>
<li>캐글의 머신 러닝 경연을 살펴보면 동향을 알 수 있다</li>
<li>2016 2017년 캐글 주류는 그래디언트 부스팅 머신(구조적인 데이터의 경우)과 딥러닝(이미지 분류 등 지각에 관한 문제)</li>
</ul>
<h3 id="왜-딥러닝일까-왜-지금"><a href="#왜-딥러닝일까-왜-지금" class="headerlink" title="왜 딥러닝일까? 왜 지금?"></a>왜 딥러닝일까? 왜 지금?</h3><ul>
<li>컴퓨터 비전 딥러닝 핵심 아이디어인 합성곱 신경망과 역전파는 1989년에, 시계열을 위한 딥러닝 기본인 LSTM(Long Short-Term Memory) 알고리즘은 1997년에 개발됐는데 2012년 이후에 딥러닝이 부상한 배경에는 다음 세 가지가 있다<ul>
<li>하드웨어</li>
<li>데이터셋과 벤치마크</li>
<li>알고리즘 향상</li>
</ul>
</li>
</ul>
<h4 id="새로운-투자의-바람"><a href="#새로운-투자의-바람" class="headerlink" title="새로운 투자의 바람"></a>새로운 투자의 바람</h4><ul>
<li>딥러닝은 테크 공룡들의 핵심 상품 전략</li>
</ul>
<h4 id="딥러닝의-대중화"><a href="#딥러닝의-대중화" class="headerlink" title="딥러닝의 대중화"></a>딥러닝의 대중화</h4><ul>
<li>초창기에는 C++과 CUDA 전문가가 되어야 했지만 요즘에는 기본 파이썬 스크립트 기술만 있으면 딥러닝 연구에 충분(씨아노와 텐서플로 덕분)</li>
</ul>
<h4 id="지속될까"><a href="#지속될까" class="headerlink" title="지속될까?"></a>지속될까?</h4><ul>
<li>20년 뒤에는 신경망을 쓰지 않을지도 모르지만 딥러닝과 그 핵심개념에서 파생된 무엇인가를 사용할 것</li>
<li>단순함, 확장성, 다용도와 재사용성</li>
</ul>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/DSBooks/">DSBooks</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Keras-Deep-Learning-with-Python/">Keras, Deep Learning with Python</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-World-This-Week-Jan-26th-2019" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/01/28/World-This-Week-Jan-26th-2019/">[World This Week] Jan 26th 2019</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/01/28/World-This-Week-Jan-26th-2019/" class="article-date">
	  <time datetime="2019-01-28T05:42:35.000Z" itemprop="datePublished">January 28, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<h4 id="All-the-world-is-a-laboratory-to-the-inquiring-mind"><a href="#All-the-world-is-a-laboratory-to-the-inquiring-mind" class="headerlink" title="All the world is a laboratory to the inquiring mind."></a><strong>All the world is a laboratory to the inquiring mind.</strong></h4><p> <em>- Martin H. Fischer</em></p>
</blockquote>
<p><br></p>
<p><a href="https://www.economist.com/printedition/2019-01-26" target="_blank" rel="noopener"><img src="https://www.economist.com/sites/default/files/imagecache/400-width/print-covers/20190126_cuk400hires_0.jpg" alt="2019-01-26"></a></p>
<h4 id="The-world-this-week-Business-this-Week"><a href="#The-world-this-week-Business-this-Week" class="headerlink" title="| The world this week | Business this Week"></a>| The world this week | Business this Week</h4><ul>
<li></li>
</ul>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/The-Economist/">The Economist</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/The-economist/">The economist</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-FC-Tensorflow-패키지-소개-2" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/01/23/FC-Tensorflow-패키지-소개-2/">[FC] Tensorflow 패키지 소개(2)</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/01/23/FC-Tensorflow-패키지-소개-2/" class="article-date">
	  <time datetime="2019-01-23T01:47:33.000Z" itemprop="datePublished">January 23, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p>패스트캠퍼스 데이터사이언스스쿨의 김도형 박사님 수업을 듣고 강의자료를 요약한 글입니다. 개인적으로 참고하기 위한 요약노트이니 보다 자세한 내용을 원하시는 분은 <a href="https://datascienceschool.net" target="_blank" rel="noopener">https://datascienceschool.net</a> 에 올라와 있는 강의자료를 참고하시기 바랍니다.</p>
<hr>
<h3 id="변수-공간과-변수의-재사용"><a href="#변수-공간과-변수의-재사용" class="headerlink" title="변수 공간과 변수의 재사용"></a>변수 공간과 변수의 재사용</h3><h3 id="Tensor-연산"><a href="#Tensor-연산" class="headerlink" title="Tensor 연산"></a>Tensor 연산</h3><h3 id="자동-형변환"><a href="#자동-형변환" class="headerlink" title="자동 형변환"></a>자동 형변환</h3><h3 id="미분"><a href="#미분" class="headerlink" title="미분"></a>미분</h3><h3 id="TensorFlow를-이용한-선형회귀"><a href="#TensorFlow를-이용한-선형회귀" class="headerlink" title="TensorFlow를 이용한 선형회귀"></a>TensorFlow를 이용한 선형회귀</h3><h3 id="퍼셉트론"><a href="#퍼셉트론" class="headerlink" title="퍼셉트론"></a>퍼셉트론</h3><h3 id="최적화"><a href="#최적화" class="headerlink" title="최적화"></a>최적화</h3><h3 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h3><h4 id="텐서보드용-로그-생성"><a href="#텐서보드용-로그-생성" class="headerlink" title="텐서보드용 로그 생성"></a>텐서보드용 로그 생성</h4><h4 id="텐서보드-가동"><a href="#텐서보드-가동" class="headerlink" title="텐서보드 가동"></a>텐서보드 가동</h4><p><strong>연습문제 20.1.2</strong></p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/datascienceschool/">datascienceschool</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/fastcampus-Dr-Kim-datascienceschool/">fastcampus, Dr.Kim, datascienceschool</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/">Next</a>
  </nav>

</section>
          <aside id="sidebar">
  
    <div class="widget-wrap" style="margin: 20px 0;">
	<div id="search-form-wrap">

    <form class="search-form">
        <label style="width: 75%;">
            <span class="screen-reader-text">Search for:</span>
            <input type="search" class="search-field" style="height: 42px;" placeholder=" Search…" value="" name="s" title="Search for:">
        </label>
        <input type="submit" class="search-form-submit" value="Search">
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something...">
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
</div>
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Connect With Us</h3>
    <div class="widget widget_athemes_social_icons">

    	<ul class="clearfix widget-social-icons">   
    	
          
     			  <li><a href="https://github.com/foxsayy" title="Github"><i class="fa fa-github" aria-hidden="true"></i></a></li> 
          
   		
          
     			  <li><a href="https://www.instagram.com/roh.sng.hwan/?hl=ko" title="Instagram"><i class="fa fa-instagram" aria-hidden="true"></i></a></li> 
          
   		
          
            <li><a href="mailto:roh.sng.hwan@gmail.com?subject=请联系我&body=我能帮你什么" title="email"><i class="fa fa-envelope" aria-hidden="true"></i></a></li> 
          
   		
   		</ul>


   		<!--
   		<ul class="clearfix widget-social-icons">   		
   		<li class="widget-si-twitter"><a href="http://twitter.com" title="Twitter"><i class="ico-twitter"></i></a></li> 
		<li class="widget-si-facebook"><a href="http://facebook.com" title="Facebook"><i class="ico-facebook"></i></a></li>
			<li class="widget-si-gplus"><a href="http://plus.google.com" title="Google+"><i class="ico-gplus"></i></a></li>
			<li class="widget-si-pinterest"><a href="http://pinterest.com" title="Pinterest"><i class="ico-pinterest"></i></a></li>
			<li class="widget-si-flickr"><a href="http://flickr.com" title="Flickr"><i class="ico-flickr"></i></a></li>
			<li class="widget-si-instagram"><a href="http://instagram.com" title="Instagram"><i class="ico-instagram"></i></a></li>
		</ul> -->

    </div>
  </div>


  
    
  <div class="widget_athemes_tabs">
    <ul id="widget-tab" class="clearfix widget-tab-nav">
      <li class="active"><a>Recent Posts</a></li>
    </ul>
    <div class="widget">
      <ul>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/02/06/Deep-Learning-with-Python-Ch-03/">Deep Learning with Python - Ch.03</a></h6>
              <span>February 6, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/01/28/Deep-Learning-with-Python-Ch-02/">Deep Learning with Python - Ch.02</a></h6>
              <span>January 28, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/01/28/Deep-Learning-with-Python-Ch-01/">Deep Learning with Python - Ch.01</a></h6>
              <span>January 28, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/01/28/World-This-Week-Jan-26th-2019/">[World This Week] Jan 26th 2019</a></h6>
              <span>January 28, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/01/23/FC-Tensorflow-패키지-소개-2/">[FC] Tensorflow 패키지 소개(2)</a></h6>
              <span>January 23, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/01/22/FC-Tensorflow-패키지-소개/">[FC] Tensorflow 패키지 소개</a></h6>
              <span>January 22, 2019</span>
            </div>

          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/DSBooks/">DSBooks</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/MySQL/">MySQL</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NoSQL/">NoSQL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/The-Economist/">The Economist</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/books/">books</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/datascienceschool/">datascienceschool</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/패턴-인식과-머신-러닝/">패턴 인식과 머신 러닝</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>

    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">16</span></li></ul>
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2019 THE DATASCIENTIST All Rights Reserved.
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hiero" target="_blank">hiero</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var contentdiv = document.getElementById("content");

    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
</script>

<!-- Custome JS -->
<script src="/js/my.js"></script>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories/index.html" class="mobile-nav-link">Categories</a>
  
    <a href="/log/index.html" class="mobile-nav-link">Log</a>
  
    <a href="/about/index.html" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.js"></script>


<script src="/js/scripts.js"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>








  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
