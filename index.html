<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">

  <!-- PACE Progress Bar START -->
  
    <script src="https://raw.githubusercontent.com/HubSpot/pace/v1.0.2/pace.min.js"></script>
    <link rel="stylesheet" href="https://github.com/HubSpot/pace/raw/master/themes/orange/pace-theme-flash.css">
  
  

  <!-- PACE Progress Bar START -->

  
  <title>THE DATASCIENTIST</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  
  
  
  <meta property="og:type" content="website">
<meta property="og:title" content="THE DATASCIENTIST">
<meta property="og:url" content="http://foxsayy.github.io/index.html">
<meta property="og:site_name" content="THE DATASCIENTIST">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="THE DATASCIENTIST">
  
    <link rel="alternate" href="/atom.xml" title="THE DATASCIENTIST" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="https://cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="https://code.jquery.com/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="https://netdna.bootstrapcdn.com/bootstrap/3.0.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/hiero.css">
  <link rel="stylesheet" href="/css/glyphs.css">
  

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/css/my.css">
  <!-- Google Adsense -->
  
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
      (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-0123456789ABCDEF",
          enable_page_level_ads: true
      });
  </script>
  
</head>
</html>
<script>
var themeMenus = {};

  themeMenus["/"] = "Home"; 

  themeMenus["/archives"] = "Archives"; 

  themeMenus["/categories/index.html"] = "Categories"; 

  themeMenus["/log/index.html"] = "log"; 

  themeMenus["/about/index.html"] = "About"; 

</script>


  <body>


  <header id="allheader" class="site-header" role="banner">
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="THE DATASCIENTIST" rel="home"> THE DATASCIENTIST </a>
            
          </h1>

          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>
            <div class="clearfix sf-menu">

              <ul id="main-nav" class="nmenu sf-js-enabled">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">Home</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">Archives</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories/index.html">Categories</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/log/index.html">log</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about/index.html">About</a> </li>
                    
              </ul>
            </div>
          </nav>


      </div>
  </div>
</header>


  <div id="originBgDiv" style="background: #fff; width: 100%;">

      <div style="max-height:600px; overflow: hidden;  display: flex; display: -webkit-flex; align-items: center;">
        <img id="originBg" width="100%" alt="" src="">
      </div>

  </div>

  <script>
  function setAboutIMG(){
      var imgUrls = "css/images/pose.jpg,https://source.unsplash.com/collection/954550/1920x1080".split(",");
      var random = Math.floor((Math.random() * imgUrls.length ));
      if (imgUrls[random].startsWith('http') || imgUrls[random].indexOf('://') >= 0) {
        document.getElementById("originBg").src=imgUrls[random];
      } else {
        document.getElementById("originBg").src='/' + imgUrls[random];
      }
  }
  bgDiv=document.getElementById("originBgDiv");
  if(location.pathname.match('about')){
    setAboutIMG();
    bgDiv.style.display='block';
  }else{
    bgDiv.style.display='none';
  }
  </script>



  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main">
  
    <article id="post-Python-딕셔너리-key와-value-뒤집기" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/02/20/Python-딕셔너리-key와-value-뒤집기/">[Python] 딕셔너리 key와 value 뒤집기</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/02/20/Python-딕셔너리-key와-value-뒤집기/" class="article-date">
	  <time datetime="2019-02-20T06:55:25.000Z" itemprop="datePublished">February 20, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="딕셔너리-key와-value-바꾸기"><a href="#딕셔너리-key와-value-바꾸기" class="headerlink" title="딕셔너리 key와 value 바꾸기"></a>딕셔너리 key와 value 바꾸기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">morse = &#123;</span><br><span class="line">    <span class="string">'A'</span> : <span class="string">'.-'</span>,</span><br><span class="line">    <span class="string">'B'</span> : <span class="string">'-...'</span>,</span><br><span class="line">    <span class="string">'C'</span> : <span class="string">'-.-.'</span>,</span><br><span class="line">    <span class="string">'D'</span> : <span class="string">'-..'</span>,</span><br><span class="line">    <span class="string">'E'</span> : <span class="string">'.'</span>,</span><br><span class="line">    <span class="string">'F'</span> : <span class="string">'..-.'</span>,</span><br><span class="line">    <span class="string">'G'</span> : <span class="string">'--.'</span>,</span><br><span class="line">    <span class="string">'H'</span> : <span class="string">'....'</span>,</span><br><span class="line">    <span class="string">'I'</span> : <span class="string">'..'</span>,</span><br><span class="line">    <span class="string">'J'</span> : <span class="string">'.---'</span>,</span><br><span class="line">    <span class="string">'K'</span> : <span class="string">'-.-'</span>,</span><br><span class="line">    <span class="string">'L'</span> : <span class="string">'.-..'</span>,</span><br><span class="line">    <span class="string">'M'</span> : <span class="string">'--'</span>,</span><br><span class="line">    <span class="string">'N'</span> : <span class="string">'-.'</span>,</span><br><span class="line">    <span class="string">'O'</span> : <span class="string">'---'</span>,</span><br><span class="line">    <span class="string">'P'</span> : <span class="string">'.--.'</span>,</span><br><span class="line">    <span class="string">'Q'</span> : <span class="string">'--.-'</span>,</span><br><span class="line">    <span class="string">'R'</span> : <span class="string">'.-.'</span>,</span><br><span class="line">    <span class="string">'S'</span> : <span class="string">'...'</span>,</span><br><span class="line">    <span class="string">'T'</span> : <span class="string">'-'</span>,</span><br><span class="line">    <span class="string">'U'</span> : <span class="string">'..-'</span>,</span><br><span class="line">    <span class="string">'V'</span> : <span class="string">'...-'</span>,</span><br><span class="line">    <span class="string">'W'</span> : <span class="string">'.--'</span>,</span><br><span class="line">    <span class="string">'X'</span> : <span class="string">'-..-'</span>,</span><br><span class="line">    <span class="string">'Y'</span> : <span class="string">'-.--'</span>,</span><br><span class="line">    <span class="string">'Z'</span> : <span class="string">'--..'</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>위와 같은 딕셔너리에서 키와 밸류를 바꿔야 한다고 하겠습니다.<br>아래처럼 새 딕셔너리를 만든 뒤 for 문으로 키와 밸류의 자리를 바꿔 지정해주면 됩니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">new_morse = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> morse.items():</span><br><span class="line">    new_morse[v] = k</span><br><span class="line"></span><br><span class="line">print(new_morse)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&apos;.-&apos;: &apos;A&apos;, &apos;-...&apos;: &apos;B&apos;, &apos;-.-.&apos;: &apos;C&apos;, &apos;-..&apos;: &apos;D&apos;, &apos;.&apos;: &apos;E&apos;, &apos;..-.&apos;: &apos;F&apos;, &apos;--.&apos;: &apos;G&apos;, &apos;....&apos;: &apos;H&apos;, &apos;..&apos;: &apos;I&apos;, &apos;.---&apos;: &apos;J&apos;, &apos;-.-&apos;: &apos;K&apos;, &apos;.-..&apos;: &apos;L&apos;, &apos;--&apos;: &apos;M&apos;, &apos;-.&apos;: &apos;N&apos;, &apos;---&apos;: &apos;O&apos;, &apos;.--.&apos;: &apos;P&apos;, &apos;--.-&apos;: &apos;Q&apos;, &apos;.-.&apos;: &apos;R&apos;, &apos;...&apos;: &apos;S&apos;, &apos;-&apos;: &apos;T&apos;, &apos;..-&apos;: &apos;U&apos;, &apos;...-&apos;: &apos;V&apos;, &apos;.--&apos;: &apos;W&apos;, &apos;-..-&apos;: &apos;X&apos;, &apos;-.--&apos;: &apos;Y&apos;, &apos;--..&apos;: &apos;Z&apos;&#125;</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/Python/">Python</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Deep-Learning-with-Python-Ch-06" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/02/15/Deep-Learning-with-Python-Ch-06/">Deep Learning with Python - Ch.06</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/02/15/Deep-Learning-with-Python-Ch-06/" class="article-date">
	  <time datetime="2019-02-15T09:35:43.000Z" itemprop="datePublished">February 15, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p><a href="https://ridibooks.com/v2/Detail?id=754024868&amp;_s=search&amp;_q=%EC%BC%80%EB%9D%BC%EC%8A%A4" target="_blank" rel="noopener">케라스 창시자에게 배우는 딥러닝</a>을 실습하면서 정리한 포스트입니다. 코드 예제와 코드 설명은 <a href="https://github.com/rickiepark/deep-learning-with-python-notebooks" target="_blank" rel="noopener">역자 깃허브</a>에서 받아볼 수 있습니다. 출판물이고 개인적으로만 참고하기 위한 요약노트이다 보니 설명이 불친절한 점은 양해 바랍니다. 보다 자세한 내용을 원하시는 분은 위 링크의 책을 참고하시기 바랍니다.</p>
<hr>
<p><strong>익힐 것</strong></p>
<ul>
<li>원본 텍스트를 신경망이 처리할 수 있는 형태로 변환</li>
<li>케라스 모델에 임베딩 층을 추가해 특정 작업에 특화된 토큰 임베딩 학습</li>
<li>데이터가 부족한 자연어 처리 문제에서 사전 훈련된 단어 임베딩을 사용해 성능 높이기</li>
<li>RNN과 동작방법</li>
<li>LSTM과 이게 긴 시퀀스에서 단순 RNN보다 더 잘 작동하는 이유</li>
<li>케라스의 RNN 층을 사용해 시퀀스 데이터 처리하기</li>
</ul>
<h2 id="6장-텍스트와-시퀀스를-위한-딥러닝"><a href="#6장-텍스트와-시퀀스를-위한-딥러닝" class="headerlink" title="6장. 텍스트와 시퀀스를 위한 딥러닝"></a>6장. 텍스트와 시퀀스를 위한 딥러닝</h2><ul>
<li>단어의 시퀀스, 문자의 시퀀스, 시계열 또는 일반적인 시퀀스 데이터를 처리하는 딥러닝 모델.</li>
<li>크게 두 가지<ul>
<li>순환 신경망(Recurrent Neural Network)</li>
<li>1D 컨브넷</li>
</ul>
</li>
<li>사용처<ul>
<li>문서 분류나 시계열 분류, 글의 주제나 책의 저자 식별</li>
<li>시계열 비교. 두 문서나 두 주식 가격이 얼마나 밀접하게 관련 있는지 추정</li>
<li>시퀀스 투 시퀀스 학습. 번역</li>
<li>감성 분석. 트윗이나 영화 리뷰가 긍정적인가 부정적인가</li>
<li>시계열 예측. 최근 날씨 데이터로 향후 날씨 예측하기</li>
</ul>
</li>
</ul>
<h3 id="텍스트-데이터-다루기"><a href="#텍스트-데이터-다루기" class="headerlink" title="텍스트 데이터 다루기"></a>텍스트 데이터 다루기</h3><ul>
<li>문서 분류, 감성 분석, 저자 식별, 질문 응답 등</li>
<li>텍스트 벡터화(텍스트를 수치형 텐서로 변환하는 과정)<ul>
<li>텍스트를 단어로 나누고 각 단어를 하나의 벡터로 변환</li>
<li>텍스트를 문자로 나누고 각 단어를 하나의 벡터로 변환</li>
<li>텍스트에서 단어나 문자의 n-그램을 추출해 각 n-그램을 하나의 벡터로 변환</li>
</ul>
</li>
<li>토큰<ul>
<li>텍스트를 나누는 단위(단어, 문자, n-그램)</li>
</ul>
</li>
<li>토큰화<ul>
<li>텍스트를 토큰으로 나누는 작업</li>
</ul>
</li>
<li>토큰과 벡터를 연결하는 방법<ul>
<li>원핫 인코딩</li>
<li>토큰 임베딩(단어 임베딩)</li>
</ul>
</li>
</ul>
<h4 id="단어와-문자의-원핫-인코딩"><a href="#단어와-문자의-원핫-인코딩" class="headerlink" title="단어와 문자의 원핫 인코딩"></a>단어와 문자의 원핫 인코딩</h4><ul>
<li>토큰을 벡터로 변환하는 가장 기본적인 방법</li>
</ul>
<p><strong>단어 수준의 원핫 인코딩_케라스</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</span><br><span class="line">samples = [<span class="string">"The cat sat on the mat"</span>, <span class="string">"The dog ate my homework."</span>]</span><br><span class="line">tokenizer = Tokenizer(num_words=<span class="number">1000</span>)</span><br><span class="line">tokenizer.fit_on_texts(samples)</span><br><span class="line">sequences = tokenizer.texts_to_sequences(samples)</span><br><span class="line">print(sequences)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">one_hot_results = tokenizer.texts_to_matrix(samples, mode=<span class="string">'binary'</span>)</span><br><span class="line">print(one_hot_results)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[0. 1. 1. ... 0. 0. 0.]</span><br><span class="line"> [0. 1. 0. ... 0. 0. 0.]]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">word_index = tokenizer.word_index</span><br><span class="line">word_index</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;&apos;the&apos;: 1,</span><br><span class="line"> &apos;cat&apos;: 2,</span><br><span class="line"> &apos;sat&apos;: 3,</span><br><span class="line"> &apos;on&apos;: 4,</span><br><span class="line"> &apos;mat&apos;: 5,</span><br><span class="line"> &apos;dog&apos;: 6,</span><br><span class="line"> &apos;ate&apos;: 7,</span><br><span class="line"> &apos;my&apos;: 8,</span><br><span class="line"> &apos;homework&apos;: 9&#125;</span><br></pre></td></tr></table></figure>
<p><strong>원핫 해싱</strong></p>
<ul>
<li>원핫인코딩의 변종. 어휘 사전에 있는 고유 토큰 수가 너무 커서 모두 다루기 어려울 때 사용</li>
<li>각 단어에 명시적으로 인덱스를 할당하고 이 인덱스를 단어를 해싱해 고정된 크기의 벡터로 변환</li>
<li>장점<ul>
<li>명시적인 단어 인덱스가 필요 없어서 메모리 절약</li>
</ul>
</li>
<li>단점<ul>
<li>해시 충돌: 2개의 단어가 같은 해시를 만들면 모델이 차이를 인식하지 못함</li>
<li>해싱 공간의 차원이 해싱될 고유 토큰의 전체 수보다 훨씬 크면 충돌 가능성은 감소</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">samples = [<span class="string">'The cat sat on the mat.'</span>, <span class="string">'The dog ate my homework.'</span>]</span><br><span class="line">dimensionality = <span class="number">1000</span> <span class="comment"># 단어를 크기가 1000인 벡터로 저장.</span></span><br><span class="line"><span class="comment"># 1000개 이상의 단어가 있다면 해싱 충돌 가능성 up</span></span><br><span class="line"></span><br><span class="line">max_length = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">results = np.zeros((len(samples), max_length, dimensionality))</span><br><span class="line"><span class="keyword">for</span> i, sample <span class="keyword">in</span> enumerate(samples):</span><br><span class="line">    <span class="keyword">for</span> j, word <span class="keyword">in</span> list(enumerate(sample.split()))[:max_length]:</span><br><span class="line">        index = abs(hash(word)) % dimensionality</span><br><span class="line">        <span class="comment"># 단어를 해싱해 0과 1000사이의 랜덤한 정수 인덱스로 변환</span></span><br><span class="line">        results[i, j, index] = <span class="number">1.</span></span><br><span class="line">print(results.shape)</span><br><span class="line">print(results)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">(2, 10, 1000)</span><br><span class="line">[[[0. 0. 0. ... 0. 0. 0.]</span><br><span class="line">  [0. 0. 0. ... 0. 0. 0.]</span><br><span class="line">  [0. 0. 0. ... 0. 0. 0.]</span><br><span class="line">  ...</span><br><span class="line">  [0. 0. 0. ... 0. 0. 0.]</span><br><span class="line">  [0. 0. 0. ... 0. 0. 0.]</span><br><span class="line">  [0. 0. 0. ... 0. 0. 0.]]</span><br><span class="line"></span><br><span class="line"> [[0. 0. 0. ... 0. 0. 0.]</span><br><span class="line">  [0. 0. 0. ... 0. 0. 0.]</span><br><span class="line">  [0. 0. 0. ... 0. 0. 0.]</span><br><span class="line">  ...</span><br><span class="line">  [0. 0. 0. ... 0. 0. 0.]</span><br><span class="line">  [0. 0. 0. ... 0. 0. 0.]</span><br><span class="line">  [0. 0. 0. ... 0. 0. 0.]]]</span><br></pre></td></tr></table></figure>
<h4 id="단어-임베딩-사용하기"><a href="#단어-임베딩-사용하기" class="headerlink" title="단어 임베딩 사용하기"></a>단어 임베딩 사용하기</h4><ul>
<li>밀집 단어 벡터를 사용하는 방법</li>
<li>원핫 인코딩으로 만든 벡터는 sparse하고 고차원(어휘 사전의 단어 수와 같은 차원)</li>
<li>단어 임베딩은 저차원의 실수형 벡터</li>
<li>단어 임베딩은 데이터로부터 학습됨</li>
<li>256, 512차원 또는 큰 어휘 사전을 다룰 때는 1024차원 단어 임베딩 사용(원핫인코딩은 20000차원 이상일 때가 많음)</li>
<li>만드는 법<ul>
<li>관심 대상 문제와 함께 단어 임베딩을 학습. 랜덤한 단어 벡터로 시작해서 신경망 가중치 학습하는 방식으로 학습</li>
<li>다른 머신러닝 작업에서 미리 계산된 단어 임베딩을 로드(pretrained word embedding)</li>
</ul>
</li>
</ul>
<p><strong>임베딩 층을 사용해 단어 임베딩 학습하기</strong></p>
<ul>
<li>언어를 기하학적 공간에 매핑</li>
<li>새로운 작업에는 새로운 임베딩. 케라스를 사용해 임베딩 층의 가중치를 학습</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 임베딩 층의 객체 생성하기</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Embedding</span><br><span class="line">embedding_layer = Embedding(<span class="number">1000</span>, <span class="number">64</span>)</span><br><span class="line"><span class="comment"># 임베딩 층은 적어도 2개의 매개변수를 받음</span></span><br><span class="line"><span class="comment"># 가능한 토큰의 개수(1000=단어 인덱스 최댓값 + 1)와 임베딩 차원(64)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>임베딩 층을 (특정 단어를 나타내는) 정수 인덱스를 밀집 벡터로 매핑하는 딕셔널로 이해하자</li>
<li>정수를 입력으로 받아 내부 딕셔너리에서 이 정수에 연관된 벡터를 찾아 반환함</li>
<li>임베딩 층은 크기가 (samples, sequence_length)인 2D 정수 텐서를 입력으로 받음. 각 샘플은 정수의 시퀀스<br>-</li>
</ul>
<h4 id="모든-내용을-적용하기-원본-텍스트에서-단어-임베딩까지"><a href="#모든-내용을-적용하기-원본-텍스트에서-단어-임베딩까지" class="headerlink" title="모든 내용을 적용하기: 원본 텍스트에서 단어 임베딩까지"></a>모든 내용을 적용하기: 원본 텍스트에서 단어 임베딩까지</h4><h4 id="정리"><a href="#정리" class="headerlink" title="정리"></a>정리</h4><h3 id="순환-신경망-이해하기"><a href="#순환-신경망-이해하기" class="headerlink" title="순환 신경망 이해하기"></a>순환 신경망 이해하기</h3><h4 id="케라스의-순환-층"><a href="#케라스의-순환-층" class="headerlink" title="케라스의 순환 층"></a>케라스의 순환 층</h4><h4 id="LSTM과-GRU-층-이해하기"><a href="#LSTM과-GRU-층-이해하기" class="headerlink" title="LSTM과 GRU 층 이해하기"></a>LSTM과 GRU 층 이해하기</h4><h4 id="케라스를-사용한-LSTM-예제"><a href="#케라스를-사용한-LSTM-예제" class="headerlink" title="케라스를 사용한 LSTM 예제"></a>케라스를 사용한 LSTM 예제</h4><h3 id="순환-신경망의-고급-사용법"><a href="#순환-신경망의-고급-사용법" class="headerlink" title="순환 신경망의 고급 사용법"></a>순환 신경망의 고급 사용법</h3><h4 id="기온-예측-문제"><a href="#기온-예측-문제" class="headerlink" title="기온 예측 문제"></a>기온 예측 문제</h4><h4 id="데이터-준비"><a href="#데이터-준비" class="headerlink" title="데이터 준비"></a>데이터 준비</h4><h4 id="상식-수준의-기준점"><a href="#상식-수준의-기준점" class="headerlink" title="상식 수준의 기준점"></a>상식 수준의 기준점</h4><h4 id="기본적인-머신-러닝-방법"><a href="#기본적인-머신-러닝-방법" class="headerlink" title="기본적인 머신 러닝 방법"></a>기본적인 머신 러닝 방법</h4><h4 id="첫-번째-순환-신경망"><a href="#첫-번째-순환-신경망" class="headerlink" title="첫 번째 순환 신경망"></a>첫 번째 순환 신경망</h4><h4 id="과대적합을-줄이기-위해-순환-드랍아웃-사용하기"><a href="#과대적합을-줄이기-위해-순환-드랍아웃-사용하기" class="headerlink" title="과대적합을 줄이기 위해 순환 드랍아웃 사용하기"></a>과대적합을 줄이기 위해 순환 드랍아웃 사용하기</h4><h4 id="스태킹-순환-층"><a href="#스태킹-순환-층" class="headerlink" title="스태킹 순환 층"></a>스태킹 순환 층</h4><h4 id="양방향-RNN-사용하기"><a href="#양방향-RNN-사용하기" class="headerlink" title="양방향 RNN 사용하기"></a>양방향 RNN 사용하기</h4><h3 id="컨브넷을-사용한-시퀀스-처리"><a href="#컨브넷을-사용한-시퀀스-처리" class="headerlink" title="컨브넷을 사용한 시퀀스 처리"></a>컨브넷을 사용한 시퀀스 처리</h3><h4 id="시퀀스-데이터를-위한-1D-합성곱-이해하기"><a href="#시퀀스-데이터를-위한-1D-합성곱-이해하기" class="headerlink" title="시퀀스 데이터를 위한 1D 합성곱 이해하기"></a>시퀀스 데이터를 위한 1D 합성곱 이해하기</h4><h4 id="시퀀스-데이터를-위한-1D-풀링"><a href="#시퀀스-데이터를-위한-1D-풀링" class="headerlink" title="시퀀스 데이터를 위한 1D 풀링"></a>시퀀스 데이터를 위한 1D 풀링</h4><h4 id="1D-컨브넷-구현"><a href="#1D-컨브넷-구현" class="headerlink" title="1D 컨브넷 구현"></a>1D 컨브넷 구현</h4><h4 id="CNN과-RNN을-연결해-긴-시퀀스-처리"><a href="#CNN과-RNN을-연결해-긴-시퀀스-처리" class="headerlink" title="CNN과 RNN을 연결해 긴 시퀀스 처리"></a>CNN과 RNN을 연결해 긴 시퀀스 처리</h4><h3 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h3>
      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/DSBooks/">DSBooks</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Keras-Deep-Learning-with-Python/">Keras, Deep Learning with Python</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Deep-Learning-with-Python-Ch-05" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/02/15/Deep-Learning-with-Python-Ch-05/">Deep Learning with Python - Ch.05</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/02/15/Deep-Learning-with-Python-Ch-05/" class="article-date">
	  <time datetime="2019-02-15T09:32:56.000Z" itemprop="datePublished">February 15, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p><a href="https://ridibooks.com/v2/Detail?id=754024868&amp;_s=search&amp;_q=%EC%BC%80%EB%9D%BC%EC%8A%A4" target="_blank" rel="noopener">케라스 창시자에게 배우는 딥러닝</a>을 실습하면서 정리한 포스트입니다. 코드 예제와 코드 설명은 <a href="https://github.com/rickiepark/deep-learning-with-python-notebooks" target="_blank" rel="noopener">역자 깃허브</a>에서 받아볼 수 있습니다. 출판물이고 개인적으로만 참고하기 위한 요약노트이다 보니 설명이 불친절한 점은 양해 바랍니다. 보다 자세한 내용을 원하시는 분은 위 링크의 책을 참고하시기 바랍니다.</p>
<hr>
<h2 id="5장-컴퓨터-비전을-위한-딥러닝"><a href="#5장-컴퓨터-비전을-위한-딥러닝" class="headerlink" title="5장. 컴퓨터 비전을 위한 딥러닝"></a>5장. 컴퓨터 비전을 위한 딥러닝</h2><p>6장(텍스트)부터 학습 뒤 볼 예정입니다.</p>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/DSBooks/">DSBooks</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Keras-Deep-Learning-with-Python/">Keras, Deep Learning with Python</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Deep-Learning-with-Python-Ch-04" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/02/14/Deep-Learning-with-Python-Ch-04/">Deep Learning with Python - Ch.04</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/02/14/Deep-Learning-with-Python-Ch-04/" class="article-date">
	  <time datetime="2019-02-14T02:00:45.000Z" itemprop="datePublished">February 14, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p><a href="https://ridibooks.com/v2/Detail?id=754024868&amp;_s=search&amp;_q=%EC%BC%80%EB%9D%BC%EC%8A%A4" target="_blank" rel="noopener">케라스 창시자에게 배우는 딥러닝</a>을 실습하면서 정리한 포스트입니다. 코드 예제와 코드 설명은 <a href="https://github.com/rickiepark/deep-learning-with-python-notebooks" target="_blank" rel="noopener">역자 깃허브</a>에서 받아볼 수 있습니다. 출판물이고 개인적으로만 참고하기 위한 요약노트이다 보니 설명이 불친절한 점은 양해 바랍니다. 보다 자세한 내용을 원하시는 분은 위 링크의 책을 참고하시기 바랍니다.</p>
<hr>
<h2 id="4장-머신러닝의-기본-요소"><a href="#4장-머신러닝의-기본-요소" class="headerlink" title="4장. 머신러닝의 기본 요소"></a>4장. 머신러닝의 기본 요소</h2><h3 id="머신러닝의-4가지-분류"><a href="#머신러닝의-4가지-분류" class="headerlink" title="머신러닝의 4가지 분류"></a>머신러닝의 4가지 분류</h3><h4 id="지도학습"><a href="#지도학습" class="headerlink" title="지도학습"></a>지도학습</h4><ul>
<li>가장 흔한 경우. 앞의 예제들과 광학 문자 판독, 음성 인식, 이미지 분류, 번역 등</li>
<li>대부분 회귀지만 이런 변종도 있음<ul>
<li>sequence generation: 사진이 주어지면 이를 설명하는 캡션 생성</li>
<li>syntax tree expectation: 문장이 주어지면 분해된 구문 트리를 예측</li>
<li>object detection: 사진 안의 특정 물체에 bounding box를 그림</li>
<li>image segmentation: 사진을 픽셀 단위로 특정 물체에 masking</li>
</ul>
</li>
</ul>
<h4 id="비지도학습"><a href="#비지도학습" class="headerlink" title="비지도학습"></a>비지도학습</h4><ul>
<li>타깃 사용하지 않고 입력에 대한 흥미로운 변환을 찾는다</li>
<li>데이터 시각화, 데이터 압축, 데이터 노이즈 제거, 상관관계 이해에 사용</li>
<li>차원 축소와 클러스터링</li>
</ul>
<h4 id="자기지도학습"><a href="#자기지도학습" class="headerlink" title="자기지도학습"></a>자기지도학습</h4><ul>
<li>지도학습의 특별한 경우. 지도학습이지만 사람이 만든 라벨을 사용하지 않음</li>
<li>라벨이 필요하지만 휴리스틱 알고리즘(경험적인 알고리즘)을 사용해 입력 데이터에서 생성</li>
<li>ex) 오토인코더, 지난 프레임이 주어졌을 때 다음 프레임을 예측, 단어가 주어졌을때 다음 단어를 예측</li>
</ul>
<h4 id="강화학습"><a href="#강화학습" class="headerlink" title="강화학습"></a>강화학습</h4><ul>
<li>자율주행 자동차, 자원 관리, 교육 등에서 애플리케이션 등장 예상됨</li>
</ul>
<h3 id="머신러닝-모델-평가"><a href="#머신러닝-모델-평가" class="headerlink" title="머신러닝 모델 평가"></a>머신러닝 모델 평가</h3><ul>
<li>과대적합을 완화하고 일반화를 최대화하기 위한 전략(처음 본 데이터에서 잘 작동하는 모델 찾기)</li>
</ul>
<h4 id="훈련-검증-테스트셋"><a href="#훈련-검증-테스트셋" class="headerlink" title="훈련, 검증, 테스트셋"></a>훈련, 검증, 테스트셋</h4><ul>
<li>데이터가 적을 때 데이터셋을 나누려면 다음과 같은 고급 기법이 도움이 됨<ul>
<li>단순 홀드아웃 검증: 데이터 일부를 테스트셋으로 떼어 둠</li>
<li>K-겹 교차검증: 데이터를 동일한 크기를 가진 K개 분할로 나눠 각 분할 i에 대해 남은 K-1개의 분할로 모델을 훈련하고 분할 i 에서 모델을 평가</li>
<li>셔플링을 사용한 반복 K-겹 교차 검증: K개의 분할로 나누기 전에 매번 데이터를 무작위로 섞기</li>
</ul>
</li>
</ul>
<h4 id="기억해야-할-것"><a href="#기억해야-할-것" class="headerlink" title="기억해야 할 것"></a>기억해야 할 것</h4><ul>
<li>대표성 있는 데이터를 골라야 한다. 타깃이 0~9까지 9가지 숫자인데 테스트셋에 타깃이 0~7까지 있는 데이터만 넣는다면?</li>
<li>시간의 방향: 과거로부터 미래를 예측하려고 한다면 테스트셋에 있는 데이터가 트레이닝셋 데이터보다 미래에 있어야 한다</li>
<li>데이터 중복: 한 데이터셋에 같은 데이터가 두 번 등장하면 트레이닝셋의 일부로 테스트를 하는 일이 발생할 수 있다.</li>
</ul>
<h3 id="데이터-전처리-피쳐-엔지니어링-피쳐-학습"><a href="#데이터-전처리-피쳐-엔지니어링-피쳐-학습" class="headerlink" title="데이터 전처리, 피쳐 엔지니어링, 피쳐 학습"></a>데이터 전처리, 피쳐 엔지니어링, 피쳐 학습</h3><h4 id="신경망을-위한-데이터-전처리"><a href="#신경망을-위한-데이터-전처리" class="headerlink" title="신경망을 위한 데이터 전처리"></a>신경망을 위한 데이터 전처리</h4><ul>
<li>원본 데이터를 신경망에 적용하기 쉽게 만들기 위해 데이터를 전처리</li>
<li>벡터화<ul>
<li>신경망에서 모든 입력과 타깃은 부동 소수 데이터로 이뤄진 텐서여야 함(특정 경우에는 정수로 이뤄진 텐서)</li>
<li>데이터가 사운드 이미지 텍스트 뭐든 일단 텐서로 변환</li>
</ul>
</li>
<li>값 정규화<ul>
<li>(MNIST) 숫자 이미지를 그레이스케일 인코딩인 0~255 사이의 정수로 인코딩. 이를 네트워크에 주입하기 전 float32 타입으로 변경하고 255로 나눠 최종적으로 0~1 사이의 부동 소수 값으로 만듦.</li>
<li>(보스턴 집값) 데이터를 네트워크에 주입하기 전 각 특성을 정규화해 평균 0, 표준편차 1이 되도록 만듦</li>
<li>비교적 큰 값이나 균일하지 않은 데이터를 신경망에 주입하는건 위험(업데이트할 그래디언트가 커져 네트워크가 수렴하는걸 방해함)</li>
<li>대부분 값이 0~1 사이, 모든 특성이 대체로 비슷한 범위를 가질수록 네트워크를 쉽게 학습시킬 수있음</li>
<li>도움이 되는 정규화 방법</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x -= x.mean(axix=<span class="number">0</span>)</span><br><span class="line">x /= x.std(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>누락값 처리<ul>
<li>일반적으로 0이 사전에 정의된 의미 있는 값이 아니라면 누락값을 0으로 처리해도 괜찮음</li>
<li>트레이닝셋에는 누락값이 없는데 테스트셋에 누락값이 있을 가능성이 있다면 트레이닝셋에 고의적으로 누락값이 있는 샘플을 만들어야 함</li>
</ul>
</li>
</ul>
<h4 id="특성-공학"><a href="#특성-공학" class="headerlink" title="특성 공학"></a>특성 공학</h4><ul>
<li>모델이 수월하게 작업할 수 있는 어떤 방식으로 데이터가 표현될 필요</li>
</ul>
<h3 id="과대적합과-과소적합"><a href="#과대적합과-과소적합" class="headerlink" title="과대적합과 과소적합"></a>과대적합과 과소적합</h3><ul>
<li>언더피팅<ul>
<li>훈련 데이터의 손실이 낮아질수록 테스트 데이터의 손실도 낮아짐</li>
<li>모델의 성능이 계속 발전될 여지가 있음</li>
</ul>
</li>
<li>오버피팅<ul>
<li>어느 시점부터 일반화 성능이 더 높아지지 않음</li>
<li>검증 세트의 성능이 멈추고 감소하기 시작</li>
<li>훈련 데이터에 특화된 패턴을 학습하기 시작했다는 의미</li>
</ul>
</li>
<li>regularization<ul>
<li>과대적합을 피하는 저리 과정</li>
</ul>
</li>
</ul>
<h4 id="네트워크-크기-축소"><a href="#네트워크-크기-축소" class="headerlink" title="네트워크 크기 축소"></a>네트워크 크기 축소</h4><ul>
<li>오버피팅을 막는 가장 단순한 방법은 모델에 있는 학습 파라미터의 수를 줄이는 것</li>
<li>파라미터의 수(모델의 용량)는 층의 수와 각 층의 유닛 수에 의해 결정</li>
<li>언더피팅되지 않도록 충분한 파라미터를 가진 모델을 사용해야 함.</li>
<li>데이터에 알맞은 모델 크기를 찾으려면 각기 다른 구조를 평가해봐야 함<ul>
<li>비교적 적은 수의 층과 파라미터로 시작해 검증 손실이 감소되기 시작할 때까지 층이나 유닛 수를 늘리는게 일반적인 작업 흐름</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 원본 모델</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 작은 용량의 모델</span></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">6</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">6</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 큰 용량의 모델</span></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">1024</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">1024</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>작은 네트워크가 기본 네트워크보다 더 나중에 과대적합되기 시작. 과대적합이 시작됐을 때 성능이 더 천천히 감소</li>
<li>용량이 큰 네트워크는 빨리 과대적합이 시작돼 갈수록 더 심해짐. 검증손실도 불안정</li>
<li>용량이 큰 네트워크일수록 빠르게 훈련 데이터를 모델링하지만 과대적합에 민감해짐(트레이닝과 테스트 손실 사이 차이 발생)</li>
</ul>
<h4 id="가중치-규제-추가"><a href="#가중치-규제-추가" class="headerlink" title="가중치 규제 추가"></a>가중치 규제 추가</h4><ul>
<li>오캄의 면도날 이론<ul>
<li>두 가지 설명이 있다면 더 적은 가정이 필요한 간단한 설명이 옳다는 이론</li>
<li>신경망 학습모델에도 적용됨. 복잡한 모델이 간단한 모델보다 과대적합될 가능성이 높음</li>
</ul>
</li>
<li>간단한 모델이란 파라미터 값 분포의 엔트로피가 작은 모델(혹은 적은 수의 파라미터를 가진 모델)</li>
<li>과대적합 완화법: 네트워크의 복잡도에 제한을 둬서 가중치가 작은 값을 가지도록 강제하는 것</li>
<li>가중치 값의 분포가 더 균일해짐(가중치 규제) -&gt; 네트워크의 손실 함수에 큰 가중치에 연관된 두 가지 형태의 비용을 추가함<ul>
<li>L1 규제: 가중치의 절댓값에 비례하는 비용이 추가됨(가중치의 L1 norm)</li>
<li>L2 규제(=가중치 감쇠, weight decay): 가중치의 제곱에 비례하는 비용이 추가됨(가중치의 L2 norm).</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델에 L2 가중치 추가하기</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> regularizers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, kernel_regularizer=regularizers.l2(<span class="number">0.001</span>),</span><br><span class="line">                       activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, kernel_regularizer=regularizers.l2(<span class="number">0.001</span>),</span><br><span class="line">                       activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>l2(0.001)는 가중치 행렬의 모든 원소를 제곱하고 0.001을 곱해 네트워크 전체 손실에 더해진다는 의미. 이 페널티 항은 트레이닝에서만 추가됨</li>
<li>L2 규제를 사용한 모델이 사용하지 않은 모델보다 과대적합을 잘 견딤(에포크 반복에 따라 loss가 덜 오름)</li>
<li>L2 규제 대신 사용 가능한 옵션<ul>
<li>L1 규제 <code>regularizers.l1(0.001)</code></li>
<li>L1, L2 규제 병행 <code>regularizers.l1_l2(l1=0.001, l2=0.001)</code></li>
</ul>
</li>
</ul>
<h4 id="dropout-추가"><a href="#dropout-추가" class="headerlink" title="dropout 추가"></a>dropout 추가</h4><ul>
<li>네트워크 층에 드랍아웃을 적용하면 트레이닝 동안 랜덤으로 층의 일부 출력 특성을 제외시킴(0으로..)<ul>
<li>ex) 한 층이 트레이닝되는 동안 어떤 입력샘플에 대해 [0.2, 0.5, 1.3, 0.8, 1.1] 벡터를 출력한다고 가정하면, 일부가 무작위로 0이 됨([0, 0.5, 1.3, 0, 1.1]</li>
</ul>
</li>
<li>드랍아웃 비율은 0이 될 특성의 비율(대개 0.2~0.5 로 지정)</li>
<li><p>테스트 단계에서는 드랍아웃이 일어나지 않는다</p>
</li>
<li><p><code>layer_output *= np.random.randint(0, high=2, size=layer_output.shape)</code> : 트레이닝시 유닛의 출력 중 50%를 버림</p>
</li>
<li>테스트 시 드랍아웃 비율로 출력을 낮춰야: <code>layer_output *= 0.5</code></li>
<li>드랍아웃이 과대적합을 줄이는 원리<ul>
<li>층의 출력값에 노이즈를 추가해 중요하지 않은 우연한 패턴을 깨뜨림</li>
</ul>
</li>
<li>케라스에선 층의 출력 바로 뒤에 Dropout 층을 추가해 네트워크에 드랍아웃 적용 가능</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># IMDB 네트워크에 드랍아웃 추가</span></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<h3 id="보편적인-머신러닝-작업-흐름"><a href="#보편적인-머신러닝-작업-흐름" class="headerlink" title="보편적인 머신러닝 작업 흐름"></a>보편적인 머신러닝 작업 흐름</h3><h4 id="문제-정의와-데이터셋-수집"><a href="#문제-정의와-데이터셋-수집" class="headerlink" title="문제 정의와 데이터셋 수집"></a>문제 정의와 데이터셋 수집</h4><ul>
<li>무엇을 예측할 것인가</li>
<li>입력 데이터는?</li>
<li>어떤 종류의 문제인가? (이진분류 / 다중분류 / 스칼라 회귀 / 벡터회귀 / 다중 레이블 분류 / 군집 / 생성 / 강화학습)</li>
<li>입력과 출력이 무엇인지</li>
</ul>
<h4 id="성공-지표-선택"><a href="#성공-지표-선택" class="headerlink" title="성공 지표 선택"></a>성공 지표 선택</h4><ul>
<li>클래스 분포가 균일한 분류 문제<ul>
<li>정확도와 ROC AUC</li>
</ul>
</li>
<li>클래스 분폴가 균일하지 않은 문제<ul>
<li>정밀도와 재현율</li>
</ul>
</li>
<li>랭킹 문제나 다중 레이블 문제<ul>
<li>평균 정밀도</li>
</ul>
</li>
</ul>
<h4 id="평가-방법-선택"><a href="#평가-방법-선택" class="headerlink" title="평가 방법 선택"></a>평가 방법 선택</h4><ul>
<li>현재의 진척 상황 평가법<ul>
<li>홀드아웃 검증 세트 분리(데이터가 풍부할 때)</li>
<li>K-겹 교차 검증(샘플 수가 너무 적을 때)</li>
<li>반복 K-겹 교차 검증(데이터가 적고 정확한 모델 평가 필요시)</li>
</ul>
</li>
</ul>
<h4 id="데이터-준비"><a href="#데이터-준비" class="headerlink" title="데이터 준비"></a>데이터 준비</h4><ul>
<li>머신러닝 모델을 심층 신경망이라 가정<ul>
<li>데이터는 텐서로 구성</li>
<li>텐서에 있는 값은 일반적으로 작은 값으로 스케일 조정돼 있음 [-1, 1] or [0, 1]</li>
<li>특성마다 범위가 다르면 정규화</li>
<li>피처 엔지니어링</li>
</ul>
</li>
</ul>
<h4 id="기본보다-나은-모델-훈련하기"><a href="#기본보다-나은-모델-훈련하기" class="headerlink" title="기본보다 나은 모델 훈련하기"></a>기본보다 나은 모델 훈련하기</h4><ul>
<li>통계적 검정력을 달성하는게 목표</li>
<li>MNIST에서 통계적 검정력을 달성하려면 0.1보다 높은 정확도를 내는 모델이어야 함</li>
<li>모델을 위해 고려할 세 가지<ul>
<li>마지막 층의 활성화 함수: 네트워크 출력에 필요한 제한을 가함. IMDB 분류에선 마지막 층에 시그모이드 함수 사용. 회귀에서는 마지막 층에 활성화 함수 사용 안함</li>
<li>손실 함수: 풀려고 하는 문제의 종류에 적합해야. IMDB에선 binary_crossentropy, 회귀에선 mse.</li>
<li>최적화 설정: 대부분의 경우 rmsprop과 기본 학습률 사용하는게 무난함</li>
</ul>
</li>
<li>손실함수는 미니 배치 데이터에서 계산 가능해야 하고 미분 가능해야 함</li>
<li>문제 유형에 따른 마지막층 활성화 함수와 손실 함수 선택<ul>
<li>이진 분류<ul>
<li>시그모이드 / binary_crossentropy</li>
</ul>
</li>
<li>단일 레이블 다중 분류<ul>
<li>소프트맥스 / categorical_crossentropy</li>
</ul>
</li>
<li>다중 레이블 다중 분류<ul>
<li>시그모이드 / binary_crossentropy</li>
</ul>
</li>
<li>임의 값에 대한 회귀<ul>
<li>없음 / mse</li>
</ul>
</li>
<li>0과 1 가시 값에 대한 회귀<ul>
<li>시그모이드 / mse or binary_crossentropy</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="몸집-키우기-과대적합-모델-구축"><a href="#몸집-키우기-과대적합-모델-구축" class="headerlink" title="몸집 키우기: 과대적합 모델 구축"></a>몸집 키우기: 과대적합 모델 구축</h4><ul>
<li>머신러닝은 최적화와 일반화 사이의 줄다리기<ul>
<li>과소적합과 과대적합 사이</li>
<li>과소용량과 과대용량 사이</li>
</ul>
</li>
<li>얼마나 큰 모델을 만들어야 할까? 일단 과대적합된 모델을 만들어본다<ol>
<li>층을 추가</li>
<li>층의 크기를 키움</li>
<li>더 많은 에포크 동안 트레이닝</li>
</ol>
</li>
<li>훈련 손실과 검증 손실을 모니터링. 검증 데이터에서 모델 성능이 감소하기 시작했을 때 과대적합에 도달한 것</li>
</ul>
<h4 id="모델-규제와-하이퍼파라미터-튜닝"><a href="#모델-규제와-하이퍼파라미터-튜닝" class="headerlink" title="모델 규제와 하이퍼파라미터 튜닝"></a>모델 규제와 하이퍼파라미터 튜닝</h4><ul>
<li>드랍아웃 추가</li>
<li>층을 추가하거나 제거</li>
<li>L1, L2 또는 둘 다를 추가해보기</li>
<li>하이퍼파라미터를 바꿔보기(층의 유닛 수나 옵티마이저의 학습률 등)</li>
<li>피처 엔지니어링</li>
</ul>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/DSBooks/">DSBooks</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Keras-Deep-Learning-with-Python/">Keras, Deep Learning with Python</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  
    <article id="post-Deep-Learning-with-Python-Ch-03" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      <a class="article-title" href="/2019/02/06/Deep-Learning-with-Python-Ch-03/">Deep Learning with Python - Ch.03</a>
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	Posted on <a href="/2019/02/06/Deep-Learning-with-Python-Ch-03/" class="article-date">
	  <time datetime="2019-02-05T17:02:23.000Z" itemprop="datePublished">February 6, 2019</time>
	</a>

      

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <hr>
<p><a href="https://ridibooks.com/v2/Detail?id=754024868&amp;_s=search&amp;_q=%EC%BC%80%EB%9D%BC%EC%8A%A4" target="_blank" rel="noopener">케라스 창시자에게 배우는 딥러닝</a>을 실습하면서 정리한 포스트입니다. 코드 예제와 코드 설명은 <a href="https://github.com/rickiepark/deep-learning-with-python-notebooks" target="_blank" rel="noopener">역자 깃허브</a>에서 받아볼 수 있습니다. 출판물이고 개인적으로만 참고하기 위한 요약노트이다 보니 설명이 불친절한 점은 양해 바랍니다. 보다 자세한 내용을 원하시는 분은 위 링크의 책을 참고하시기 바랍니다.</p>
<hr>
<h2 id="3장-신경망-시작하기"><a href="#3장-신경망-시작하기" class="headerlink" title="3장. 신경망 시작하기"></a>3장. 신경망 시작하기</h2><h3 id="신경망-구조"><a href="#신경망-구조" class="headerlink" title="신경망 구조"></a>신경망 구조</h3><ul>
<li>네트워크(모델)를 구성하는 층</li>
<li>입력데이터, 타겟</li>
<li>손실 함수: 피드백 신호를 정의</li>
<li>옵티마이저: 학습 진행 방식을 결정</li>
</ul>
<h4 id="층-딥러닝의-구성-단위"><a href="#층-딥러닝의-구성-단위" class="headerlink" title="층: 딥러닝의 구성 단위"></a>층: 딥러닝의 구성 단위</h4><ul>
<li>층: 하나 이상의 텐서를 입력받아 하나 이상의 텐서를 출력하는 데이터 처리 모듈</li>
<li>대부분 가중치라는 층의 상태를 가짐(상태가 없는 층도 존재)</li>
<li>가중치는 확률적 경사 하강법에 의해 학습되는 하나 이상의 텐서</li>
<li>층마다 적절한 텐서 포맷과 데이터 처리 방식이 다름<ul>
<li>벡터 데이터(2D 텐서) : 밀집 연결 층</li>
<li>시퀀스 데이터(3D 텐서) : LSTM 같은 순환 층</li>
<li>이미지 데이터(4D 텐서) : Conv2D 클래스. 2D 합성곱 층</li>
</ul>
</li>
<li>케라스는 호환 가능한 층(호환성: 각 층이 특정 크기의 입력 텐서만 받고 특정 크기의 출력 텐서를 반환)을 엮어 데이터 변환 파이프라인을 구성해 딥러딩 모델을 만듦</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 첫 번째 차원이 784인 2D 텐서만 입력으로 받는 층</span></span><br><span class="line"><span class="comment"># 첫 번째 차원 크기가 32로 변환된 텐서를 출력</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line">layer = layers.Dense(<span class="number">32</span>, input_shape=(<span class="number">784</span>,))</span><br></pre></td></tr></table></figure>
<ul>
<li>위 층에는 32차원의 벡터를 입력으로 받는 하위 층이 연결돼야 함</li>
<li>케라스가 모델에 추가된 층을 자동으로 상위 층에 맞춰줌</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">32</span>, input_shape=(<span class="number">784</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>두 번째 층에는 input_shape 매개변수를 지정하지 않음(앞 층의 출력 크기를 입력 크기로 자동으로 채택)</li>
</ul>
<h4 id="모델-층의-네트워크"><a href="#모델-층의-네트워크" class="headerlink" title="모델: 층의 네트워크"></a>모델: 층의 네트워크</h4><ul>
<li>딥러닝 모델은 층으로 만든 Directed Acyclic Graph(DAG)</li>
<li>자주 등장하는 네트워크 구조<ul>
<li>branch가 2개인 네트워크</li>
<li>출력이 여러 개인 네트워크</li>
<li>inception 블록</li>
</ul>
</li>
<li>네트워크 구조는 가설 공간(가능성 있는 공간)을 정의</li>
<li>네트워크 구조 선택 : 가설 공간을 입력 데이터에서 출력 데이터로 매핑하는 일련의 특정 텐서 연산으로 제한</li>
<li>딱 맞는 네트워크 구조 찾기는 과학보다 예술</li>
<li>네트워크 구조 정의 후에는 손실함수와 옵티마이저를 선택해야 함</li>
</ul>
<h4 id="손실-함수와-옵티마이저-학습-과정을-조절하는-열쇠"><a href="#손실-함수와-옵티마이저-학습-과정을-조절하는-열쇠" class="headerlink" title="손실 함수와 옵티마이저: 학습 과정을 조절하는 열쇠"></a>손실 함수와 옵티마이저: 학습 과정을 조절하는 열쇠</h4><ul>
<li>손실 함수: 훈련하는 동안 최소화될 값. 문제에 대한 성공 지표</li>
<li>옵티마이저: 손실 함수를 기반으로 네트워크가 어떻게 업데이트될지 결정.</li>
<li>출력 여러 개를 내는 신경망은 여러 개의 손실 함수를 가질 수 있음</li>
<li>But, 경사 하강법 과정은 하나의 스칼라 손실 값을 기준으로 함</li>
<li>따라서 손실이 여러 개인 네트워크에서는 모든 손실이 (평균을 내서) 하나의 스칼라 양으로 합쳐짐</li>
<li>문제에 따라 올바른 목적 함수 선택해야<ul>
<li>2개의 클래스 분류 문제 : binary crossentropy</li>
<li>여러개 클래스 분류 문제 : categorical crossentropy</li>
<li>회귀 문제 : 평균 제곱 오차</li>
<li>시퀀스 학습 문제 : Connection Temporal Classification</li>
<li>완전히 새로운 연구: 독자적인 목적 함수</li>
</ul>
</li>
</ul>
<h3 id="케라스-소개"><a href="#케라스-소개" class="headerlink" title="케라스 소개"></a>케라스 소개</h3><ul>
<li>생략</li>
</ul>
<h3 id="딥러닝-컴퓨터-셋팅"><a href="#딥러닝-컴퓨터-셋팅" class="headerlink" title="딥러닝 컴퓨터 셋팅"></a>딥러닝 컴퓨터 셋팅</h3><ul>
<li>생략</li>
</ul>
<h3 id="영화-리뷰-분류-이진-분류-예제"><a href="#영화-리뷰-분류-이진-분류-예제" class="headerlink" title="영화 리뷰 분류: 이진 분류 예제"></a>영화 리뷰 분류: 이진 분류 예제</h3><p>리뷰 텍스트를 기반으로 영화 리뷰를 긍정과 부정으로 분류</p>
<h4 id="IMDB-데이터셋"><a href="#IMDB-데이터셋" class="headerlink" title="IMDB 데이터셋"></a>IMDB 데이터셋</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>num_words=10000 : 트레이닝셋에서 가장 많이 등장하는 단어 1만 개만 사용</li>
<li>labels는 0(부정)과 1(긍정)을 나타내는 리스트</li>
</ul>
<h4 id="데이터-준비"><a href="#데이터-준비" class="headerlink" title="데이터 준비"></a>데이터 준비</h4><ul>
<li>숫자 리스트를 신경망에 넣기 위해 텐서로 바꾸는 두 가지 방법<ul>
<li>리스트에 padding을 추가하고 (samples, sequence-length) 크기의 정수 텐서로 변환. 신경망 첫 번째 층으로 사용</li>
<li>리스트를 원핫인코딩해 0, 1 벡터로 변환. (아래 예시)</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 정수 시퀀스를 이진 행렬로 인코딩</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    <span class="comment"># (시퀀스 길이, 차원) 크기의 0행렬 만들기</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># results[i]에서 특정 인덱스 위치를 1로 바꾸기</span></span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터를 벡터로 변환</span></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br></pre></td></tr></table></figure>
<p>x_train.shape, x_test.shape는 각각 (25000, 10000) 모양이 됨</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 라벨을 벡터로 바꾸기</span></span><br><span class="line">y_train = np.asarray(train_labels).astype(<span class="string">'float32'</span>)</span><br><span class="line">y_test = np.asarray(test_labels).astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="신경망-모델-만들기"><a href="#신경망-모델-만들기" class="headerlink" title="신경망 모델 만들기"></a>신경망 모델 만들기</h4><ul>
<li>입력 데이터는 벡터, 라벨은 1 or 0의 스칼라</li>
<li>이런 문제에 잘 작동하는 네트워크는 relu 활성화 함수를 사용한 완전 연결 층(Dense(16, activation=’relu’))을 그냥 쌓은 것</li>
<li>16은 은닉 유닛의 수. 하나의 은닉 유닛은 층이 나타내는 표현 공간에서 하나의 차원이 됨.</li>
<li><code>output = relu(dot(W, input) + b)</code></li>
<li>16개 은닉 유닛이 있다는 건 가중치 행렬 W의 크기가 (input_dimension, 16)이라는 의미. 입력 데이터와 W를 점곱하면 입력 데이터가 16차원으로 표현된 공간으로 투영됨(+ 편향 벡터 b를 더하고 relu 연산 적용)</li>
<li>표현공간의 차원: ‘신경망이 내재된 표현을 학습할 때 가질 수 있는 자유도’</li>
<li>중간의 은닉 층은 활성화 함수로 relu를, 마지막 층은 확률을 출력하기 위해 시그모이드 활성화 함수를 사용.</li>
<li>relu는 음수를 0으로 만드는 함수. 시그모이드는 임의의 값을 [0, 1] 사이로 압축-&gt; 출력 값을 확률처럼 해석 가능</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 위 신경망의 케라스 구현</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>이진 분류 문제고 신경망 출력이 확률 -&gt; binary_crossentropy 나 mean_squared_error</li>
<li>binary_crossentropy<ul>
<li>확률을 출력하는 모델 사용 시 최선의 선택</li>
<li>크로스엔트로피: 확률 분포 간의 차이를 측정(여기선 원본 분포와 예측 분포 사이를 측정)</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델 컴파일하기</span></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>옵티마이저의 매개변수를 바꾸거나 손실함수, 측정함수를 직접 만들어야 할 경우는 아래와 같이 설정</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 옵티마이저 설정하기</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line">model.compile(optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">              loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 손실, 측정함수 객체로 지정하기</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> losses</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=optimizers.RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">             loss=losses.binary_crossentropy,</span><br><span class="line">             metrics=[metrics.binary_accuracy])</span><br></pre></td></tr></table></figure>
<h4 id="훈련-검증"><a href="#훈련-검증" class="headerlink" title="훈련 검증"></a>훈련 검증</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 원본 훈련 데이터에서 1만개 샘플 떼어내 검증 셋 만들기</span></span><br><span class="line">x_val = x_train[:<span class="number">10000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">10000</span>:]</span><br><span class="line">v_val = y_train[:<span class="number">10000</span>]</span><br><span class="line">partial_y_train = y_train[<span class="number">10000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 512개 샘플씩 미니 배치를 만들어 20번의 에포크 동안 훈련</span></span><br><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                    partial_y_train,</span><br><span class="line">                    epochs=<span class="number">20</span>,</span><br><span class="line">                    batch_size=<span class="number">512</span>,</span><br><span class="line">                    validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">Train on 15000 samples, validate on 10000 samples</span><br><span class="line">Epoch 1/20</span><br><span class="line">15000/15000 [==============================] - 3s 200us/step - loss: 0.5084 - acc: 0.7810 - val_loss: 0.3798 - val_acc: 0.8683</span><br><span class="line">Epoch 2/20</span><br><span class="line">15000/15000 [==============================] - 1s 95us/step - loss: 0.3005 - acc: 0.9043 - val_loss: 0.3002 - val_acc: 0.8901</span><br><span class="line">Epoch 3/20</span><br><span class="line">15000/15000 [==============================] - 1s 95us/step - loss: 0.2179 - acc: 0.9289 - val_loss: 0.3083 - val_acc: 0.8711</span><br><span class="line">Epoch 4/20</span><br><span class="line">15000/15000 [==============================] - 2s 124us/step - loss: 0.1750 - acc: 0.9437 - val_loss: 0.2843 - val_acc: 0.8835</span><br><span class="line">Epoch 5/20</span><br><span class="line">15000/15000 [==============================] - 1s 95us/step - loss: 0.1426 - acc: 0.9542 - val_loss: 0.2842 - val_acc: 0.8870</span><br><span class="line">Epoch 6/20</span><br><span class="line">15000/15000 [==============================] - 1s 94us/step - loss: 0.1150 - acc: 0.9653 - val_loss: 0.3154 - val_acc: 0.8772</span><br><span class="line">Epoch 7/20</span><br><span class="line">15000/15000 [==============================] - 1s 95us/step - loss: 0.0978 - acc: 0.9709 - val_loss: 0.3129 - val_acc: 0.8846</span><br><span class="line">Epoch 8/20</span><br><span class="line">15000/15000 [==============================] - 1s 94us/step - loss: 0.0807 - acc: 0.9765 - val_loss: 0.3857 - val_acc: 0.8650</span><br><span class="line">Epoch 9/20</span><br><span class="line">15000/15000 [==============================] - 2s 107us/step - loss: 0.0660 - acc: 0.9820 - val_loss: 0.3636 - val_acc: 0.8782</span><br><span class="line">Epoch 10/20</span><br><span class="line">15000/15000 [==============================] - 2s 134us/step - loss: 0.0561 - acc: 0.9849 - val_loss: 0.3844 - val_acc: 0.8793</span><br><span class="line">Epoch 11/20</span><br><span class="line">15000/15000 [==============================] - 2s 137us/step - loss: 0.0436 - acc: 0.9899 - val_loss: 0.4151 - val_acc: 0.8783</span><br><span class="line">Epoch 12/20</span><br><span class="line">15000/15000 [==============================] - 2s 117us/step - loss: 0.0379 - acc: 0.9920 - val_loss: 0.4542 - val_acc: 0.8684</span><br><span class="line">Epoch 13/20</span><br><span class="line">15000/15000 [==============================] - 2s 109us/step - loss: 0.0300 - acc: 0.9929 - val_loss: 0.4703 - val_acc: 0.8728</span><br><span class="line">Epoch 14/20</span><br><span class="line">15000/15000 [==============================] - 2s 125us/step - loss: 0.0247 - acc: 0.9945 - val_loss: 0.5042 - val_acc: 0.8718</span><br><span class="line">Epoch 15/20</span><br><span class="line">15000/15000 [==============================] - 2s 132us/step - loss: 0.0192 - acc: 0.9964 - val_loss: 0.5316 - val_acc: 0.8704</span><br><span class="line">Epoch 16/20</span><br><span class="line">15000/15000 [==============================] - 2s 109us/step - loss: 0.0164 - acc: 0.9969 - val_loss: 0.5650 - val_acc: 0.8690</span><br><span class="line">Epoch 17/20</span><br><span class="line">15000/15000 [==============================] - 1s 99us/step - loss: 0.0125 - acc: 0.9981 - val_loss: 0.5973 - val_acc: 0.8668</span><br><span class="line">Epoch 18/20</span><br><span class="line">15000/15000 [==============================] - 2s 108us/step - loss: 0.0108 - acc: 0.9983 - val_loss: 0.6285 - val_acc: 0.8670</span><br><span class="line">Epoch 19/20</span><br><span class="line">15000/15000 [==============================] - 2s 109us/step - loss: 0.0079 - acc: 0.9991 - val_loss: 0.7197 - val_acc: 0.8553</span><br><span class="line">Epoch 20/20</span><br><span class="line">15000/15000 [==============================] - 1s 100us/step - loss: 0.0048 - acc: 0.9998 - val_loss: 0.6812 - val_acc: 0.8674</span><br></pre></td></tr></table></figure>
<ul>
<li>model.fit() 메서드는 History 객체를 반환</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 훈련과 검증 손실</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">history_dict = history.history</span><br><span class="line">loss = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'Validation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'훈련과 검증 손실'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/Deep Learning with Python - Ch.03_01.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 훈련과 검증 정확도</span></span><br><span class="line">plt.clf()</span><br><span class="line">acc = history_dict[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history_dict[<span class="string">'val_acc'</span>]</span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'훈련과 검증 정확도'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/Deep Learning with Python - Ch.03_02.png" alt=""></p>
<ul>
<li>훈련 손실은 에포크마다 감소, 훈련 정확도는 에포크마다 증가</li>
<li>트레이닝셋에서 잘 작동하지만 테스트셋에서는 아님(overfitting 됐기 때문)</li>
<li>오버피팅을 막기 위해 세번째 에포크 이후 훈련을 중지</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 처음부터 다시 훈련하기</span></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">             loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">4</span>, batch_size=<span class="number">512</span>)</span><br><span class="line">model.evaluate(x_test, y_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch 1/4</span><br><span class="line">25000/25000 [==============================] - 3s 102us/step - loss: 0.4749 - acc: 0.8216</span><br><span class="line">Epoch 2/4</span><br><span class="line">25000/25000 [==============================] - 2s 76us/step - loss: 0.2659 - acc: 0.9096</span><br><span class="line">Epoch 3/4</span><br><span class="line">25000/25000 [==============================] - 2s 70us/step - loss: 0.1983 - acc: 0.9298</span><br><span class="line">Epoch 4/4</span><br><span class="line">25000/25000 [==============================] - 2s 67us/step - loss: 0.1678 - acc: 0.9403</span><br><span class="line">25000/25000 [==============================] - 3s 140us/step</span><br><span class="line">[0.3244189430713654, 0.87316]</span><br></pre></td></tr></table></figure>
<ul>
<li>87%의 정확도 달성</li>
</ul>
<h4 id="훈련된-모델로-새로운-데이터에-대해-예측하기"><a href="#훈련된-모델로-새로운-데이터에-대해-예측하기" class="headerlink" title="훈련된 모델로 새로운 데이터에 대해 예측하기"></a>훈련된 모델로 새로운 데이터에 대해 예측하기</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.predict(x_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">array([[0.23489225],</span><br><span class="line">       [0.99956626],</span><br><span class="line">       [0.95799285],</span><br><span class="line">       ...,</span><br><span class="line">       [0.16514498],</span><br><span class="line">       [0.11655141],</span><br><span class="line">       [0.74928373]], dtype=float32)</span><br></pre></td></tr></table></figure>
<h3 id="뉴스-기사-분류-다중-분류-문제"><a href="#뉴스-기사-분류-다중-분류-문제" class="headerlink" title="뉴스 기사 분류: 다중 분류 문제"></a>뉴스 기사 분류: 다중 분류 문제</h3><ul>
<li>로이터 뉴스를 46개 토픽으로 분류하는 신경망 만들기</li>
<li>각 데이터가 하나의 카테고리로 분류되는 단일 레이블 다중 분류 문제</li>
<li>각 데이터가 여러 개의 카테고리에 속할 수 있다면 다중 레이블 다중 분류 문제</li>
</ul>
<h4 id="로이터-데이터셋"><a href="#로이터-데이터셋" class="headerlink" title="로이터 데이터셋"></a>로이터 데이터셋</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 케라스에서 데이터셋 불러오기</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> reuters</span><br><span class="line"></span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>num_words=10000</code> : 데이터에서 가장 자주 등장하는 단어 10000개로 제한</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 샘플 수 확인하기</span></span><br><span class="line">len(train_data), len(test_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(8982, 2246)</span><br></pre></td></tr></table></figure>
<ul>
<li>트레이닝셋 8982개, 테스트셋 2246개</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line">[1,</span><br><span class="line"> 227,</span><br><span class="line"> 2406,</span><br><span class="line"> 91,</span><br><span class="line"> 2,</span><br><span class="line"> 125,</span><br><span class="line"> 2855,</span><br><span class="line"> 21,</span><br><span class="line"> 4,</span><br><span class="line"> 3976,</span><br><span class="line"> 76,</span><br><span class="line"> 7,</span><br><span class="line"> 4,</span><br><span class="line"> 757,</span><br><span class="line"> 481,</span><br><span class="line"> 3976,</span><br><span class="line"> 790,</span><br><span class="line"> 5259,</span><br><span class="line"> 5654,</span><br><span class="line"> 9,</span><br><span class="line"> 111,</span><br><span class="line"> 149,</span><br><span class="line"> 8,</span><br><span class="line"> 7,</span><br><span class="line"> 10,</span><br><span class="line"> 76,</span><br><span class="line"> 223,</span><br><span class="line"> 51,</span><br><span class="line"> 4,</span><br><span class="line"> 417,</span><br><span class="line"> 8,</span><br><span class="line"> 1047,</span><br><span class="line"> 91,</span><br><span class="line"> 6917,</span><br><span class="line"> 1688,</span><br><span class="line"> 340,</span><br><span class="line"> 7,</span><br><span class="line"> 194,</span><br><span class="line"> 9411,</span><br><span class="line"> 6,</span><br><span class="line"> 1894,</span><br><span class="line"> 21,</span><br><span class="line"> 127,</span><br><span class="line"> 2151,</span><br><span class="line"> 2394,</span><br><span class="line"> 1456,</span><br><span class="line"> 6,</span><br><span class="line"> 3034,</span><br><span class="line"> 4,</span><br><span class="line"> 329,</span><br><span class="line"> 433,</span><br><span class="line"> 7,</span><br><span class="line"> 65,</span><br><span class="line"> 87,</span><br><span class="line"> 1127,</span><br><span class="line"> 10,</span><br><span class="line"> 8219,</span><br><span class="line"> 1475,</span><br><span class="line"> 290,</span><br><span class="line"> 9,</span><br><span class="line"> 21,</span><br><span class="line"> 567,</span><br><span class="line"> 16,</span><br><span class="line"> 1926,</span><br><span class="line"> 24,</span><br><span class="line"> 4,</span><br><span class="line"> 76,</span><br><span class="line"> 209,</span><br><span class="line"> 30,</span><br><span class="line"> 4033,</span><br><span class="line"> 6655,</span><br><span class="line"> 5654,</span><br><span class="line"> 8,</span><br><span class="line"> 4,</span><br><span class="line"> 60,</span><br><span class="line"> 8,</span><br><span class="line"> 4,</span><br><span class="line"> 966,</span><br><span class="line"> 308,</span><br><span class="line"> 40,</span><br><span class="line"> 2575,</span><br><span class="line"> 129,</span><br><span class="line"> 2,</span><br><span class="line"> 295,</span><br><span class="line"> 277,</span><br><span class="line"> 1071,</span><br><span class="line"> 9,</span><br><span class="line"> 24,</span><br><span class="line"> 286,</span><br><span class="line"> 2114,</span><br><span class="line"> 234,</span><br><span class="line"> 222,</span><br><span class="line"> 9,</span><br><span class="line"> 4,</span><br><span class="line"> 906,</span><br><span class="line"> 3994,</span><br><span class="line"> 8519,</span><br><span class="line"> 114,</span><br><span class="line"> 5758,</span><br><span class="line"> 1752,</span><br><span class="line"> 7,</span><br><span class="line"> 4,</span><br><span class="line"> 113,</span><br><span class="line"> 17,</span><br><span class="line"> 12]</span><br></pre></td></tr></table></figure>
<ul>
<li>각 샘플은 정수 리스트</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 텍스트로 디코딩</span></span><br><span class="line">word_index = reuters.get_word_index()</span><br><span class="line">reverse_word_index = dict((value, key) <span class="keyword">for</span> (key, value) <span class="keyword">in</span> word_index.items())</span><br><span class="line">decoded_newswire = <span class="string">' '</span>.join([reverse_word_index.get(i - <span class="number">3</span>, <span class="string">'?'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="number">-1</span>]])</span><br><span class="line">    <span class="comment"># 0, 1, 2는 각각 '패딩, 문서 시작, 사전에없음' 인덱스이므로 3을 뺌</span></span><br><span class="line">decoded_newswire</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;? currency fluctuations may ? their influence on the bullion market in the near future bullion bankers samuel montagu and co ltd said in a market report but the firm said silver may lag behind gold in any reactions to movements on foreign exchanges opec&apos;s failure to address the recent decline in oil prices remains a worrying factor however and on balance it appears that the market should be approached cautiously montagu said the bank said the us economy has shown no ? long term improvement and that both latin american debt and the iranian arms affair could undermine confidence in the dollar reuter 3&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_labels.min(), train_labels.max()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(0, 45)</span><br></pre></td></tr></table></figure>
<ul>
<li>라벨은 0과 45 사이의 정수</li>
</ul>
<h4 id="데이터-준비-1"><a href="#데이터-준비-1" class="headerlink" title="데이터 준비"></a>데이터 준비</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터를 벡터로 변환</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results = np.zeros((len(sequences), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span>        </span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터를 벡터로 변환</span></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br></pre></td></tr></table></figure>
<ul>
<li>라벨을 벡터로 바꾸는 두 가지 방법<ul>
<li>라벨의 리스트를 정수 텐서로 변환한따</li>
<li>원 핫 인코딩(범주형 인코딩)(아래 예시)</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 케라스 내장 함수를 사용한 원핫인코딩</span></span><br><span class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</span><br><span class="line"></span><br><span class="line">one_hot_train_labels = to_categorical(train_labels)</span><br><span class="line">one_hot_test_labels = to_categorical(test_labels)</span><br></pre></td></tr></table></figure>
<h4 id="모델-구성"><a href="#모델-구성" class="headerlink" title="모델 구성"></a>모델 구성</h4><ul>
<li>출력 클래스는 46개. 규모가 작은 층을 사용하면 병목현상으로 유용한 정보를 잃게될 수 있다. 따라서 아래에선 64개 유닛을 사용한다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>마지막 Dense 층의 크기는 46. (각 입력 샘플에 대해 46차원의 벡터를 출력)</li>
<li>마지막 층의 softmax 활성화 함수 : 각 입력 샘플마다 46개 출력 클래스에 대한 확률 분포를 출력. 46개의 값을 더하면 1이 됨</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델 컴파일하기</span></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">             loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">             metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>카테고리컬 크로스엔트로피 손실함수는 두 확률 분포 사이의 거리를 측정</li>
<li>여기선 네트워크가 출력한 확률 분포와 진짜 라벨의 분포 사이의 거리를 측정</li>
<li>두 분포 사이의 거리를 좁힐수록 진짜 라벨에 가까운 출력을 내도록 훈련시킨다</li>
</ul>
<h4 id="훈련-검증-1"><a href="#훈련-검증-1" class="headerlink" title="훈련 검증"></a>훈련 검증</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 테스트셋에서 1000개 샘플을 따로 떼어내 검증셋 준비하기</span></span><br><span class="line">x_val = x_train[:<span class="number">1000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">1000</span>:]</span><br><span class="line">y_val = one_hot_train_labels[:<span class="number">1000</span>]</span><br><span class="line">partial_y_train = one_hot_train_labels[<span class="number">1000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델 트레이닝(에포크 20번)</span></span><br><span class="line">history = model.fit(partial_x_train,</span><br><span class="line">                   partial_y_train,</span><br><span class="line">                   epochs=<span class="number">20</span>,</span><br><span class="line">                   batch_size=<span class="number">512</span>,</span><br><span class="line">                   validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 훈련과 검증 손실</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">loss = history.history[<span class="string">'loss'</span>]</span><br><span class="line">val_loss = history.history[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">'bo'</span>, label=<span class="string">'훈련 손실'</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">'b'</span>, label=<span class="string">'검증 손실'</span>)</span><br><span class="line">plt.title(<span class="string">'훈련과 검증 손실'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/Deep Learning with Python - Ch.03_03.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 훈련과 검증 정확도</span></span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">acc = history.history[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history.history[<span class="string">'val_acc'</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'훈련 정확도'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'검증 정확도'</span>)</span><br><span class="line">plt.title(<span class="string">'훈련과 검증 정확도'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'에포크'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'정확도'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/Deep Learning with Python - Ch.03_04.png" alt=""></p>
<ul>
<li>아홉 번째 에포크 이후 오버피팅이 시작됨. 에포크 9로 새로운 모델 훈련하고 테스트셋에서 평가</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">46</span>, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">              loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">              metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(partial_x_train,</span><br><span class="line">          partial_y_train,</span><br><span class="line">          epochs=<span class="number">9</span>,</span><br><span class="line">          batch_size=<span class="number">512</span>,</span><br><span class="line">          validation_data=(x_val, y_val))</span><br><span class="line">results = model.evaluate(x_test, one_hot_test_labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">results</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 78%의 정확도</span><br><span class="line">[0.9839374910797907, 0.7858414960459524]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">test_labels_copy = copy.copy(test_labels)</span><br><span class="line">np.random.shuffle(test_labels_copy)</span><br><span class="line">float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.19100623330365094</span><br></pre></td></tr></table></figure>
<ul>
<li>무작위 분류시의 19%에 비하면 좋은 결과!</li>
</ul>
<h4 id="새로운-데이터에-대해-예측하기"><a href="#새로운-데이터에-대해-예측하기" class="headerlink" title="새로운 데이터에 대해 예측하기"></a>새로운 데이터에 대해 예측하기</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 테스트셋 예측하기</span></span><br><span class="line">predictions = model.predict(x_test)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>predictions</code><ul>
<li>각 항목은 길이가 46인 벡터</li>
<li>이 벡터의 원소의 합은 1</li>
<li>가장 큰 값이 가장 확률이 높은 클래스(<code>np.argmax</code> 사용)</li>
</ul>
</li>
</ul>
<h4 id="레이블과-손실을-다루는-다른-방법"><a href="#레이블과-손실을-다루는-다른-방법" class="headerlink" title="레이블과 손실을 다루는 다른 방법"></a>레이블과 손실을 다루는 다른 방법</h4><ul>
<li>라벨을 정수 텐서로 변환해서 인코딩 할때는 손실 함수 하나만 바꾸면 된다<ul>
<li><code>loss=sparse_categorical_crossentropy</code></li>
</ul>
</li>
</ul>
<h3 id="주택-가격-예측-회귀-문제"><a href="#주택-가격-예측-회귀-문제" class="headerlink" title="주택 가격 예측: 회귀 문제"></a>주택 가격 예측: 회귀 문제</h3><h4 id="보스턴-주택-가격-데이터셋"><a href="#보스턴-주택-가격-데이터셋" class="headerlink" title="보스턴 주택 가격 데이터셋"></a>보스턴 주택 가격 데이터셋</h4><ul>
<li>506개 데이터 포인트: 트레이닝셋 404개, 테스트셋 102개</li>
<li>각 피쳐는 스케일이 다름</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> boston_housing</span><br><span class="line">(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()</span><br><span class="line"></span><br><span class="line">print(train_data.shape)</span><br><span class="line">print(test_data.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(404, 13)</span><br><span class="line">(102, 13)</span><br></pre></td></tr></table></figure>
<h4 id="데이터-준비-2"><a href="#데이터-준비-2" class="headerlink" title="데이터 준비"></a>데이터 준비</h4><ul>
<li>상이한 스케일을 가진 값을 신경망에 주입하면 문제(학습을 어렵게 만든다)</li>
<li>특성별로 정규화를 해보자</li>
<li>각 특성에 대해 특성의 평균을 빼고 표준편차로 나눔</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터 정규화하기</span></span><br><span class="line">mean = train_data.mean(axis=<span class="number">0</span>)</span><br><span class="line">train_data -= mean</span><br><span class="line">std = train_data.std(axis=<span class="number">0</span>)</span><br><span class="line">train_data /= std</span><br><span class="line"></span><br><span class="line">test_data -= mean</span><br><span class="line">test_data /= std</span><br></pre></td></tr></table></figure>
<h4 id="모델-구성-1"><a href="#모델-구성-1" class="headerlink" title="모델 구성"></a>모델 구성</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델 정의하기</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">()</span>:</span></span><br><span class="line">    model = models.Sequential()</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>,</span><br><span class="line">                          input_shape=(train_data.shape[<span class="number">1</span>],)))</span><br><span class="line">    model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">    model.add(layers.Dense(<span class="number">1</span>))</span><br><span class="line">    model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'mse'</span>, metrics=[<span class="string">'mae'</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<ul>
<li>마지막 층은 선형층(하나의 유닛, 활성화 함수 없음)<ul>
<li>전형적인 스칼라 회귀(하나의 연속적인 값을 예측하는 회귀)를 위한 구성</li>
<li>sigmoid 활성화 함수를 적용하면 네트워크가 0과 1 사이의 값을 예측하도록 학습될 것. 여기선 선형이므로 범위 제한이 없음</li>
</ul>
</li>
<li>mse(평균 제곱 오차) 손실 함수로 컴파일(예측과 타깃 사이 거리의 제곱)</li>
<li>훈련하는 동안 mae(평균 절대 오차)를 측정(예측과 타깃 사이의 절대값)</li>
</ul>
<h4 id="K-겹-검증을-사용한-훈련-검증"><a href="#K-겹-검증을-사용한-훈련-검증" class="headerlink" title="K-겹 검증을 사용한 훈련 검증"></a>K-겹 검증을 사용한 훈련 검증</h4><ul>
<li>데이터셋이 작으면 트레이닝셋과 테스트셋으로 어떤 데이터가 선택됐는지에 따라 검증 점수가 크게 달라지는데 이럴 때 사용</li>
<li>데이터를 K개로 나누고 K개의 모델을 각각 만들어 K-1개의 분할에서 훈련하고 나머지 분할에서 평가</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">num_val_samples = len(train_data) // k</span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line">all_scores = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    print(<span class="string">"처리중인 폴드 #"</span>, i)</span><br><span class="line">    <span class="comment"># 검증 데이터 준비: k번째 분할</span></span><br><span class="line">    val_data = train_data[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    val_targets = train_targets[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 훈련 데이터 준비: 다른 분할 전체</span></span><br><span class="line">    partial_train_data = np.concatenate(</span><br><span class="line">        [train_data[:i * num_val_samples],</span><br><span class="line">         train_data[(i + <span class="number">1</span>) * num_val_samples:]], axis=<span class="number">0</span>)</span><br><span class="line">    partial_train_targets = np.concatenate(</span><br><span class="line">        [train_targets[:i * num_val_samples],</span><br><span class="line">         train_targets[(i + <span class="number">1</span>) * num_val_samples:]], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 케라스 모델 구성(컴파일 포함)</span></span><br><span class="line">    model = build_model()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 모델 훈련(verbose=0 이므로 훈련 과정 출력은 없다)</span></span><br><span class="line">    model.fit(partial_train_data, partial_train_targets,</span><br><span class="line">             epochs=num_epochs, batch_size=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br><span class="line">    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=<span class="number">0</span>)</span><br><span class="line">    all_scores.append(val_mae)</span><br><span class="line"></span><br><span class="line">print(all_scores)</span><br><span class="line">print(np.mean(all_scores))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[2.0463592958922434, 2.3122981940165603, 3.0172314785494665, 2.323145497553419]</span><br><span class="line">2.4247586165029222</span><br></pre></td></tr></table></figure>
<ul>
<li>4가지 검증 점수는 2.04부터 3.01까지 편차가 크지만 평균값인 2.42는 이보다 신뢰할 만하다</li>
<li>다음은 신경망을 500 에포크 동안 훈련</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 각 폴드에서 검증점수 로그에 저장하기</span></span><br><span class="line">num_epochs = <span class="number">500</span></span><br><span class="line">all_mae_histories = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">    print(<span class="string">'처리중인 폴드 #'</span>, i)</span><br><span class="line">    val_data = train_data[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    val_targets = train_targets[i * num_val_samples: (i + <span class="number">1</span>) * num_val_samples]</span><br><span class="line">    partial_train_data = np.concatenate(</span><br><span class="line">        [train_data[:i * num_val_samples],</span><br><span class="line">         train_data[(i + <span class="number">1</span>) * num_val_samples:]], axis=<span class="number">0</span>)</span><br><span class="line">    partial_train_targets = np.concatenate(</span><br><span class="line">        [train_targets[:i * num_val_samples],</span><br><span class="line">         train_targets[(i + <span class="number">1</span>) * num_val_samples:]], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 케라스 모델 구성(컴파일 포함)</span></span><br><span class="line">    model = build_model()</span><br><span class="line">    hisgory = model.fit(partial_train_data, partial_train_targets,</span><br><span class="line">                       validation_data=(val_data, val_targets),</span><br><span class="line">                        epochs=num_epochs, batch_size=<span class="number">1</span>, verbose=<span class="number">0</span>)</span><br><span class="line">    mae_history = history.history[<span class="string">'val_mean_absolute_error'</span>]</span><br><span class="line">    all_mae_histories.append(mae_history)</span><br></pre></td></tr></table></figure>
<p>**</p>
<h4 id="정리"><a href="#정리" class="headerlink" title="정리"></a>정리</h4><ul>
<li>회귀는 손실함수로 평균제곱오차(MSE)를 자주 사용한다</li>
<li>회귀는 평가 지표로 평균절대오차(MAE)를 자주 사용한다</li>
<li>입력 피쳐들이 서로 다른 범위면 스케일을 하자</li>
<li>트레이닝셋이 적다면 은닉층을 한 두개 정도만 사용하자</li>
</ul>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/DSBooks/">DSBooks</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Keras-Deep-Learning-with-Python/">Keras, Deep Learning with Python</a></li></ul>

      
            
      
    </footer>
  </div>
  
</article>

<!-- Table of Contents -->

    

  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/2/">Next</a>
  </nav>

</section>
          <aside id="sidebar">
  
    <div class="widget-wrap" style="margin: 20px 0;">
	<div id="search-form-wrap">

    <form class="search-form">
        <label style="width: 75%;">
            <span class="screen-reader-text">Search for:</span>
            <input type="search" class="search-field" style="height: 42px;" placeholder=" Search…" value="" name="s" title="Search for:">
        </label>
        <input type="submit" class="search-form-submit" value="Search">
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something...">
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
</div>
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Connect With Us</h3>
    <div class="widget widget_athemes_social_icons">

    	<ul class="clearfix widget-social-icons">   
    	
          
     			  <li><a href="https://github.com/foxsayy" title="Github"><i class="fa fa-github" aria-hidden="true"></i></a></li> 
          
   		
          
     			  <li><a href="https://www.instagram.com/roh.sng.hwan/?hl=ko" title="Instagram"><i class="fa fa-instagram" aria-hidden="true"></i></a></li> 
          
   		
          
            <li><a href="mailto:roh.sng.hwan@gmail.com?subject=请联系我&body=我能帮你什么" title="email"><i class="fa fa-envelope" aria-hidden="true"></i></a></li> 
          
   		
   		</ul>


   		<!--
   		<ul class="clearfix widget-social-icons">   		
   		<li class="widget-si-twitter"><a href="http://twitter.com" title="Twitter"><i class="ico-twitter"></i></a></li> 
		<li class="widget-si-facebook"><a href="http://facebook.com" title="Facebook"><i class="ico-facebook"></i></a></li>
			<li class="widget-si-gplus"><a href="http://plus.google.com" title="Google+"><i class="ico-gplus"></i></a></li>
			<li class="widget-si-pinterest"><a href="http://pinterest.com" title="Pinterest"><i class="ico-pinterest"></i></a></li>
			<li class="widget-si-flickr"><a href="http://flickr.com" title="Flickr"><i class="ico-flickr"></i></a></li>
			<li class="widget-si-instagram"><a href="http://instagram.com" title="Instagram"><i class="ico-instagram"></i></a></li>
		</ul> -->

    </div>
  </div>


  
    
  <div class="widget_athemes_tabs">
    <ul id="widget-tab" class="clearfix widget-tab-nav">
      <li class="active"><a>Recent Posts</a></li>
    </ul>
    <div class="widget">
      <ul>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/02/20/Python-딕셔너리-key와-value-뒤집기/">[Python] 딕셔너리 key와 value 뒤집기</a></h6>
              <span>February 20, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/02/15/Deep-Learning-with-Python-Ch-06/">Deep Learning with Python - Ch.06</a></h6>
              <span>February 15, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/02/15/Deep-Learning-with-Python-Ch-05/">Deep Learning with Python - Ch.05</a></h6>
              <span>February 15, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/02/14/Deep-Learning-with-Python-Ch-04/">Deep Learning with Python - Ch.04</a></h6>
              <span>February 14, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/02/06/Deep-Learning-with-Python-Ch-03/">Deep Learning with Python - Ch.03</a></h6>
              <span>February 6, 2019</span>
            </div>

          </li>
        
          <li class="clearfix">

            
              <div class="widget-entry-summary" style="margin: 0;">
            

              <h6 style="margin: 0;"><a href="/2019/01/28/Deep-Learning-with-Python-Ch-02/">Deep Learning with Python - Ch.02</a></h6>
              <span>January 28, 2019</span>
            </div>

          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/DSBooks/">DSBooks</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/MySQL/">MySQL</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NoSQL/">NoSQL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/The-Economist/">The Economist</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/books/">books</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/datascienceschool/">datascienceschool</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/패턴-인식과-머신-러닝/">패턴 인식과 머신 러닝</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>

    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">16</span></li></ul>
    </div>
  </div>


  
</aside>
        
      </div>
      <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2019 THE DATASCIENTIST All Rights Reserved.
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hiero" target="_blank">hiero</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var contentdiv = document.getElementById("content");

    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
</script>

<!-- Custome JS -->
<script src="/js/my.js"></script>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories/index.html" class="mobile-nav-link">Categories</a>
  
    <a href="/log/index.html" class="mobile-nav-link">Log</a>
  
    <a href="/about/index.html" class="mobile-nav-link">About</a>
  
</nav> -->
    

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.5/jquery.fancybox.min.js"></script>


<script src="/js/scripts.js"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>








  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
